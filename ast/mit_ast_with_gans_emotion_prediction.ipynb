{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf24ec8",
   "metadata": {},
   "source": [
    "# üéµ MIT AST with GAN Augmentation for Emotion Prediction\n",
    "\n",
    "## Overview\n",
    "This notebook implements emotion prediction using MIT's **Audio Spectrogram Transformer (AST)** model with **GAN-based data augmentation**. We fine-tune the pre-trained AST model on the DEAM dataset to predict valence and arousal values for music emotion recognition.\n",
    "\n",
    "### Key Features:\n",
    "- ü§ñ **MIT AST Model**: State-of-the-art audio classification transformer\n",
    "- üé® **GAN Augmentation**: Synthetic spectrogram generation for data expansion\n",
    "- üìä **DEAM Dataset**: Emotion annotations for 1,800+ music tracks\n",
    "- üèãÔ∏è **Fine-tuning**: Adaptation from AudioSet to emotion regression\n",
    "- üìà **Comprehensive Evaluation**: Detailed performance metrics and visualization\n",
    "\n",
    "### Model Architecture:\n",
    "- **Base Model**: `MIT/ast-finetuned-audioset-10-10-0.4593`\n",
    "- **Input**: Mel-spectrograms (128 bins, 1024 time frames)\n",
    "- **Output**: Valence & Arousal predictions (continuous values)\n",
    "- **Augmentation**: Conditional GAN for synthetic data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c69f5",
   "metadata": {},
   "source": [
    "## üì¶ Installation & Setup\n",
    "\n",
    "Install required packages for audio processing, deep learning, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dbbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch torchaudio librosa matplotlib seaborn scikit-learn\n",
    "!pip install Pillow numpy pandas tqdm\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b2989c",
   "metadata": {},
   "source": [
    "## üîß Import Libraries\n",
    "\n",
    "Import all necessary libraries for audio processing, machine learning, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908bb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import ASTModel, ASTFeatureExtractor\n",
    "\n",
    "# Image processing (for spectrograms)\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f7391b",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "Set up all parameters for data processing, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# DATASET CONFIGURATION\n",
    "# ========================\n",
    "AUDIO_DIR = '/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/DEAM_audio/MEMD_audio/'\n",
    "ANNOTATIONS_DIR = '/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/DEAM_Annotations/annotations/annotations averaged per song/song_level/'\n",
    "\n",
    "# ========================\n",
    "# AUDIO PROCESSING CONFIG\n",
    "# ========================\n",
    "SAMPLE_RATE = 16000          # AST model expects 16kHz audio\n",
    "DURATION = 10                # Audio clip duration (seconds) - AST optimal\n",
    "TARGET_LENGTH = 1024         # AST expects 1024 time frames\n",
    "N_MELS = 128                 # Number of mel-frequency bins\n",
    "HOP_LENGTH = 160             # Hop length for STFT (16000/100 = 160)\n",
    "N_FFT = 400                  # FFT window size\n",
    "FMIN = 50                    # Minimum frequency\n",
    "FMAX = 8000                  # Maximum frequency (AST optimal: 8kHz)\n",
    "\n",
    "# ========================\n",
    "# AST MODEL CONFIGURATION\n",
    "# ========================\n",
    "# OPTION 1: Use pre-downloaded model (recommended to avoid download issues)\n",
    "AST_MODEL_NAME = '/kaggle/input/mit-ast-model-kaggle/mit-ast-model-for-kaggle'  # Update with your dataset path\n",
    "\n",
    "# OPTION 2: Fallback to online download (may fail with server issues)\n",
    "# AST_MODEL_NAME = 'MIT/ast-finetuned-audioset-10-10-0.4593'\n",
    "\n",
    "FREEZE_BACKBONE = False      # Whether to freeze AST encoder layers\n",
    "DROPOUT = 0.3                # Dropout rate for emotion head\n",
    "\n",
    "# ========================\n",
    "# GAN CONFIGURATION\n",
    "# ========================\n",
    "LATENT_DIM = 100             # Dimension of GAN noise vector\n",
    "CONDITION_DIM = 2            # Valence + Arousal\n",
    "GAN_LR = 0.0002              # GAN learning rate\n",
    "GAN_BETA1 = 0.5              # Adam beta1 for GAN\n",
    "GAN_BETA2 = 0.999            # Adam beta2 for GAN\n",
    "GAN_EPOCHS = 15              # GAN pre-training epochs\n",
    "GAN_BATCH_SIZE = 32          # GAN batch size\n",
    "NUM_SYNTHETIC = 3200         # Number of synthetic samples to generate\n",
    "\n",
    "# ========================\n",
    "# TRAINING CONFIGURATION\n",
    "# ========================\n",
    "BATCH_SIZE = 16              # Training batch size (AST is memory-intensive)\n",
    "NUM_EPOCHS = 24              # Training epochs\n",
    "LEARNING_RATE = 5e-5         # Learning rate for fine-tuning (smaller for pre-trained)\n",
    "WEIGHT_DECAY = 0.01          # AdamW weight decay\n",
    "WARMUP_STEPS = 100           # Learning rate warmup steps\n",
    "TRAIN_SPLIT = 0.8            # Train/validation split ratio\n",
    "\n",
    "# ========================\n",
    "# SYSTEM CONFIGURATION\n",
    "# ========================\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "OUTPUT_DIR = '/kaggle/working/ast_augmented'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Audio Duration: {DURATION}s @ {SAMPLE_RATE}Hz\")\n",
    "print(f\"Mel-Spectrogram: {N_MELS} bins x {TARGET_LENGTH} frames\")\n",
    "print(f\"\\\\nü§ñ AST Configuration:\")\n",
    "print(f\"  - Model Path: {AST_MODEL_NAME}\")\n",
    "print(f\"  - Input Shape: [{N_MELS}, {TARGET_LENGTH}]\")\n",
    "print(f\"  - Freeze Backbone: {FREEZE_BACKBONE}\")\n",
    "print(f\"\\\\nüé® GAN Configuration:\")\n",
    "print(f\"  - Latent Dim: {LATENT_DIM}\")\n",
    "print(f\"  - GAN Epochs: {GAN_EPOCHS}\")\n",
    "print(f\"  - Synthetic Samples: {NUM_SYNTHETIC}\")\n",
    "print(f\"\\\\nüèãÔ∏è Training Configuration:\")\n",
    "print(f\"  - Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Warmup Steps: {WARMUP_STEPS}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377fffe1",
   "metadata": {},
   "source": [
    "## üìÇ Dataset Loading and Exploration\n",
    "\n",
    "Load the DEAM dataset and explore its structure for emotion prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053e03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_deam_annotations():\n",
    "    \"\"\"Load and process DEAM emotion annotations.\"\"\"\n",
    "    print(\"üìä Loading DEAM annotations...\")\n",
    "    \n",
    "    # Use the same approach as ViT notebook - load from two separate CSV files\n",
    "    root = Path('/kaggle/input').resolve()\n",
    "    static_2000 = root / 'static-annotations-1-2000' / 'static_annotations_averaged_songs_1_2000.csv'\n",
    "    static_2058 = root / 'static-annots-2058' / 'static_annots_2058.csv'\n",
    "    \n",
    "    try:\n",
    "        # Load both annotation files\n",
    "        df1 = pd.read_csv(static_2000)\n",
    "        df2 = pd.read_csv(static_2058)\n",
    "        df = pd.concat([df1, df2], axis=0).reset_index(drop=True)\n",
    "        print(f\"‚úÖ Loaded {len(df)} annotations from both files\")\n",
    "        \n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading annotations: {e}\")\n",
    "        print(\"Trying alternative single file approach...\")\n",
    "        \n",
    "        # Fallback to single file approach\n",
    "        static_file = os.path.join(ANNOTATIONS_DIR, 'static_annotations.csv')\n",
    "        \n",
    "        if not os.path.exists(static_file):\n",
    "            print(f\"‚ùå Annotations file not found: {static_file}\")\n",
    "            return None\n",
    "        \n",
    "        # Read annotations\n",
    "        df = pd.read_csv(static_file)\n",
    "        print(f\"‚úÖ Loaded {len(df)} annotations from single file\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(f\"\\nüìà Dataset Statistics:\")\n",
    "    print(f\"  - Total songs: {len(df)}\")\n",
    "    print(f\"  - Valence range: [{df['valence_mean'].min():.3f}, {df['valence_mean'].max():.3f}]\")\n",
    "    print(f\"  - Arousal range: [{df['arousal_mean'].min():.3f}, {df['arousal_mean'].max():.3f}]\")\n",
    "    \n",
    "    # Check for missing audio files\n",
    "    audio_files = []\n",
    "    missing_files = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        song_id = str(int(row['song_id']))\n",
    "        audio_file = os.path.join(AUDIO_DIR, f\"{song_id}.mp3\")\n",
    "        if os.path.exists(audio_file):\n",
    "            audio_files.append(audio_file)\n",
    "        else:\n",
    "            missing_files.append(f\"{song_id}.mp3\")\n",
    "    \n",
    "    print(f\"  - Available audio files: {len(audio_files)}\")\n",
    "    if missing_files:\n",
    "        print(f\"  - Missing audio files: {len(missing_files)}\")\n",
    "    \n",
    "    # Filter dataframe to only include available audio files\n",
    "    available_songs = [os.path.basename(f).replace('.mp3', '') for f in audio_files]\n",
    "    df_filtered = df[df['song_id'].astype(str).isin(available_songs)].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"  - Final dataset size: {len(df_filtered)}\")\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35619bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "annotations_df = load_deam_annotations()\n",
    "\n",
    "if annotations_df is not None:\n",
    "    # Display sample data\n",
    "    print(\"\\nüìã Sample annotations:\")\n",
    "    display(annotations_df.head())\n",
    "    \n",
    "    # Plot emotion distribution\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Valence distribution\n",
    "    axes[0].hist(annotations_df['valence_mean'], bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "    axes[0].set_title('Valence Distribution')\n",
    "    axes[0].set_xlabel('Valence')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Arousal distribution\n",
    "    axes[1].hist(annotations_df['arousal_mean'], bins=30, alpha=0.7, color='red', edgecolor='black')\n",
    "    axes[1].set_title('Arousal Distribution')\n",
    "    axes[1].set_xlabel('Arousal')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Valence vs Arousal scatter plot\n",
    "    scatter = axes[2].scatter(annotations_df['valence_mean'], annotations_df['arousal_mean'], \n",
    "                             alpha=0.6, c=annotations_df['arousal_mean'], cmap='viridis')\n",
    "    axes[2].set_title('Valence vs Arousal')\n",
    "    axes[2].set_xlabel('Valence')\n",
    "    axes[2].set_ylabel('Arousal')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[2], label='Arousal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ùå Failed to load dataset. Please check the file paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70136519",
   "metadata": {},
   "source": [
    "## üéµ Audio Processing for AST\n",
    "\n",
    "Process audio files to create AST-compatible spectrograms (128 mel bins, 1024 time frames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_audio(file_path, duration=DURATION, sr=SAMPLE_RATE):\n",
    "    \"\"\"Load audio file and preprocess for AST model.\"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        audio, _ = librosa.load(file_path, sr=sr, duration=duration)\n",
    "        \n",
    "        # Ensure minimum length\n",
    "        if len(audio) < sr * duration:\n",
    "            # Pad with zeros if too short\n",
    "            audio = np.pad(audio, (0, sr * duration - len(audio)), mode='constant')\n",
    "        else:\n",
    "            # Trim if too long\n",
    "            audio = audio[:sr * duration]\n",
    "        \n",
    "        return audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def audio_to_spectrogram(audio, sr=SAMPLE_RATE, n_mels=N_MELS, target_length=TARGET_LENGTH):\n",
    "    \"\"\"Convert audio to mel-spectrogram compatible with AST.\"\"\"\n",
    "    try:\n",
    "        # Compute mel-spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio,\n",
    "            sr=sr,\n",
    "            n_mels=n_mels,\n",
    "            n_fft=N_FFT,\n",
    "            hop_length=HOP_LENGTH,\n",
    "            fmin=FMIN,\n",
    "            fmax=FMAX\n",
    "        )\n",
    "        \n",
    "        # Convert to log scale (dB)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Normalize to [0, 1] range\n",
    "        mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min())\n",
    "        \n",
    "        # Resize to target length (AST expects 1024 time frames)\n",
    "        if mel_spec_norm.shape[1] != target_length:\n",
    "            # Use interpolation to resize\n",
    "            from scipy.ndimage import zoom\n",
    "            zoom_factor = target_length / mel_spec_norm.shape[1]\n",
    "            mel_spec_norm = zoom(mel_spec_norm, (1, zoom_factor))\n",
    "        \n",
    "        # Ensure exact target shape\n",
    "        if mel_spec_norm.shape[1] > target_length:\n",
    "            mel_spec_norm = mel_spec_norm[:, :target_length]\n",
    "        elif mel_spec_norm.shape[1] < target_length:\n",
    "            pad_width = target_length - mel_spec_norm.shape[1]\n",
    "            mel_spec_norm = np.pad(mel_spec_norm, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        \n",
    "        return mel_spec_norm\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating spectrogram: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_audio_file(file_path):\n",
    "    \"\"\"Complete preprocessing pipeline for a single audio file.\"\"\"\n",
    "    # Load audio\n",
    "    audio = load_and_preprocess_audio(file_path)\n",
    "    if audio is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert to spectrogram\n",
    "    spectrogram = audio_to_spectrogram(audio)\n",
    "    if spectrogram is None:\n",
    "        return None\n",
    "    \n",
    "    return spectrogram\n",
    "\n",
    "# Test the preprocessing pipeline\n",
    "if annotations_df is not None and len(annotations_df) > 0:\n",
    "    print(\"üß™ Testing audio preprocessing pipeline...\")\n",
    "    \n",
    "    # Get first audio file\n",
    "    test_song_id = annotations_df.iloc[0]['song_id']\n",
    "    test_file = os.path.join(AUDIO_DIR, f\"{test_song_id}.mp3\")\n",
    "    \n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"Processing: {test_file}\")\n",
    "        \n",
    "        # Preprocess the audio\n",
    "        spectrogram = preprocess_audio_file(test_file)\n",
    "        \n",
    "        if spectrogram is not None:\n",
    "            print(f\"‚úÖ Preprocessing successful!\")\n",
    "            print(f\"   Spectrogram shape: {spectrogram.shape}\")\n",
    "            print(f\"   Expected shape: ({N_MELS}, {TARGET_LENGTH})\")\n",
    "            print(f\"   Value range: [{spectrogram.min():.3f}, {spectrogram.max():.3f}]\")\n",
    "            \n",
    "            # Visualize the spectrogram\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            librosa.display.specshow(spectrogram, \n",
    "                                   x_axis='time', \n",
    "                                   y_axis='mel',\n",
    "                                   sr=SAMPLE_RATE,\n",
    "                                   hop_length=HOP_LENGTH,\n",
    "                                   fmax=FMAX)\n",
    "            plt.colorbar(format='%+2.0f dB')\n",
    "            plt.title(f'Mel-Spectrogram: {test_song_id}')\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.hist(spectrogram.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
    "            plt.title('Spectrogram Value Distribution')\n",
    "            plt.xlabel('Normalized Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"‚ùå Preprocessing failed!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Test file not found: {test_file}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No annotations available for testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the preprocessing pipeline\n",
    "if annotations_df is not None and len(annotations_df) > 0:\n",
    "    print(\"üß™ Testing audio preprocessing pipeline...\")\n",
    "    \n",
    "    # Get first audio file (ensure integer conversion for file naming)\n",
    "    test_song_id = str(int(annotations_df.iloc[0]['song_id']))  # Convert to integer to remove decimal\n",
    "    test_file = os.path.join(AUDIO_DIR, f\"{test_song_id}.mp3\")\n",
    "    \n",
    "    if os.path.exists(test_file):\n",
    "        print(f\"Processing: {test_file}\")\n",
    "        \n",
    "        # Preprocess the audio\n",
    "        spectrogram = preprocess_audio_file(test_file)\n",
    "        \n",
    "        if spectrogram is not None:\n",
    "            print(f\"‚úÖ Preprocessing successful!\")\n",
    "            print(f\"   Spectrogram shape: {spectrogram.shape}\")\n",
    "            print(f\"   Expected shape: ({N_MELS}, {TARGET_LENGTH})\")\n",
    "            print(f\"   Value range: [{spectrogram.min():.3f}, {spectrogram.max():.3f}]\")\n",
    "            \n",
    "            # Visualize the spectrogram\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            librosa.display.specshow(spectrogram, \n",
    "                                   x_axis='time', \n",
    "                                   y_axis='mel',\n",
    "                                   sr=SAMPLE_RATE,\n",
    "                                   hop_length=HOP_LENGTH,\n",
    "                                   fmax=FMAX)\n",
    "            plt.colorbar(format='%+2.0f dB')\n",
    "            plt.title(f'Mel-Spectrogram: {test_song_id}')\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.hist(spectrogram.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
    "            plt.title('Spectrogram Value Distribution')\n",
    "            plt.xlabel('Normalized Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"‚ùå Preprocessing failed!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Test file not found: {test_file}\")\n",
    "        print(\"Available files in audio directory:\")\n",
    "        audio_files = [f for f in os.listdir(AUDIO_DIR) if f.endswith('.mp3')][:5]  # Show first 5\n",
    "        for af in audio_files:\n",
    "            print(f\"  - {af}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No annotations available for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d8ddf4",
   "metadata": {},
   "source": [
    "## üì¶ Dataset Class for AST\n",
    "\n",
    "Create PyTorch dataset class to handle audio loading and AST feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8927e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEAMASTDataset(Dataset):\n",
    "    \"\"\"Dataset class for DEAM emotion data compatible with AST model.\"\"\"\n",
    "    \n",
    "    def __init__(self, annotations_df, audio_dir, feature_extractor=None, augment=False, use_ast_features=False):\n",
    "        self.annotations_df = annotations_df.reset_index(drop=True)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.augment = augment\n",
    "        self.use_ast_features = use_ast_features  # Whether to use AST feature extractor or manual spectrograms\n",
    "        \n",
    "        print(f\"üìä Dataset initialized with {len(self.annotations_df)} samples\")\n",
    "        if self.augment:\n",
    "            print(\"üé® Data augmentation enabled\")\n",
    "        if self.use_ast_features and self.feature_extractor is not None:\n",
    "            print(\"ü§ñ Using AST feature extractor\")\n",
    "        else:\n",
    "            print(\"üìä Using manual spectrogram conversion\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Get annotation\n",
    "            row = self.annotations_df.iloc[idx]\n",
    "            song_id = str(int(row['song_id']))  # Convert to integer to remove decimal\n",
    "            valence = float(row['valence_mean'])\n",
    "            arousal = float(row['arousal_mean'])\n",
    "            \n",
    "            # Load audio file\n",
    "            audio_file = os.path.join(self.audio_dir, f\"{song_id}.mp3\")\n",
    "            \n",
    "            if not os.path.exists(audio_file):\n",
    "                raise FileNotFoundError(f\"Audio file not found: {audio_file}\")\n",
    "            \n",
    "            # Load and preprocess audio\n",
    "            audio = load_and_preprocess_audio(audio_file)\n",
    "            if audio is None:\n",
    "                raise ValueError(f\"Failed to load audio: {audio_file}\")\n",
    "            \n",
    "            # Apply data augmentation if enabled\n",
    "            if self.augment:\n",
    "                audio = self._apply_audio_augmentation(audio)\n",
    "            \n",
    "            # Use AST feature extractor or manual spectrogram based on configuration\n",
    "            if self.use_ast_features and self.feature_extractor is not None:\n",
    "                # AST feature extractor for model training\n",
    "                inputs = self.feature_extractor(\n",
    "                    audio, \n",
    "                    sampling_rate=SAMPLE_RATE, \n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                audio_features = inputs.input_values.squeeze(0)  # Remove batch dimension\n",
    "            else:\n",
    "                # Manual spectrogram conversion for GAN training or when no feature extractor\n",
    "                spectrogram = audio_to_spectrogram(audio)\n",
    "                if spectrogram is None:\n",
    "                    raise ValueError(f\"Failed to create spectrogram: {audio_file}\")\n",
    "                \n",
    "                # Convert to tensor and add channel dimension for discriminator\n",
    "                # Shape: [1, N_MELS, TARGET_LENGTH] = [1, 128, 1024]\n",
    "                audio_features = torch.from_numpy(spectrogram).float().unsqueeze(0)\n",
    "            \n",
    "            # Create emotion target\n",
    "            emotions = torch.tensor([valence, arousal], dtype=torch.float32)\n",
    "            \n",
    "            return {\n",
    "                'input_values': audio_features,\n",
    "                'emotions': emotions,\n",
    "                'song_id': song_id\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing item {idx} (song_id: {song_id}): {str(e)}\")\n",
    "            # Return a dummy sample to prevent crashes\n",
    "            dummy_features = torch.zeros((1, N_MELS, TARGET_LENGTH))\n",
    "            dummy_emotions = torch.zeros(2)\n",
    "            return {\n",
    "                'input_values': dummy_features,\n",
    "                'emotions': dummy_emotions,\n",
    "                'song_id': 'dummy'\n",
    "            }\n",
    "    \n",
    "    def _apply_audio_augmentation(self, audio):\n",
    "        \"\"\"Apply simple audio augmentation techniques.\"\"\"\n",
    "        augmented = audio.copy()\n",
    "        \n",
    "        # Random volume scaling (0.8 to 1.2)\n",
    "        if random.random() < 0.5:\n",
    "            volume_factor = random.uniform(0.8, 1.2)\n",
    "            augmented = augmented * volume_factor\n",
    "        \n",
    "        # Random time shift (up to 10% of duration)\n",
    "        if random.random() < 0.3:\n",
    "            shift_samples = int(len(augmented) * random.uniform(-0.1, 0.1))\n",
    "            if shift_samples > 0:\n",
    "                augmented = np.concatenate([np.zeros(shift_samples), augmented[:-shift_samples]])\n",
    "            elif shift_samples < 0:\n",
    "                augmented = np.concatenate([augmented[-shift_samples:], np.zeros(-shift_samples)])\n",
    "        \n",
    "        # Random noise injection (very light)\n",
    "        if random.random() < 0.2:\n",
    "            noise_factor = random.uniform(0.01, 0.03)\n",
    "            noise = np.random.normal(0, noise_factor, len(augmented))\n",
    "            augmented = augmented + noise\n",
    "        \n",
    "        return augmented\n",
    "\n",
    "# Test the dataset class\n",
    "if annotations_df is not None:\n",
    "    print(\"üß™ Testing AST Dataset class...\")\n",
    "    \n",
    "    # Create a small test dataset\n",
    "    test_df = annotations_df.head(3).copy()\n",
    "    \n",
    "    # Initialize dataset without feature extractor first\n",
    "    test_dataset = DEAMASTDataset(test_df, AUDIO_DIR, feature_extractor=None, augment=False)\n",
    "    \n",
    "    print(f\"Dataset length: {len(test_dataset)}\")\n",
    "    \n",
    "    # Test data loading\n",
    "    try:\n",
    "        sample = test_dataset[0]\n",
    "        print(f\"‚úÖ Sample loaded successfully!\")\n",
    "        print(f\"   Input shape: {sample['input_values'].shape}\")\n",
    "        print(f\"   Emotions shape: {sample['emotions'].shape}\")\n",
    "        print(f\"   Emotions values: {sample['emotions'].numpy()}\")\n",
    "        print(f\"   Song ID: {sample['song_id']}\")\n",
    "        \n",
    "        # Visualize the sample\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Convert to numpy for visualization\n",
    "        spectrogram = sample['input_values'].squeeze().numpy()\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(spectrogram, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.title(f'Spectrogram: {sample[\"song_id\"]}')\n",
    "        plt.xlabel('Time Frames')\n",
    "        plt.ylabel('Mel Bins')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        emotions = sample['emotions'].numpy()\n",
    "        plt.bar(['Valence', 'Arousal'], emotions, color=['blue', 'red'], alpha=0.7)\n",
    "        plt.title('Emotion Values')\n",
    "        plt.ylabel('Score')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        for i, v in enumerate(emotions):\n",
    "            plt.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Dataset test failed: {str(e)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No annotations available for dataset testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e5ada",
   "metadata": {},
   "source": [
    "## ü§ñ MIT AST Model for Emotion Regression\n",
    "\n",
    "Implement the Audio Spectrogram Transformer (AST) model for valence/arousal prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e66b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASTForEmotionRegression(nn.Module):\n",
    "    \"\"\"Audio Spectrogram Transformer for emotion regression with valence/arousal prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=AST_MODEL_NAME, num_emotions=2, freeze_backbone=False, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_emotions = num_emotions\n",
    "        \n",
    "        print(f\"\\\\nü§ñ Initializing AST Model: {model_name}\")\n",
    "        \n",
    "        # Load AST model and feature extractor with robust error handling\n",
    "        self.ast_model, self.feature_extractor = self._load_ast_model(model_name)\n",
    "        \n",
    "        # Get the hidden size from the model configuration\n",
    "        self.hidden_size = self.ast_model.config.hidden_size\n",
    "        print(f\"  Hidden Size: {self.hidden_size}\")\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            self._freeze_backbone()\n",
    "            print(\"  üßä Backbone frozen\")\n",
    "        else:\n",
    "            print(\"  üî• Backbone trainable\")\n",
    "        \n",
    "        # Add custom regression head\n",
    "        self.emotion_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.hidden_size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_size, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, self.num_emotions),\n",
    "            nn.Sigmoid()  # Output range [0, 1] for valence/arousal\n",
    "        )\n",
    "        \n",
    "    def _load_ast_model(self, model_name):\n",
    "        \"\"\"Load AST model with comprehensive error handling.\"\"\"\n",
    "        print(f\"  üì• Loading model from: {model_name}\")\n",
    "        \n",
    "        # Check if this is a local path or online model\n",
    "        if os.path.exists(model_name):\n",
    "            print(f\"  üóÇÔ∏è Loading from local path...\")\n",
    "            return self._load_local_model(model_name)\n",
    "        else:\n",
    "            print(f\"  üåê Loading from Hugging Face Hub...\")\n",
    "            return self._load_online_model(model_name)\n",
    "    \n",
    "    def _load_local_model(self, model_path):\n",
    "        \"\"\"Load model from local filesystem.\"\"\"\n",
    "        try:\n",
    "            print(f\"  üìÇ Checking local model at: {model_path}\")\n",
    "            \n",
    "            # Verify the path exists and contains model files\n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Model path does not exist: {model_path}\")\n",
    "            \n",
    "            # Check for required model files\n",
    "            required_files = ['config.json']\n",
    "            missing_files = [f for f in required_files if not os.path.exists(os.path.join(model_path, f))]\n",
    "            \n",
    "            if missing_files:\n",
    "                raise FileNotFoundError(f\"Missing model files: {missing_files}\")\n",
    "            \n",
    "            # Load the model and feature extractor\n",
    "            print(f\"  ‚ö° Loading AST from local path...\")\n",
    "            model = ASTModel.from_pretrained(model_path, local_files_only=True)\n",
    "            feature_extractor = ASTFeatureExtractor.from_pretrained(model_path, local_files_only=True)\n",
    "            print(f\"  ‚úÖ Successfully loaded local model!\")\n",
    "            return model, feature_extractor\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Local model loading failed: {str(e)}\")\n",
    "            print(f\"  üîÑ Falling back to online download...\")\n",
    "            return self._load_online_model('MIT/ast-finetuned-audioset-10-10-0.4593')\n",
    "    \n",
    "    def _load_online_model(self, model_name):\n",
    "        \"\"\"Load model from Hugging Face Hub with retry logic.\"\"\"\n",
    "        max_retries = 3\n",
    "        retry_delays = [5, 10, 20]  # seconds\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"  üåê Download attempt {attempt + 1}/{max_retries}...\")\n",
    "                \n",
    "                # Try to load from cache first\n",
    "                model = ASTModel.from_pretrained(\n",
    "                    model_name,\n",
    "                    resume_download=True,\n",
    "                    force_download=False,\n",
    "                    cache_dir='/kaggle/working/model_cache'\n",
    "                )\n",
    "                \n",
    "                feature_extractor = ASTFeatureExtractor.from_pretrained(\n",
    "                    model_name,\n",
    "                    resume_download=True,\n",
    "                    force_download=False,\n",
    "                    cache_dir='/kaggle/working/model_cache'\n",
    "                )\n",
    "                \n",
    "                print(f\"  ‚úÖ Successfully loaded {model_name}\")\n",
    "                return model, feature_extractor\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                \n",
    "                if attempt < max_retries - 1:\n",
    "                    delay = retry_delays[attempt]\n",
    "                    print(f\"  ‚è≥ Retrying in {delay} seconds...\")\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    print(f\"  üíÄ All download attempts failed!\")\n",
    "                    print(f\"  üí° SOLUTION: Download the model locally using the provided scripts:\")\n",
    "                    print(f\"     1. Run download_mit_ast.py on your local machine\")\n",
    "                    print(f\"     2. Upload the model as a Kaggle dataset\")\n",
    "                    print(f\"     3. Update AST_MODEL_NAME to your dataset path\")\n",
    "                    raise RuntimeError(f\"Failed to load AST model after {max_retries} attempts: {str(e)}\")\n",
    "    \n",
    "    def _freeze_backbone(self):\n",
    "        \"\"\"Freeze the AST backbone parameters.\"\"\"\n",
    "        for param in self.ast_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_values):\n",
    "        \"\"\"Forward pass through AST + emotion head.\"\"\"\n",
    "        # Get AST outputs\n",
    "        outputs = self.ast_model(input_values=input_values)\n",
    "        \n",
    "        # Use the pooled output (classification token representation)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Pass through emotion regression head\n",
    "        emotions = self.emotion_head(pooled_output)\n",
    "        \n",
    "        return emotions\n",
    "\n",
    "# Test AST model initialization\n",
    "print(\"üß™ Testing AST model initialization...\")\n",
    "\n",
    "try:\n",
    "    # Initialize the model\n",
    "    ast_model = ASTForEmotionRegression(\n",
    "        model_name=AST_MODEL_NAME,\n",
    "        num_emotions=2,\n",
    "        freeze_backbone=FREEZE_BACKBONE,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ AST model initialized successfully!\")\n",
    "    print(f\"   Model parameters: {sum(p.numel() for p in ast_model.parameters()):,}\")\n",
    "    print(f\"   Trainable parameters: {sum(p.numel() for p in ast_model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    # Move to device\n",
    "    ast_model = ast_model.to(DEVICE)\n",
    "    print(f\"   Model moved to: {DEVICE}\")\n",
    "    \n",
    "    # Test forward pass with dummy input\n",
    "    batch_size = 2\n",
    "    dummy_input = torch.randn(batch_size, TARGET_LENGTH * N_MELS).to(DEVICE)  # AST expects flattened spectrogram\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        emotions = ast_model(dummy_input)\n",
    "        print(f\"   Forward pass test: {emotions.shape} (expected: [{batch_size}, 2])\")\n",
    "        print(f\"   Output range: [{emotions.min().item():.3f}, {emotions.max().item():.3f}]\")\n",
    "    \n",
    "    print(\"üéâ AST model is ready for training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå AST model initialization failed: {str(e)}\")\n",
    "    print(\"Please check the model path and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62035ac",
   "metadata": {},
   "source": [
    "## üé® GAN Architecture for Data Augmentation\n",
    "\n",
    "Implement Conditional GAN to generate synthetic spectrograms based on emotion labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4598d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramGenerator(nn.Module):\n",
    "    \"\"\"Generator network for creating synthetic spectrograms conditioned on emotions.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=LATENT_DIM, condition_dim=CONDITION_DIM, \n",
    "                 output_height=N_MELS, output_width=TARGET_LENGTH):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.condition_dim = condition_dim\n",
    "        self.output_height = output_height\n",
    "        self.output_width = output_width\n",
    "        \n",
    "        # Calculate initial feature map size\n",
    "        self.init_height = output_height // 16  # 128 // 16 = 8\n",
    "        self.init_width = output_width // 16   # 1024 // 16 = 64\n",
    "        \n",
    "        # Linear layer to project latent + condition to initial feature map\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(latent_dim + condition_dim, 512 * self.init_height * self.init_width),\n",
    "            nn.BatchNorm1d(512 * self.init_height * self.init_width),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # Transpose convolution layers for upsampling\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # 8x64 -> 16x128\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 16x128 -> 32x256\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 32x256 -> 64x512\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 64x512 -> 128x1024\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()  # Output in range [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise, conditions):\n",
    "        \"\"\"Generate spectrograms from noise and emotion conditions.\"\"\"\n",
    "        # Concatenate noise and conditions\n",
    "        x = torch.cat([noise, conditions], dim=1)\n",
    "        \n",
    "        # Project to initial feature map\n",
    "        x = self.project(x)\n",
    "        x = x.view(x.size(0), 512, self.init_height, self.init_width)\n",
    "        \n",
    "        # Apply transpose convolutions\n",
    "        x = self.conv_blocks(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class SpectrogramDiscriminator(nn.Module):\n",
    "    \"\"\"Discriminator network for distinguishing real vs fake spectrograms.\"\"\"\n",
    "    \n",
    "    def __init__(self, condition_dim=CONDITION_DIM, input_height=N_MELS, input_width=TARGET_LENGTH):\n",
    "        super().__init__()\n",
    "        self.condition_dim = condition_dim\n",
    "        \n",
    "        # Convolutional layers for feature extraction\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # 128x1024 -> 64x512\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 64x512 -> 32x256\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 32x256 -> 16x128\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 16x128 -> 8x64\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Calculate flattened size after convolutions\n",
    "        self.feature_size = 512 * 8 * 64\n",
    "        \n",
    "        # Classification head (real/fake + condition matching)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.feature_size + condition_dim, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, spectrograms, conditions):\n",
    "        \"\"\"Classify spectrograms as real/fake given emotion conditions.\"\"\"\n",
    "        # Extract features from spectrograms\n",
    "        features = self.conv_blocks(spectrograms)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # Concatenate with conditions\n",
    "        x = torch.cat([features, conditions], dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        output = self.classifier(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Initialize GAN components\n",
    "print(\"üé® Initializing GAN components...\")\n",
    "\n",
    "try:\n",
    "    # Create generator and discriminator\n",
    "    generator = SpectrogramGenerator(\n",
    "        latent_dim=LATENT_DIM,\n",
    "        condition_dim=CONDITION_DIM,\n",
    "        output_height=N_MELS,\n",
    "        output_width=TARGET_LENGTH\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    discriminator = SpectrogramDiscriminator(\n",
    "        condition_dim=CONDITION_DIM,\n",
    "        input_height=N_MELS,\n",
    "        input_width=TARGET_LENGTH\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(f\"‚úÖ GAN components initialized!\")\n",
    "    print(f\"   Generator parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "    print(f\"   Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    batch_size = 4\n",
    "    test_noise = torch.randn(batch_size, LATENT_DIM).to(DEVICE)\n",
    "    test_conditions = torch.rand(batch_size, CONDITION_DIM).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Test generator\n",
    "        fake_spectrograms = generator(test_noise, test_conditions)\n",
    "        print(f\"   Generator output shape: {fake_spectrograms.shape}\")\n",
    "        \n",
    "        # Test discriminator\n",
    "        real_prob = discriminator(fake_spectrograms, test_conditions)\n",
    "        print(f\"   Discriminator output shape: {real_prob.shape}\")\n",
    "        print(f\"   Discriminator output range: [{real_prob.min().item():.3f}, {real_prob.max().item():.3f}]\")\n",
    "    \n",
    "    print(\"üéâ GAN components are ready for training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå GAN initialization failed: {str(e)}\")\n",
    "    generator = None\n",
    "    discriminator = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d7a26d",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Training Pipeline\n",
    "\n",
    "Implement the complete training pipeline for GAN pre-training and AST fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "if annotations_df is not None:\n",
    "    print(\"üìä Creating data loaders...\")\n",
    "    \n",
    "    # Split dataset\n",
    "    train_df, val_df = train_test_split(\n",
    "        annotations_df, \n",
    "        test_size=1-TRAIN_SPLIT, \n",
    "        random_state=42,\n",
    "        stratify=None  # Can't stratify continuous values\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {len(train_df)}\")\n",
    "    print(f\"Validation samples: {len(val_df)}\")\n",
    "    \n",
    "    # Create datasets - use manual spectrograms for GAN training compatibility\n",
    "    if 'ast_model' in locals() and ast_model is not None:\n",
    "        feature_extractor = ast_model.feature_extractor\n",
    "    else:\n",
    "        feature_extractor = None\n",
    "    \n",
    "    # For GAN training, we need manual spectrograms (1 channel) not AST features (16 channels)\n",
    "    train_dataset = DEAMASTDataset(train_df, AUDIO_DIR, feature_extractor, augment=True, use_ast_features=False)\n",
    "    val_dataset = DEAMASTDataset(val_df, AUDIO_DIR, feature_extractor, augment=False, use_ast_features=False)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Data loaders created!\")\n",
    "    print(f\"   Train batches: {len(train_loader)}\")\n",
    "    print(f\"   Validation batches: {len(val_loader)}\")\n",
    "\n",
    "def train_gan(generator, discriminator, train_loader, num_epochs=GAN_EPOCHS):\n",
    "    \"\"\"Train the GAN for data augmentation.\"\"\"\n",
    "    print(f\"\\\\nüé® Starting GAN training for {num_epochs} epochs...\")\n",
    "    \n",
    "    # Optimizers\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=GAN_LR, betas=(GAN_BETA1, GAN_BETA2))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=GAN_LR, betas=(GAN_BETA1, GAN_BETA2))\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Training history\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_g_loss = 0.0\n",
    "        epoch_d_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'GAN Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            if batch['song_id'][0] == 'dummy':  # Skip dummy batches\n",
    "                continue\n",
    "                \n",
    "            batch_size = batch['input_values'].size(0)\n",
    "            real_spectrograms = batch['input_values'].to(DEVICE)\n",
    "            real_emotions = batch['emotions'].to(DEVICE)\n",
    "            \n",
    "            # Real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1).to(DEVICE)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(DEVICE)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            # Real spectrograms\n",
    "            d_real = discriminator(real_spectrograms, real_emotions)\n",
    "            d_real_loss = criterion(d_real, real_labels)\n",
    "            \n",
    "            # Fake spectrograms\n",
    "            noise = torch.randn(batch_size, LATENT_DIM).to(DEVICE)\n",
    "            fake_spectrograms = generator(noise, real_emotions)\n",
    "            d_fake = discriminator(fake_spectrograms.detach(), real_emotions)\n",
    "            d_fake_loss = criterion(d_fake, fake_labels)\n",
    "            \n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            d_fake = discriminator(fake_spectrograms, real_emotions)\n",
    "            g_loss = criterion(d_fake, real_labels)  # Generator wants to fool discriminator\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            epoch_g_loss += g_loss.item()\n",
    "            epoch_d_loss += d_loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'G_Loss': f'{g_loss.item():.4f}',\n",
    "                'D_Loss': f'{d_loss.item():.4f}'\n",
    "            })\n",
    "        \n",
    "        avg_g_loss = epoch_g_loss / len(train_loader)\n",
    "        avg_d_loss = epoch_d_loss / len(train_loader)\n",
    "        \n",
    "        g_losses.append(avg_g_loss)\n",
    "        d_losses.append(avg_d_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: G_Loss={avg_g_loss:.4f}, D_Loss={avg_d_loss:.4f}')\n",
    "    \n",
    "    return g_losses, d_losses\n",
    "\n",
    "def train_ast_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS):\n",
    "    \"\"\"Train the AST model for emotion regression.\"\"\"\n",
    "    print(f\"\\\\nü§ñ Starting AST training for {num_epochs} epochs...\")\n",
    "    \n",
    "    # Optimizer with warmup\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'AST Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            if batch['song_id'][0] == 'dummy':  # Skip dummy batches\n",
    "                continue\n",
    "                \n",
    "            inputs = batch['input_values'].to(DEVICE)\n",
    "            targets = batch['emotions'].to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                if batch['song_id'][0] == 'dummy':\n",
    "                    continue\n",
    "                    \n",
    "                inputs = batch['input_values'].to(DEVICE)\n",
    "                targets = batch['emotions'].to(DEVICE)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                epoch_val_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, 'best_ast_model.pth'))\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train={avg_train_loss:.4f}, Val={avg_val_loss:.4f}, LR={scheduler.get_last_lr()[0]:.6f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Run training if all components are available\n",
    "if (annotations_df is not None and \n",
    "    'generator' in locals() and generator is not None and\n",
    "    'discriminator' in locals() and discriminator is not None and\n",
    "    'ast_model' in locals() and ast_model is not None):\n",
    "    \n",
    "    print(\"üöÄ Starting complete training pipeline...\")\n",
    "    \n",
    "    # Step 1: Train GAN with manual spectrograms (1-channel)\n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "    print(\"STEP 1: GAN PRE-TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"üìä Using manual spectrograms for GAN training (discriminator compatibility)...\")\n",
    "    \n",
    "    # The current train_loader uses manual spectrograms (use_ast_features=False) - perfect for GAN\n",
    "    g_losses, d_losses = train_gan(generator, discriminator, train_loader, GAN_EPOCHS)\n",
    "    \n",
    "    # Step 2: Train AST model with AST features (16-channel)\n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "    print(\"STEP 2: AST FINE-TUNING\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"üìä Creating AST-compatible data loaders with feature extractor...\")\n",
    "    \n",
    "    # Create new data loaders specifically for AST training (use AST features)\n",
    "    ast_train_dataset = DEAMASTDataset(train_df, AUDIO_DIR, ast_model.feature_extractor, augment=True, use_ast_features=True)\n",
    "    ast_val_dataset = DEAMASTDataset(val_df, AUDIO_DIR, ast_model.feature_extractor, augment=False, use_ast_features=True)\n",
    "    \n",
    "    ast_train_loader = DataLoader(\n",
    "        ast_train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    ast_val_loader = DataLoader(\n",
    "        ast_val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ AST data loaders created (feature shape will be compatible with model)\")\n",
    "    \n",
    "    train_losses, val_losses = train_ast_model(ast_model, ast_train_loader, ast_val_loader, NUM_EPOCHS)\n",
    "    \n",
    "    print(\"\\\\nüéâ Training completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping training - some components are not available.\")\n",
    "    print(\"Please ensure annotations, models, and data loaders are properly initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e3712",
   "metadata": {},
   "source": [
    "## üìà Model Evaluation and Results\n",
    "\n",
    "Comprehensive evaluation of the trained AST model with metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f38431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Comprehensive evaluation of the trained model.\"\"\"\n",
    "    print(\"üìä Evaluating model performance...\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Evaluating'):\n",
    "            if batch['song_id'][0] == 'dummy':\n",
    "                continue\n",
    "                \n",
    "            inputs = batch['input_values'].to(DEVICE)\n",
    "            targets = batch['emotions'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all predictions and targets\n",
    "    predictions = np.concatenate(all_predictions, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # Overall metrics\n",
    "    metrics['mse'] = mean_squared_error(targets, predictions)\n",
    "    metrics['mae'] = mean_absolute_error(targets, predictions)\n",
    "    metrics['r2'] = r2_score(targets, predictions)\n",
    "    \n",
    "    # Per-dimension metrics\n",
    "    metrics['valence_mse'] = mean_squared_error(targets[:, 0], predictions[:, 0])\n",
    "    metrics['arousal_mse'] = mean_squared_error(targets[:, 1], predictions[:, 1])\n",
    "    metrics['valence_mae'] = mean_absolute_error(targets[:, 0], predictions[:, 0])\n",
    "    metrics['arousal_mae'] = mean_absolute_error(targets[:, 1], predictions[:, 1])\n",
    "    metrics['valence_r2'] = r2_score(targets[:, 0], predictions[:, 0])\n",
    "    metrics['arousal_r2'] = r2_score(targets[:, 1], predictions[:, 1])\n",
    "    \n",
    "    return metrics, predictions, targets\n",
    "\n",
    "def plot_training_history(train_losses, val_losses, g_losses=None, d_losses=None):\n",
    "    \"\"\"Plot training history and losses.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # AST Training Losses\n",
    "    axes[0, 0].plot(train_losses, label='Train Loss', color='blue')\n",
    "    axes[0, 0].plot(val_losses, label='Validation Loss', color='red')\n",
    "    axes[0, 0].set_title('AST Training History')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('MSE Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # GAN Losses (if available)\n",
    "    if g_losses is not None and d_losses is not None:\n",
    "        axes[0, 1].plot(g_losses, label='Generator Loss', color='green')\n",
    "        axes[0, 1].plot(d_losses, label='Discriminator Loss', color='orange')\n",
    "        axes[0, 1].set_title('GAN Training History')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('BCE Loss')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'GAN losses not available', \n",
    "                       ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "        axes[0, 1].set_title('GAN Training History')\n",
    "    \n",
    "    # Learning curves comparison\n",
    "    axes[1, 0].plot(train_losses, label='Train', alpha=0.7)\n",
    "    axes[1, 0].plot(val_losses, label='Validation', alpha=0.7)\n",
    "    axes[1, 0].fill_between(range(len(train_losses)), train_losses, alpha=0.2)\n",
    "    axes[1, 0].fill_between(range(len(val_losses)), val_losses, alpha=0.2)\n",
    "    axes[1, 0].set_title('Learning Curves (Filled)')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss difference\n",
    "    loss_diff = [abs(t - v) for t, v in zip(train_losses, val_losses)]\n",
    "    axes[1, 1].plot(loss_diff, color='purple')\n",
    "    axes[1, 1].set_title('Train-Validation Loss Difference')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('|Train Loss - Val Loss|')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_prediction_results(metrics, predictions, targets):\n",
    "    \"\"\"Plot prediction results and analysis.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Scatter plots for predictions vs targets\n",
    "    axes[0, 0].scatter(targets[:, 0], predictions[:, 0], alpha=0.6, color='blue')\n",
    "    axes[0, 0].plot([0, 1], [0, 1], 'r--', lw=2)  # Perfect prediction line\n",
    "    axes[0, 0].set_xlabel('True Valence')\n",
    "    axes[0, 0].set_ylabel('Predicted Valence')\n",
    "    axes[0, 0].set_title(f'Valence Prediction\\\\n(R¬≤ = {metrics[\"valence_r2\"]:.3f})')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].scatter(targets[:, 1], predictions[:, 1], alpha=0.6, color='red')\n",
    "    axes[0, 1].plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "    axes[0, 1].set_xlabel('True Arousal')\n",
    "    axes[0, 1].set_ylabel('Predicted Arousal')\n",
    "    axes[0, 1].set_title(f'Arousal Prediction\\\\n(R¬≤ = {metrics[\"arousal_r2\"]:.3f})')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error distributions\n",
    "    valence_errors = predictions[:, 0] - targets[:, 0]\n",
    "    arousal_errors = predictions[:, 1] - targets[:, 1]\n",
    "    \n",
    "    axes[0, 2].hist(valence_errors, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "    axes[0, 2].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, 2].set_xlabel('Prediction Error')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].set_title(f'Valence Error Distribution\\\\n(MAE = {metrics[\"valence_mae\"]:.3f})')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].hist(arousal_errors, bins=30, alpha=0.7, color='red', edgecolor='black')\n",
    "    axes[1, 0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Prediction Error')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title(f'Arousal Error Distribution\\\\n(MAE = {metrics[\"arousal_mae\"]:.3f})')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Emotion space visualization\n",
    "    scatter = axes[1, 1].scatter(targets[:, 0], targets[:, 1], \n",
    "                               c=np.sqrt(valence_errors**2 + arousal_errors**2),\n",
    "                               cmap='viridis', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('True Valence')\n",
    "    axes[1, 1].set_ylabel('True Arousal')\n",
    "    axes[1, 1].set_title('Prediction Error in Emotion Space')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[1, 1], label='Prediction Error')\n",
    "    \n",
    "    # Metrics summary\n",
    "    axes[1, 2].axis('off')\n",
    "    metrics_text = f'''\n",
    "    üìä Overall Performance:\n",
    "    \n",
    "    MSE: {metrics[\"mse\"]:.4f}\n",
    "    MAE: {metrics[\"mae\"]:.4f}\n",
    "    R¬≤:  {metrics[\"r2\"]:.4f}\n",
    "    \n",
    "    üîµ Valence:\n",
    "    MSE: {metrics[\"valence_mse\"]:.4f}\n",
    "    MAE: {metrics[\"valence_mae\"]:.4f}\n",
    "    R¬≤:  {metrics[\"valence_r2\"]:.4f}\n",
    "    \n",
    "    üî¥ Arousal:\n",
    "    MSE: {metrics[\"arousal_mse\"]:.4f}\n",
    "    MAE: {metrics[\"arousal_mae\"]:.4f}\n",
    "    R¬≤:  {metrics[\"arousal_r2\"]:.4f}\n",
    "    '''\n",
    "    axes[1, 2].text(0.1, 0.9, metrics_text, transform=axes[1, 2].transAxes,\n",
    "                   fontsize=12, verticalalignment='top', fontfamily='monospace')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run evaluation if model and data are available\n",
    "if ('ast_model' in locals() and ast_model is not None and \n",
    "    'val_loader' in locals() and val_loader is not None):\n",
    "    \n",
    "    print(\"üîç Running model evaluation...\")\n",
    "    \n",
    "    # Load best model if available\n",
    "    best_model_path = os.path.join(OUTPUT_DIR, 'best_ast_model.pth')\n",
    "    if os.path.exists(best_model_path):\n",
    "        print(\"üì¶ Loading best model checkpoint...\")\n",
    "        ast_model.load_state_dict(torch.load(best_model_path))\n",
    "        print(\"‚úÖ Best model loaded!\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics, predictions, targets = evaluate_model(ast_model, val_loader)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\\\nüìä EVALUATION RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Overall MSE:     {metrics['mse']:.4f}\")\n",
    "    print(f\"Overall MAE:     {metrics['mae']:.4f}\")\n",
    "    print(f\"Overall R¬≤:      {metrics['r2']:.4f}\")\n",
    "    print()\n",
    "    print(f\"Valence MSE:     {metrics['valence_mse']:.4f}\")\n",
    "    print(f\"Valence MAE:     {metrics['valence_mae']:.4f}\")\n",
    "    print(f\"Valence R¬≤:      {metrics['valence_r2']:.4f}\")\n",
    "    print()\n",
    "    print(f\"Arousal MSE:     {metrics['arousal_mse']:.4f}\")\n",
    "    print(f\"Arousal MAE:     {metrics['arousal_mae']:.4f}\")\n",
    "    print(f\"Arousal R¬≤:      {metrics['arousal_r2']:.4f}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Plot results\n",
    "    if ('train_losses' in locals() and 'val_losses' in locals()):\n",
    "        g_loss_plot = g_losses if 'g_losses' in locals() else None\n",
    "        d_loss_plot = d_losses if 'd_losses' in locals() else None\n",
    "        plot_training_history(train_losses, val_losses, g_loss_plot, d_loss_plot)\n",
    "    \n",
    "    plot_prediction_results(metrics, predictions, targets)\n",
    "    \n",
    "    print(\"‚úÖ Evaluation completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping evaluation - model or data not available.\")\n",
    "    print(\"Please ensure the model is trained and validation data is loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba173ee4",
   "metadata": {},
   "source": [
    "## üéØ Conclusion and Next Steps\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "This notebook demonstrates the implementation of MIT's Audio Spectrogram Transformer (AST) for music emotion prediction with GAN-based data augmentation:\n",
    "\n",
    "**Key Achievements:**\n",
    "- ‚úÖ Successfully fine-tuned MIT AST model for emotion regression\n",
    "- ‚úÖ Implemented conditional GAN for synthetic spectrogram generation  \n",
    "- ‚úÖ Created robust data pipeline with error handling\n",
    "- ‚úÖ Comprehensive evaluation with multiple metrics\n",
    "- ‚úÖ Local model deployment to avoid download issues\n",
    "\n",
    "**Technical Highlights:**\n",
    "- **Model**: MIT/ast-finetuned-audioset-10-10-0.4593 (86M parameters)\n",
    "- **Input**: 10-second audio clips ‚Üí 128 mel-frequency bins √ó 1024 time frames\n",
    "- **Output**: Continuous valence and arousal predictions [0, 1]\n",
    "- **Augmentation**: Conditional GAN generating 3200+ synthetic samples\n",
    "- **Training**: AdamW optimizer with cosine annealing and warmup\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| Metric | AST (This Work) | Baseline Methods |\n",
    "|--------|----------------|------------------|\n",
    "| Valence R¬≤ | TBD | ~0.65 (traditional) |\n",
    "| Arousal R¬≤ | TBD | ~0.58 (traditional) |\n",
    "| Overall MAE | TBD | ~0.15-0.20 |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Hyperparameter Optimization**:\n",
    "   - Grid search for learning rates and dropout\n",
    "   - Architecture ablation studies\n",
    "   - GAN training stability improvements\n",
    "\n",
    "2. **Advanced Augmentation**:\n",
    "   - Progressive GAN training\n",
    "   - Style transfer techniques\n",
    "   - Multi-modal data fusion\n",
    "\n",
    "3. **Model Comparison**:\n",
    "   - Benchmark against Vision Transformer (ViT)\n",
    "   - Compare with traditional CNN-RNN approaches\n",
    "   - Ensemble methods evaluation\n",
    "\n",
    "4. **Production Deployment**:\n",
    "   - Model quantization and optimization\n",
    "   - Real-time inference pipeline\n",
    "   - API development for music applications\n",
    "\n",
    "### References\n",
    "\n",
    "- MIT AST Paper: [Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778)\n",
    "- DEAM Dataset: [Database for Emotional Analysis in Music](https://cvml.unige.ch/databases/DEAM/)\n",
    "- Hugging Face Transformers: [ü§ó Transformers Library](https://huggingface.co/transformers/)\n",
    "\n",
    "---\n",
    "**üìß Contact**: For questions about this implementation, please refer to the project documentation.\n",
    "**‚≠ê Citation**: If you use this code, please cite the original AST paper and the DEAM dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ded53",
   "metadata": {},
   "source": [
    "## üß™ Comprehensive MIT AST Model Testing\n",
    "\n",
    "Perform thorough testing of the trained MIT AST model including robustness, edge cases, and performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850e85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ast_model_robustness(model, test_loader, device=DEVICE):\n",
    "    \"\"\"Test AST model robustness with audio-specific perturbations.\"\"\"\n",
    "    print(\"üß™ Testing MIT AST model robustness...\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Test results storage\n",
    "    test_results = {\n",
    "        'normal_predictions': [],\n",
    "        'noisy_predictions': [],\n",
    "        'time_shifted_predictions': [],\n",
    "        'frequency_masked_predictions': [],\n",
    "        'targets': [],\n",
    "        'confidence_scores': []\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_loader, desc='AST Robustness Testing')):\n",
    "            if i >= 10:  # Limit to first 10 batches for testing\n",
    "                break\n",
    "                \n",
    "            inputs = batch['input_values'].to(device)\n",
    "            targets = batch['emotions'].to(device)\n",
    "            \n",
    "            # 1. Normal prediction\n",
    "            normal_output = model(inputs)\n",
    "            \n",
    "            # 2. Add noise and test (audio-specific noise)\n",
    "            noise = torch.randn_like(inputs) * 0.05  # Smaller noise for audio\n",
    "            noisy_inputs = inputs + noise\n",
    "            noisy_output = model(noisy_inputs)\n",
    "            \n",
    "            # 3. Time shifting (circular shift in time dimension)\n",
    "            time_shift = torch.randint(-50, 50, (1,)).item()\n",
    "            time_shifted_inputs = torch.roll(inputs, shifts=time_shift, dims=-1)\n",
    "            time_shifted_output = model(time_shifted_inputs)\n",
    "            \n",
    "            # 4. Frequency masking (mask random frequency bands)\n",
    "            freq_masked_inputs = inputs.clone()\n",
    "            if len(inputs.shape) == 3:  # [batch, freq, time]\n",
    "                mask_start = torch.randint(0, inputs.shape[1] - 10, (1,)).item()\n",
    "                freq_masked_inputs[:, mask_start:mask_start+10, :] = 0\n",
    "            freq_masked_output = model(freq_masked_inputs)\n",
    "            \n",
    "            # Calculate confidence (inverse of prediction variance)\n",
    "            confidence = 1.0 / (torch.var(normal_output, dim=1) + 1e-6)\n",
    "            \n",
    "            # Store results\n",
    "            test_results['normal_predictions'].append(normal_output.cpu())\n",
    "            test_results['noisy_predictions'].append(noisy_output.cpu())\n",
    "            test_results['time_shifted_predictions'].append(time_shifted_output.cpu())\n",
    "            test_results['frequency_masked_predictions'].append(freq_masked_output.cpu())\n",
    "            test_results['targets'].append(targets.cpu())\n",
    "            test_results['confidence_scores'].append(confidence.cpu())\n",
    "    \n",
    "    # Concatenate all results\n",
    "    for key in test_results:\n",
    "        if test_results[key]:\n",
    "            test_results[key] = torch.cat(test_results[key], dim=0).numpy()\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "def analyze_ast_prediction_patterns(test_results):\n",
    "    \"\"\"Analyze AST prediction patterns and audio-specific robustness.\"\"\"\n",
    "    print(\"\\\\nüìä Analyzing AST prediction patterns...\")\n",
    "    \n",
    "    normal_pred = test_results['normal_predictions']\n",
    "    noisy_pred = test_results['noisy_predictions']\n",
    "    time_pred = test_results['time_shifted_predictions']\n",
    "    freq_pred = test_results['frequency_masked_predictions']\n",
    "    targets = test_results['targets']\n",
    "    \n",
    "    # Calculate robustness metrics\n",
    "    noise_robustness = np.mean(np.abs(normal_pred - noisy_pred))\n",
    "    time_robustness = np.mean(np.abs(normal_pred - time_pred))\n",
    "    freq_robustness = np.mean(np.abs(normal_pred - freq_pred))\n",
    "    \n",
    "    print(f\"üîä Noise Robustness (MAE): {noise_robustness:.4f}\")\n",
    "    print(f\"‚è∞ Time Shift Robustness (MAE): {time_robustness:.4f}\")\n",
    "    print(f\"üéµ Frequency Mask Robustness (MAE): {freq_robustness:.4f}\")\n",
    "    \n",
    "    # Plot AST-specific robustness analysis\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 16))\n",
    "    \n",
    "    # Row 1: Noise robustness\n",
    "    axes[0, 0].scatter(normal_pred[:, 0], noisy_pred[:, 0], alpha=0.6, color='blue')\n",
    "    axes[0, 0].plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "    axes[0, 0].set_xlabel('Normal Prediction (Valence)')\n",
    "    axes[0, 0].set_ylabel('Noisy Prediction (Valence)')\n",
    "    axes[0, 0].set_title('Noise Robustness - Valence')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].scatter(normal_pred[:, 1], noisy_pred[:, 1], alpha=0.6, color='red')\n",
    "    axes[0, 1].plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "    axes[0, 1].set_xlabel('Normal Prediction (Arousal)')\n",
    "    axes[0, 1].set_ylabel('Noisy Prediction (Arousal)')\n",
    "    axes[0, 1].set_title('Noise Robustness - Arousal')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 2].hist(np.abs(normal_pred - noisy_pred).flatten(), bins=20, alpha=0.7, color='blue')\n",
    "    axes[0, 2].set_xlabel('Absolute Difference')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].set_title('Noise Robustness Distribution')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Row 2: Time shift robustness\n",
    "    axes[1, 0].scatter(normal_pred[:, 0], time_pred[:, 0], alpha=0.6, color='green')\n",
    "    axes[1, 0].plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "    axes[1, 0].set_xlabel('Normal Prediction (Valence)')\n",
    "    axes[1, 0].set_ylabel('Time Shifted Prediction (Valence)')\n",
    "    axes[1, 0].set_title('Time Shift Robustness - Valence')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].scatter(normal_pred[:, 1], time_pred[:, 1], alpha=0.6, color='orange')\n",
    "    axes[1, 1].plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "    axes[1, 1].set_xlabel('Normal Prediction (Arousal)')\n",
    "    axes[1, 1].set_ylabel('Time Shifted Prediction (Arousal)')\n",
    "    axes[1, 1].set_title('Time Shift Robustness - Arousal')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 2].hist(np.abs(normal_pred - time_pred).flatten(), bins=20, alpha=0.7, color='green')\n",
    "    axes[1, 2].set_xlabel('Absolute Difference')\n",
    "    axes[1, 2].set_ylabel('Frequency')\n",
    "    axes[1, 2].set_title('Time Shift Robustness Distribution')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Row 3: Frequency masking robustness\n",
    "    axes[2, 0].scatter(normal_pred[:, 0], freq_pred[:, 0], alpha=0.6, color='purple')\n",
    "    axes[2, 0].plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "    axes[2, 0].set_xlabel('Normal Prediction (Valence)')\n",
    "    axes[2, 0].set_ylabel('Freq Masked Prediction (Valence)')\n",
    "    axes[2, 0].set_title('Frequency Mask Robustness - Valence')\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2, 1].scatter(normal_pred[:, 1], freq_pred[:, 1], alpha=0.6, color='brown')\n",
    "    axes[2, 1].plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "    axes[2, 1].set_xlabel('Normal Prediction (Arousal)')\n",
    "    axes[2, 1].set_ylabel('Freq Masked Prediction (Arousal)')\n",
    "    axes[2, 1].set_title('Frequency Mask Robustness - Arousal')\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2, 2].hist(np.abs(normal_pred - freq_pred).flatten(), bins=20, alpha=0.7, color='purple')\n",
    "    axes[2, 2].set_xlabel('Absolute Difference')\n",
    "    axes[2, 2].set_ylabel('Frequency')\n",
    "    axes[2, 2].set_title('Frequency Mask Robustness Distribution')\n",
    "    axes[2, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'noise_robustness': noise_robustness,\n",
    "        'time_robustness': time_robustness,\n",
    "        'frequency_robustness': freq_robustness,\n",
    "        'mean_confidence': np.mean(test_results['confidence_scores'])\n",
    "    }\n",
    "\n",
    "def test_ast_edge_cases(model, device=DEVICE):\n",
    "    \"\"\"Test AST model behavior on audio-specific edge cases.\"\"\"\n",
    "    print(\"\\\\nüö® Testing AST edge cases...\")\n",
    "    \n",
    "    model.eval()\n",
    "    edge_cases = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Test with silence (all zeros)\n",
    "        silence_input = torch.zeros(1, TARGET_LENGTH * N_MELS).to(device)\n",
    "        silence_pred = model(silence_input)\n",
    "        edge_cases['silence'] = silence_pred.cpu().numpy()\n",
    "        \n",
    "        # Test with white noise\n",
    "        noise_input = torch.randn(1, TARGET_LENGTH * N_MELS).to(device)\n",
    "        noise_pred = model(noise_input)\n",
    "        edge_cases['white_noise'] = noise_pred.cpu().numpy()\n",
    "        \n",
    "        # Test with sine wave pattern\n",
    "        t = torch.linspace(0, 2*np.pi, TARGET_LENGTH * N_MELS).unsqueeze(0)\n",
    "        sine_input = (torch.sin(t) + 1) / 2  # Normalize to [0, 1]\n",
    "        sine_input = sine_input.to(device)\n",
    "        sine_pred = model(sine_input)\n",
    "        edge_cases['sine_wave'] = sine_pred.cpu().numpy()\n",
    "        \n",
    "        # Test with impulse (single spike)\n",
    "        impulse_input = torch.zeros(1, TARGET_LENGTH * N_MELS).to(device)\n",
    "        impulse_input[0, TARGET_LENGTH * N_MELS // 2] = 1.0\n",
    "        impulse_pred = model(impulse_input)\n",
    "        edge_cases['impulse'] = impulse_pred.cpu().numpy()\n",
    "        \n",
    "        # Test with constant value (DC signal)\n",
    "        dc_input = torch.full((1, TARGET_LENGTH * N_MELS), 0.5).to(device)\n",
    "        dc_pred = model(dc_input)\n",
    "        edge_cases['dc_signal'] = dc_pred.cpu().numpy()\n",
    "    \n",
    "    print(\"AST Edge case predictions:\")\n",
    "    for case, pred in edge_cases.items():\n",
    "        valence, arousal = pred[0]\n",
    "        print(f\"  {case:12}: Valence={valence:.3f}, Arousal={arousal:.3f}\")\n",
    "    \n",
    "    return edge_cases\n",
    "\n",
    "def ast_performance_benchmark(model, test_loader, device=DEVICE):\n",
    "    \"\"\"Benchmark AST model performance and memory usage.\"\"\"\n",
    "    print(\"\\\\n‚ö° AST Performance benchmarking...\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    dummy_input = torch.randn(1, TARGET_LENGTH * N_MELS).to(device)\n",
    "    for _ in range(5):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Timing test with different sequence lengths\n",
    "    import time\n",
    "    times = []\n",
    "    seq_lengths = [512, 1024, 2048]  # Different audio lengths\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        test_input = torch.randn(4, seq_len * N_MELS).to(device)  # Fixed batch size of 4\n",
    "        \n",
    "        # Measure inference time\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):  # Average over 10 runs\n",
    "                _ = model(test_input)\n",
    "        \n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_time = (end_time - start_time) / 10\n",
    "        times.append(avg_time)\n",
    "        \n",
    "        print(f\"  Sequence length {seq_len:4d}: {avg_time:.4f}s ({4/avg_time:.1f} samples/s)\")\n",
    "    \n",
    "    # Memory usage analysis\n",
    "    if device.type == 'cuda':\n",
    "        memory_usage = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "        print(f\"  Max GPU memory: {memory_usage:.1f} MB\")\n",
    "        \n",
    "        # Memory per parameter\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        memory_per_param = memory_usage / total_params * 1000  # bytes per parameter\n",
    "        print(f\"  Memory per parameter: {memory_per_param:.2f} bytes\")\n",
    "    \n",
    "    return {'seq_lengths': seq_lengths, 'inference_times': times}\n",
    "\n",
    "def compare_with_baseline(test_results):\n",
    "    \"\"\"Compare AST results with simple baseline predictions.\"\"\"\n",
    "    print(\"\\\\nüìä Comparing with baseline methods...\")\n",
    "    \n",
    "    targets = test_results['targets']\n",
    "    predictions = test_results['normal_predictions']\n",
    "    \n",
    "    # Simple baselines\n",
    "    mean_baseline = np.mean(targets, axis=0)  # Predict dataset mean\n",
    "    median_baseline = np.median(targets, axis=0)  # Predict dataset median\n",
    "    \n",
    "    # Calculate metrics for baselines\n",
    "    mean_baseline_mae = np.mean(np.abs(targets - mean_baseline))\n",
    "    median_baseline_mae = np.mean(np.abs(targets - median_baseline))\n",
    "    ast_mae = np.mean(np.abs(targets - predictions))\n",
    "    \n",
    "    # Improvement over baselines\n",
    "    mean_improvement = (mean_baseline_mae - ast_mae) / mean_baseline_mae * 100\n",
    "    median_improvement = (median_baseline_mae - ast_mae) / median_baseline_mae * 100\n",
    "    \n",
    "    print(f\"\\\\nüìà Baseline Comparison:\")\n",
    "    print(f\"  Mean Baseline MAE:     {mean_baseline_mae:.4f}\")\n",
    "    print(f\"  Median Baseline MAE:   {median_baseline_mae:.4f}\")\n",
    "    print(f\"  AST Model MAE:         {ast_mae:.4f}\")\n",
    "    print(f\"  Improvement over Mean: {mean_improvement:.1f}%\")\n",
    "    print(f\"  Improvement over Median: {median_improvement:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'mean_baseline_mae': mean_baseline_mae,\n",
    "        'median_baseline_mae': median_baseline_mae,\n",
    "        'ast_mae': ast_mae,\n",
    "        'mean_improvement': mean_improvement,\n",
    "        'median_improvement': median_improvement\n",
    "    }\n",
    "\n",
    "# Run comprehensive AST testing\n",
    "if ('ast_model' in locals() and ast_model is not None and \n",
    "    'val_loader' in locals() and val_loader is not None):\n",
    "    \n",
    "    print(\"üöÄ Starting comprehensive MIT AST model testing...\")\n",
    "    \n",
    "    # Load best model if available\n",
    "    try:\n",
    "        best_model_path = os.path.join(OUTPUT_DIR, 'best_ast_model.pth')\n",
    "        if os.path.exists(best_model_path):\n",
    "            ast_model.load_state_dict(torch.load(best_model_path))\n",
    "            print(\"‚úÖ Best AST model loaded for testing\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Using current AST model state for testing\")\n",
    "    \n",
    "    # 1. Robustness testing\n",
    "    ast_test_results = test_ast_model_robustness(ast_model, val_loader)\n",
    "    ast_robustness_metrics = analyze_ast_prediction_patterns(ast_test_results)\n",
    "    \n",
    "    # 2. Edge case testing\n",
    "    ast_edge_results = test_ast_edge_cases(ast_model)\n",
    "    \n",
    "    # 3. Performance benchmarking\n",
    "    ast_perf_results = ast_performance_benchmark(ast_model, val_loader)\n",
    "    \n",
    "    # 4. Baseline comparison\n",
    "    baseline_comparison = compare_with_baseline(ast_test_results)\n",
    "    \n",
    "    # Summary report\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"üìã COMPREHENSIVE MIT AST TESTING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"‚úÖ Robustness Testing:\")\n",
    "    print(f\"   - Noise Robustness: {ast_robustness_metrics['noise_robustness']:.4f}\")\n",
    "    print(f\"   - Time Shift Robustness: {ast_robustness_metrics['time_robustness']:.4f}\")\n",
    "    print(f\"   - Frequency Mask Robustness: {ast_robustness_metrics['frequency_robustness']:.4f}\")\n",
    "    print(f\"   - Mean Confidence: {ast_robustness_metrics['mean_confidence']:.4f}\")\n",
    "    print(f\"\\\\n‚úÖ Edge Cases: All {len(ast_edge_results)} audio-specific test cases completed\")\n",
    "    print(f\"\\\\n‚úÖ Performance: Benchmarked across {len(ast_perf_results['seq_lengths'])} sequence lengths\")\n",
    "    print(f\"\\\\n‚úÖ Baseline Comparison:\")\n",
    "    print(f\"   - Improvement over Mean Baseline: {baseline_comparison['mean_improvement']:.1f}%\")\n",
    "    print(f\"   - Improvement over Median Baseline: {baseline_comparison['median_improvement']:.1f}%\")\n",
    "    print(\"\\\\nüéâ All MIT AST tests completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping comprehensive testing - AST model or validation data not available\")\n",
    "    print(\"Please ensure the AST model is trained and validation data is prepared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a83a43",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Tensor Compatibility Fixes Applied\n",
    "\n",
    "### Issues Fixed:\n",
    "\n",
    "1. **Audio File Naming**: Fixed `str(row['song_id'])` ‚Üí `str(int(row['song_id']))` to handle 2.0 ‚Üí 2.mp3 conversion\n",
    "\n",
    "2. **GAN-AST Tensor Dimension Mismatch**: \n",
    "   - **Problem**: Discriminator expects 1-channel spectrograms `[batch, 1, height, width]` but AST feature extractor returns 16-channel tensors `[batch, 16, 1024, 128]`\n",
    "   - **Solution**: Use different datasets for different training phases:\n",
    "     - **GAN Training**: Manual spectrograms (1-channel) with `use_ast_features=False`\n",
    "     - **AST Training**: AST features (16-channel) with `use_ast_features=True`\n",
    "\n",
    "### Dataset Class Modifications:\n",
    "\n",
    "- Added `use_ast_features` parameter to `DEAMASTDataset`\n",
    "- When `use_ast_features=False`: Returns manual spectrograms compatible with discriminator\n",
    "- When `use_ast_features=True`: Returns AST feature extractor output compatible with AST model\n",
    "\n",
    "### Training Pipeline:\n",
    "\n",
    "- **Step 1**: GAN pre-training uses manual spectrograms for discriminator compatibility\n",
    "- **Step 2**: AST fine-tuning uses AST features for model compatibility\n",
    "\n",
    "This ensures both training phases receive the correct tensor formats while maintaining the overall training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b269460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tensor format fixes\n",
    "if annotations_df is not None and len(annotations_df) > 0:\n",
    "    print(\"üß™ Testing tensor format fixes...\")\n",
    "    \n",
    "    # Test GAN-compatible dataset (manual spectrograms)\n",
    "    test_sample_df = annotations_df.head(2)  # Just test with 2 samples\n",
    "    \n",
    "    print(\"\\\\n1. Testing GAN-compatible dataset (use_ast_features=False):\")\n",
    "    gan_dataset = DEAMASTDataset(test_sample_df, AUDIO_DIR, None, augment=False, use_ast_features=False)\n",
    "    gan_sample = gan_dataset[0]\n",
    "    print(f\"   Manual spectrogram shape: {gan_sample['input_values'].shape}\")\n",
    "    print(f\"   Expected for discriminator: [1, {N_MELS}, {TARGET_LENGTH}]\")\n",
    "    \n",
    "    if 'ast_model' in locals() and ast_model is not None:\n",
    "        print(\"\\\\n2. Testing AST-compatible dataset (use_ast_features=True):\")\n",
    "        ast_dataset = DEAMASTDataset(test_sample_df, AUDIO_DIR, ast_model.feature_extractor, augment=False, use_ast_features=True)\n",
    "        ast_sample = ast_dataset[0]\n",
    "        print(f\"   AST features shape: {ast_sample['input_values'].shape}\")\n",
    "        print(f\"   Expected for AST model: [16, 1024, 128] or similar\")\n",
    "    \n",
    "    print(\"\\\\n‚úÖ Tensor format validation completed!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot test - annotations not loaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
