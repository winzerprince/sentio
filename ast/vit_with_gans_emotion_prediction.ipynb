{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3fb8004",
   "metadata": {},
   "source": [
    "# üéµ Vision Transformer (ViT) Training with GAN-Based Data Augmentation\n",
    "\n",
    "## üìã Overview\n",
    "This notebook implements **Vision Transformer (ViT)** using the pre-trained `google/vit-base-patch16-224-in21k` model for music emotion recognition with **GAN-based data augmentation** to expand the DEAM dataset.\n",
    "\n",
    "### Key Features:\n",
    "- **Pre-trained ViT**: Uses `google/vit-base-patch16-224-in21k` trained on ImageNet-21k\n",
    "- **Transfer Learning**: Fine-tunes large vision model on audio spectrograms\n",
    "- **Conditional GAN**: Generates synthetic spectrograms conditioned on valence/arousal\n",
    "- **Data Expansion**: Increases dataset size from ~1800 to 5000+ samples\n",
    "- **Emotion Prediction**: Valence-Arousal (VA) continuous values\n",
    "\n",
    "### Pipeline:\n",
    "1. Load DEAM dataset and extract real spectrograms\n",
    "2. Train Conditional GAN to generate synthetic spectrograms\n",
    "3. Augment dataset with GAN-generated samples\n",
    "4. Fine-tune pre-trained ViT model on expanded dataset\n",
    "5. Evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb80459",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87dbfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from PIL import Image\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "root = Path('/kaggle/input').resolve()\n",
    "print(f\"Root exists: {root.exists()}\")\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeccfef",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cdd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# DATASET CONFIGURATION\n",
    "# ========================\n",
    "AUDIO_DIR = '/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/DEAM_audio/MEMD_audio/'\n",
    "ANNOTATIONS_DIR = '/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/DEAM_Annotations/annotations/annotations averaged per song/song_level/'\n",
    "\n",
    "# ========================\n",
    "# AUDIO PROCESSING CONFIG\n",
    "# ========================\n",
    "SAMPLE_RATE = 22050          # Audio sampling rate (Hz)\n",
    "DURATION = 30                # Audio clip duration (seconds)\n",
    "N_MELS = 128                 # Number of mel-frequency bins\n",
    "HOP_LENGTH = 512             # Hop length for STFT\n",
    "N_FFT = 2048                 # FFT window size\n",
    "FMIN = 20                    # Minimum frequency\n",
    "FMAX = 8000                  # Maximum frequency\n",
    "\n",
    "# ========================\n",
    "# VIT PREPROCESSING CONFIG\n",
    "# ========================\n",
    "VIT_IMAGE_SIZE = 224         # ViT expects 224x224 images\n",
    "VIT_CHANNELS = 3             # RGB channels (we'll triplicate grayscale)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]  # ImageNet normalization mean\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]   # ImageNet normalization std\n",
    "\n",
    "# ========================\n",
    "# GAN CONFIGURATION\n",
    "# ========================\n",
    "LATENT_DIM = 100             # Dimension of GAN noise vector\n",
    "CONDITION_DIM = 2            # Valence + Arousal\n",
    "GAN_LR = 0.0002              # GAN learning rate\n",
    "GAN_BETA1 = 0.5              # Adam beta1 for GAN\n",
    "GAN_BETA2 = 0.999            # Adam beta2 for GAN\n",
    "GAN_EPOCHS = 10              # GAN pre-training epochs\n",
    "GAN_BATCH_SIZE = 24          # GAN batch size (reduced from 32 to save memory)\n",
    "NUM_SYNTHETIC = 3200         # Number of synthetic samples to generate\n",
    "\n",
    "# ========================\n",
    "# VIT MODEL CONFIGURATION\n",
    "# ========================\n",
    "# OPTION 1: Use pre-downloaded model (recommended to avoid download issues)\n",
    "VIT_MODEL_NAME = '/kaggle/input/vit-model-kaggle/vit-model-for-kaggle'  # Update with your dataset path\n",
    "\n",
    "# OPTION 2: Fallback to online download (may fail with 500 errors)\n",
    "# VIT_MODEL_NAME = 'google/vit-base-patch16-224-in21k'\n",
    "\n",
    "# OPTION 3: Use smaller, more stable model\n",
    "# VIT_MODEL_NAME = 'google/vit-base-patch16-224'\n",
    "\n",
    "FREEZE_BACKBONE = False      # Whether to freeze ViT encoder layers\n",
    "DROPOUT = 0.1                # Dropout rate\n",
    "\n",
    "# ========================\n",
    "# TRAINING CONFIGURATION\n",
    "# ========================\n",
    "BATCH_SIZE = 12              # Training batch size (reduced from 16 to save memory)\n",
    "NUM_EPOCHS = 24              # Training epochs\n",
    "LEARNING_RATE = 1e-4         # Learning rate for fine-tuning\n",
    "WEIGHT_DECAY = 0.05          # AdamW weight decay\n",
    "TRAIN_SPLIT = 0.8            # Train/validation split ratio\n",
    "\n",
    "# ========================\n",
    "# MEMORY OPTIMIZATION\n",
    "# ========================\n",
    "# If you still encounter OOM errors, try these:\n",
    "# - Reduce GAN_BATCH_SIZE to 16\n",
    "# - Reduce BATCH_SIZE to 8\n",
    "# - Reduce NUM_SYNTHETIC to 2000\n",
    "# - Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "# ========================\n",
    "# SYSTEM CONFIGURATION\n",
    "# ========================\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "OUTPUT_DIR = '/kaggle/working/vit_augmented'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Enable memory efficient settings\n",
    "if torch.cuda.is_available():\n",
    "    # Enable TF32 for faster computation on Ampere GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    # Enable cudnn benchmarking for optimal performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"üöÄ CUDA optimizations enabled\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"Audio Duration: {DURATION}s @ {SAMPLE_RATE}Hz\")\n",
    "print(f\"Mel-Spectrogram: {N_MELS} bins\")\n",
    "print(f\"\\nüñºÔ∏è ViT Configuration:\")\n",
    "print(f\"  - Model Path: {VIT_MODEL_NAME}\")\n",
    "print(f\"  - Input Size: {VIT_IMAGE_SIZE}x{VIT_IMAGE_SIZE}x{VIT_CHANNELS}\")\n",
    "print(f\"  - Freeze Backbone: {FREEZE_BACKBONE}\")\n",
    "print(f\"\\nüé® GAN Configuration:\")\n",
    "print(f\"  - Latent Dim: {LATENT_DIM}\")\n",
    "print(f\"  - GAN Epochs: {GAN_EPOCHS}\")\n",
    "print(f\"  - GAN Batch Size: {GAN_BATCH_SIZE}\")\n",
    "print(f\"  - Synthetic Samples: {NUM_SYNTHETIC}\")\n",
    "print(f\"\\nüèãÔ∏è Training Configuration:\")\n",
    "print(f\"  - Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe6d571",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load DEAM Dataset & Extract Real Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc281fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "static_2000 = root / 'static-annotations-1-2000' / 'static_annotations_averaged_songs_1_2000.csv'\n",
    "static_2058 = root / 'static-annots-2058' / 'static_annots_2058.csv'\n",
    "\n",
    "try:\n",
    "    df1 = pd.read_csv(static_2000)\n",
    "    df2 = pd.read_csv(static_2058)\n",
    "    df_annotations = pd.concat([df1, df2], axis=0)\n",
    "    print(f\"‚úÖ Loaded annotations: {len(df_annotations)} songs\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading annotations: {e}\")\n",
    "    raise\n",
    "\n",
    "# Clean column names\n",
    "df_annotations.columns = df_annotations.columns.str.strip()\n",
    "\n",
    "print(\"\\\\nüìä Annotation Sample:\")\n",
    "print(df_annotations.head())\n",
    "print(f\"\\\\nColumns: {list(df_annotations.columns)}\")\n",
    "\n",
    "# Check for audio files\n",
    "audio_files = glob.glob(os.path.join(AUDIO_DIR, '*.mp3'))\n",
    "print(f\"\\\\nüéµ Found {len(audio_files)} audio files\")\n",
    "\n",
    "# Extract spectrograms with error logging\n",
    "print(\"\\\\nüîä Extracting spectrograms from real audio...\")\n",
    "\n",
    "error_log = []\n",
    "\n",
    "def extract_melspectrogram(audio_path, sr=SAMPLE_RATE, duration=DURATION):\n",
    "    \"\"\"Extract mel-spectrogram from audio file with error handling\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        y, _ = librosa.load(audio_path, sr=sr, duration=duration)\n",
    "        \n",
    "        # Compute mel-spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=y, sr=sr, n_mels=N_MELS, n_fft=N_FFT, \n",
    "            hop_length=HOP_LENGTH, fmin=FMIN, fmax=FMAX\n",
    "        )\n",
    "        \n",
    "        # Convert to log scale (dB)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        mel_spec_norm = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-8)\n",
    "        \n",
    "        return mel_spec_norm, None\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {os.path.basename(audio_path)}: {str(e)}\"\n",
    "        return None, error_msg\n",
    "\n",
    "# Extract spectrograms and labels\n",
    "real_spectrograms = []\n",
    "real_labels = []\n",
    "\n",
    "for idx, row in tqdm(df_annotations.iterrows(), total=len(df_annotations), desc=\"Extracting spectrograms\"):\n",
    "    song_id = str(int(row['song_id']))\n",
    "    audio_path = os.path.join(AUDIO_DIR, f\"{song_id}.mp3\")\n",
    "    \n",
    "    if not os.path.exists(audio_path):\n",
    "        error_log.append(f\"Missing audio file: {song_id}.mp3\")\n",
    "        continue\n",
    "    \n",
    "    # Extract spectrogram\n",
    "    spec, error = extract_melspectrogram(audio_path)\n",
    "    \n",
    "    if error is not None:\n",
    "        error_log.append(error)\n",
    "        continue\n",
    "        \n",
    "    if spec is not None:\n",
    "        real_spectrograms.append(spec)\n",
    "        \n",
    "        # Get valence and arousal\n",
    "        valence = row.get('valence_mean', row.get('valence', 0.5))\n",
    "        arousal = row.get('arousal_mean', row.get('arousal', 0.5))\n",
    "        \n",
    "        # Normalize to [-1, 1] range\n",
    "        valence_norm = (valence - 5.0) / 4.0\n",
    "        arousal_norm = (arousal - 5.0) / 4.0\n",
    "        \n",
    "        real_labels.append([valence_norm, arousal_norm])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "real_spectrograms = np.array(real_spectrograms)\n",
    "real_labels = np.array(real_labels)\n",
    "\n",
    "print(f\"\\\\n‚úÖ Extracted {len(real_spectrograms)} spectrograms\")\n",
    "print(f\"Spectrogram shape: {real_spectrograms.shape}\")\n",
    "print(f\"Labels shape: {real_labels.shape}\")\n",
    "print(f\"Spectrogram range: [{real_spectrograms.min():.2f}, {real_spectrograms.max():.2f}]\")\n",
    "print(f\"Labels range: [{real_labels.min():.2f}, {real_labels.max():.2f}]\")\n",
    "\n",
    "if error_log:\n",
    "    print(f\"\\\\n‚ö†Ô∏è {len(error_log)} errors occurred during extraction:\")\n",
    "    for i, error in enumerate(error_log[:10]):  # Show first 10 errors\n",
    "        print(f\"  {i+1}. {error}\")\n",
    "    if len(error_log) > 10:\n",
    "        print(f\"  ... and {len(error_log) - 10} more errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a89a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample spectrogram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].imshow(real_spectrograms[0], aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0].set_title(f'Sample Spectrogram\\\\nValence: {real_labels[0][0]:.2f}, Arousal: {real_labels[0][1]:.2f}')\n",
    "axes[0].set_xlabel('Time Frames')\n",
    "axes[0].set_ylabel('Mel Frequency Bins')\n",
    "\n",
    "axes[1].scatter(real_labels[:, 0], real_labels[:, 1], alpha=0.5)\n",
    "axes[1].set_xlabel('Valence (normalized)')\n",
    "axes[1].set_ylabel('Arousal (normalized)')\n",
    "axes[1].set_title('Valence-Arousal Distribution (Real Data)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(0, color='k', linewidth=0.5)\n",
    "axes[1].axvline(0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'real_data_visualization.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d39a406",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Conditional GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d55f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Memory-efficient channel attention instead of spatial self-attention.\n",
    "    Reduces memory from O(H*W * H*W) to O(C*C).\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, channels, _, _ = x.size()\n",
    "        \n",
    "        # Channel attention via global pooling\n",
    "        avg_out = self.fc(self.avg_pool(x).view(batch_size, channels))\n",
    "        max_out = self.fc(self.max_pool(x).view(batch_size, channels))\n",
    "        \n",
    "        # Combine and apply attention\n",
    "        attention = (avg_out + max_out).view(batch_size, channels, 1, 1)\n",
    "        return x * attention\n",
    "\n",
    "\n",
    "class ImprovedSpectrogramGenerator(nn.Module):\n",
    "    \"\"\"Enhanced Conditional GAN Generator with Channel Attention (memory-efficient)\"\"\"\n",
    "    def __init__(self, latent_dim=LATENT_DIM, condition_dim=CONDITION_DIM, \n",
    "                 n_mels=N_MELS, time_steps=1292):\n",
    "        super(ImprovedSpectrogramGenerator, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.condition_dim = condition_dim\n",
    "        self.n_mels = n_mels\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        # Improved condition embedding\n",
    "        self.condition_embed = nn.Sequential(\n",
    "            nn.Linear(condition_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # Initial projection with condition\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 256, 256 * 16 * 20),\n",
    "            nn.BatchNorm1d(256 * 16 * 20),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # Convolutional upsampling with channel attention\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # Channel attention module (memory-efficient)\n",
    "        self.attention = ChannelAttention(64, reduction=8)\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=(1, 8), stride=(1, 8), padding=0),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, c):\n",
    "        # Embed condition\n",
    "        c_embed = self.condition_embed(c)\n",
    "        \n",
    "        # Concatenate noise and embedded condition\n",
    "        x = torch.cat([z, c_embed], dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 256, 16, 20)\n",
    "        \n",
    "        # Upsampling with attention\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.attention(x)  # Apply channel attention (much more memory efficient)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        # Ensure correct output size\n",
    "        if x.shape[-1] != self.time_steps or x.shape[-2] != self.n_mels:\n",
    "            x = F.interpolate(x, size=(self.n_mels, self.time_steps), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class ImprovedSpectrogramDiscriminator(nn.Module):\n",
    "    \"\"\"Enhanced Conditional GAN Discriminator with Spectral Normalization\"\"\"\n",
    "    def __init__(self, condition_dim=CONDITION_DIM, n_mels=N_MELS, time_steps=1292):\n",
    "        super(ImprovedSpectrogramDiscriminator, self).__init__()\n",
    "        \n",
    "        self.n_mels = n_mels\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        # Simplified condition embedding (reduce memory)\n",
    "        self.condition_embed = nn.Sequential(\n",
    "            nn.Linear(condition_dim, 64),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # Convolutional layers with spectral normalization for stability\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.utils.spectral_norm(nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.utils.spectral_norm(nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.utils.spectral_norm(nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        conv_output_size = 256 * 8 * 80\n",
    "        \n",
    "        # Fully connected layers with dropout and condition\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_output_size + 64, 256),  # Concatenate with condition\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "            # No sigmoid - will use BCEWithLogitsLoss for better stability\n",
    "        )\n",
    "        \n",
    "    def forward(self, spec, c):\n",
    "        # Embed condition (reduced dimensionality for memory)\n",
    "        batch_size = spec.size(0)\n",
    "        c_embed = self.condition_embed(c)\n",
    "        \n",
    "        # Apply conv layers\n",
    "        features = self.conv_layers(spec)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # Concatenate with condition and classify\n",
    "        x = torch.cat([features, c_embed], dim=1)\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Initialize improved GAN models\n",
    "time_steps = real_spectrograms.shape[2]\n",
    "generator = ImprovedSpectrogramGenerator(\n",
    "    latent_dim=LATENT_DIM, \n",
    "    condition_dim=CONDITION_DIM, \n",
    "    n_mels=N_MELS, \n",
    "    time_steps=time_steps\n",
    ").to(DEVICE)\n",
    "\n",
    "discriminator = ImprovedSpectrogramDiscriminator(\n",
    "    condition_dim=CONDITION_DIM, \n",
    "    n_mels=N_MELS, \n",
    "    time_steps=time_steps\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üé® IMPROVED GAN ARCHITECTURE (Memory-Efficient)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚ú® Generator Features:\")\n",
    "print(f\"   - Channel attention (memory-efficient)\")\n",
    "print(f\"   - Enhanced condition embedding\")\n",
    "print(f\"   - Parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "print(f\"\\n‚ú® Discriminator Features:\")\n",
    "print(f\"   - Spectral normalization for stability\")\n",
    "print(f\"   - Compact condition embedding\")\n",
    "print(f\"   - Parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clear cache after model initialization\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"üíæ GPU memory after models: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d9314",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Train Conditional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a435ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéÆ BALANCED GAN TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# GAN Training Hyperparameters\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))  # Lower LR for discriminator\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Adaptive training parameters\n",
    "D_STEPS_THRESHOLD = 0.8  # Train discriminator more if accuracy < 80%\n",
    "G_STEPS_THRESHOLD = 0.2  # Train generator more if discriminator accuracy > 80%\n",
    "\n",
    "print(f\"‚öôÔ∏è  Optimizer Configuration:\")\n",
    "print(f\"   Generator LR: {0.0002} (Adam, Œ≤1=0.5, Œ≤2=0.999)\")\n",
    "print(f\"   Discriminator LR: {0.0001} (Adam, Œ≤1=0.5, Œ≤2=0.999)\")\n",
    "print(f\"   Loss Function: BCEWithLogitsLoss\")\n",
    "print(f\"\\nüéØ Adaptive Training:\")\n",
    "print(f\"   D steps if D_acc < 80%: 1-2 steps\")\n",
    "print(f\"   G steps if D_acc > 80%: 2-3 steps\")\n",
    "print(f\"   Gradient clipping: max_norm = 1.0\")\n",
    "print(f\"\\nüíæ Memory Optimization:\")\n",
    "print(f\"   Gradient accumulation: 2 steps (effective batch = {GAN_BATCH_SIZE * 2})\")\n",
    "print(f\"   Pin memory: enabled\")\n",
    "print(f\"   Periodic cache clearing: every 5 batches\")\n",
    "print(f\"   Data on CPU, batch transfer to GPU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract conditions from real labels (valence, arousal)\n",
    "real_conditions = real_labels.copy()  # Shape: (N, 2) - valence and arousal\n",
    "print(f\"\\nüìä Data prepared for GAN training:\")\n",
    "print(f\"   Real spectrograms: {real_spectrograms.shape}\")\n",
    "print(f\"   Real conditions: {real_conditions.shape}\")\n",
    "print(f\"   Condition range: [{real_conditions.min():.2f}, {real_conditions.max():.2f}]\")\n",
    "\n",
    "# Create memory-efficient DataLoader (data on CPU, transfer batches to GPU)\n",
    "real_specs_tensor = torch.FloatTensor(real_spectrograms).unsqueeze(1)  # Keep on CPU\n",
    "real_conditions_tensor = torch.FloatTensor(real_conditions)  # Keep on CPU\n",
    "\n",
    "gan_dataset = torch.utils.data.TensorDataset(real_specs_tensor, real_conditions_tensor)\n",
    "gan_loader = torch.utils.data.DataLoader(\n",
    "    gan_dataset, \n",
    "    batch_size=GAN_BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    pin_memory=True,  # Fast CPU->GPU transfer\n",
    "    num_workers=0  # Avoid multiprocessing overhead\n",
    ")\n",
    "\n",
    "# Clear cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"üßπ Cleared GPU cache\")\n",
    "    print(f\"üíæ Initial GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüöÄ Starting Balanced GAN Training...\\n\")\n",
    "\n",
    "# Training loop\n",
    "gan_losses = {'g_loss': [], 'd_loss': [], 'd_real_acc': [], 'd_fake_acc': []}\n",
    "GRADIENT_ACCUMULATION_STEPS = 2  # Effective batch size = GAN_BATCH_SIZE * 2\n",
    "\n",
    "for epoch in range(GAN_EPOCHS):\n",
    "    epoch_g_loss = 0\n",
    "    epoch_d_loss = 0\n",
    "    d_real_correct = 0\n",
    "    d_fake_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Reset gradient accumulation\n",
    "    g_optimizer.zero_grad()\n",
    "    d_optimizer.zero_grad()\n",
    "    \n",
    "    for i, (real_specs, conditions) in enumerate(tqdm(gan_loader, desc=f\"Epoch {epoch+1}/{GAN_EPOCHS}\")):\n",
    "        # Move batch to GPU (lazy loading)\n",
    "        real_specs = real_specs.to(DEVICE)\n",
    "        conditions = conditions.to(DEVICE)\n",
    "        batch_size = real_specs.size(0)\n",
    "        \n",
    "        # Labels\n",
    "        real_labels = torch.ones(batch_size, 1).to(DEVICE)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(DEVICE)\n",
    "        \n",
    "        # ========== Train Discriminator ==========\n",
    "        # Calculate discriminator accuracy for adaptive training\n",
    "        with torch.no_grad():\n",
    "            z_temp = torch.randn(batch_size, LATENT_DIM).to(DEVICE)\n",
    "            fake_specs_temp = generator(z_temp, conditions)\n",
    "            d_real_out = discriminator(real_specs, conditions)\n",
    "            d_fake_out = discriminator(fake_specs_temp, conditions)\n",
    "            \n",
    "            d_real_acc = ((torch.sigmoid(d_real_out) > 0.5).float().mean()).item()\n",
    "            d_fake_acc = ((torch.sigmoid(d_fake_out) < 0.5).float().mean()).item()\n",
    "            \n",
    "        # Adaptive discriminator steps\n",
    "        d_steps = 1 if d_real_acc > D_STEPS_THRESHOLD and d_fake_acc > D_STEPS_THRESHOLD else 2\n",
    "        \n",
    "        for _ in range(d_steps):\n",
    "            # Real spectrograms\n",
    "            real_output = discriminator(real_specs, conditions)\n",
    "            d_real_loss = criterion(real_output, real_labels)\n",
    "            \n",
    "            # Fake spectrograms\n",
    "            z = torch.randn(batch_size, LATENT_DIM).to(DEVICE)\n",
    "            fake_specs = generator(z, conditions).detach()\n",
    "            fake_output = discriminator(fake_specs, conditions)\n",
    "            d_fake_loss = criterion(fake_output, fake_labels)\n",
    "            \n",
    "            # Total discriminator loss (scaled for gradient accumulation)\n",
    "            d_loss = (d_real_loss + d_fake_loss) / (2 * GRADIENT_ACCUMULATION_STEPS)\n",
    "            d_loss.backward()\n",
    "            \n",
    "            # Update discriminator (every GRADIENT_ACCUMULATION_STEPS batches)\n",
    "            if (i + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "                d_optimizer.step()\n",
    "                d_optimizer.zero_grad()\n",
    "        \n",
    "        # ========== Train Generator ==========\n",
    "        # Adaptive generator steps\n",
    "        g_steps = 3 if d_real_acc > D_STEPS_THRESHOLD else 1\n",
    "        \n",
    "        for _ in range(g_steps):\n",
    "            z = torch.randn(batch_size, LATENT_DIM).to(DEVICE)\n",
    "            fake_specs = generator(z, conditions)\n",
    "            fake_output = discriminator(fake_specs, conditions)\n",
    "            \n",
    "            # Generator loss (scaled for gradient accumulation)\n",
    "            g_loss = criterion(fake_output, real_labels) / GRADIENT_ACCUMULATION_STEPS\n",
    "            g_loss.backward()\n",
    "            \n",
    "            # Update generator (every GRADIENT_ACCUMULATION_STEPS batches)\n",
    "            if (i + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "                g_optimizer.step()\n",
    "                g_optimizer.zero_grad()\n",
    "        \n",
    "        # Track losses and accuracy\n",
    "        epoch_g_loss += g_loss.item() * GRADIENT_ACCUMULATION_STEPS * g_steps\n",
    "        epoch_d_loss += d_loss.item() * GRADIENT_ACCUMULATION_STEPS * 2 * d_steps\n",
    "        d_real_correct += d_real_acc * batch_size\n",
    "        d_fake_correct += d_fake_acc * batch_size\n",
    "        total_samples += batch_size\n",
    "        \n",
    "        # Periodic cache clearing (every 5 batches)\n",
    "        if i % 5 == 0 and i > 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Epoch statistics\n",
    "    avg_g_loss = epoch_g_loss / len(gan_loader)\n",
    "    avg_d_loss = epoch_d_loss / len(gan_loader)\n",
    "    avg_d_real_acc = d_real_correct / total_samples\n",
    "    avg_d_fake_acc = d_fake_correct / total_samples\n",
    "    \n",
    "    gan_losses['g_loss'].append(avg_g_loss)\n",
    "    gan_losses['d_loss'].append(avg_d_loss)\n",
    "    gan_losses['d_real_acc'].append(avg_d_real_acc)\n",
    "    gan_losses['d_fake_acc'].append(avg_d_fake_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{GAN_EPOCHS}]\")\n",
    "    print(f\"  G Loss: {avg_g_loss:.4f} | D Loss: {avg_d_loss:.4f}\")\n",
    "    print(f\"  D Real Acc: {avg_d_real_acc:.2%} | D Fake Acc: {avg_d_fake_acc:.2%}\")\n",
    "    \n",
    "    # GPU memory monitoring\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  üíæ GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "        if (epoch + 1) % 3 == 0:  # Deep clean every 3 epochs\n",
    "            torch.cuda.empty_cache()\n",
    "    print()\n",
    "\n",
    "print(\"\\n‚úÖ GAN Training Complete!\")\n",
    "print(f\"Final Generator Loss: {gan_losses['g_loss'][-1]:.4f}\")\n",
    "print(f\"Final Discriminator Loss: {gan_losses['d_loss'][-1]:.4f}\")\n",
    "print(f\"Final D Real Accuracy: {gan_losses['d_real_acc'][-1]:.2%}\")\n",
    "print(f\"Final D Fake Accuracy: {gan_losses['d_fake_acc'][-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02b793",
   "metadata": {},
   "source": [
    "## 5.5Ô∏è‚É£ GAN Quality Metrics (Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "\n",
    "def calculate_statistics(spectrograms):\n",
    "    \"\"\"Calculate mean and covariance of spectrograms (FID-style)\"\"\"\n",
    "    # Flatten spectrograms\n",
    "    specs_flat = spectrograms.reshape(spectrograms.shape[0], -1)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mu = np.mean(specs_flat, axis=0)\n",
    "    sigma = np.cov(specs_flat, rowvar=False)\n",
    "    \n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate Frechet Distance (similar to FID score).\n",
    "    Lower is better - indicates generated data is closer to real data.\n",
    "    \"\"\"\n",
    "    # Calculate mean difference\n",
    "    diff = mu1 - mu2\n",
    "    \n",
    "    # Product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    \n",
    "    # Numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    # Calculate FD\n",
    "    fd = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    \n",
    "    return fd\n",
    "\n",
    "\n",
    "def evaluate_spectrogram_quality(real_specs, fake_specs, n_samples=500):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of generated spectrogram quality.\n",
    "    \n",
    "    Returns various metrics comparing real vs synthetic spectrograms.\n",
    "    \"\"\"\n",
    "    print(\"üìä Evaluating GAN Generation Quality...\\n\")\n",
    "    \n",
    "    # Subsample for efficiency\n",
    "    n_samples = min(n_samples, len(real_specs), len(fake_specs))\n",
    "    real_sample = real_specs[:n_samples]\n",
    "    fake_sample = fake_specs[:n_samples]\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Frechet Distance (FID-style)\n",
    "    print(\"  üî¢ Computing Frechet Distance...\")\n",
    "    mu_real, sigma_real = calculate_statistics(real_sample)\n",
    "    mu_fake, sigma_fake = calculate_statistics(fake_sample)\n",
    "    fd = calculate_frechet_distance(mu_real, sigma_real, mu_fake, sigma_fake)\n",
    "    metrics['frechet_distance'] = fd\n",
    "    print(f\"     Frechet Distance: {fd:.4f} (lower is better)\")\n",
    "    \n",
    "    # 2. Statistical moments comparison\n",
    "    print(\"\\n  üìà Computing statistical moments...\")\n",
    "    real_mean = np.mean(real_sample)\n",
    "    fake_mean = np.mean(fake_sample)\n",
    "    real_std = np.std(real_sample)\n",
    "    fake_std = np.std(fake_sample)\n",
    "    \n",
    "    metrics['mean_diff'] = abs(real_mean - fake_mean)\n",
    "    metrics['std_diff'] = abs(real_std - fake_std)\n",
    "    \n",
    "    print(f\"     Mean - Real: {real_mean:.4f}, Fake: {fake_mean:.4f}, Diff: {metrics['mean_diff']:.4f}\")\n",
    "    print(f\"     Std  - Real: {real_std:.4f}, Fake: {fake_std:.4f}, Diff: {metrics['std_diff']:.4f}\")\n",
    "    \n",
    "    # 3. Spectrogram smoothness (measure of noise)\n",
    "    print(\"\\n  üé® Evaluating smoothness (temporal consistency)...\")\n",
    "    real_smoothness = np.mean([np.mean(np.abs(np.diff(spec, axis=1))) for spec in real_sample])\n",
    "    fake_smoothness = np.mean([np.mean(np.abs(np.diff(spec, axis=1))) for spec in fake_sample])\n",
    "    \n",
    "    metrics['real_smoothness'] = real_smoothness\n",
    "    metrics['fake_smoothness'] = fake_smoothness\n",
    "    metrics['smoothness_ratio'] = fake_smoothness / (real_smoothness + 1e-8)\n",
    "    \n",
    "    print(f\"     Real smoothness: {real_smoothness:.4f}\")\n",
    "    print(f\"     Fake smoothness: {fake_smoothness:.4f}\")\n",
    "    print(f\"     Ratio: {metrics['smoothness_ratio']:.4f} (closer to 1.0 is better)\")\n",
    "    \n",
    "    # 4. Frequency distribution analysis\n",
    "    print(\"\\n  üéµ Analyzing frequency content...\")\n",
    "    real_freq_mean = np.mean(real_sample, axis=(0, 2))  # Average across batch and time\n",
    "    fake_freq_mean = np.mean(fake_sample, axis=(0, 2))\n",
    "    \n",
    "    freq_correlation = np.corrcoef(real_freq_mean, fake_freq_mean)[0, 1]\n",
    "    metrics['frequency_correlation'] = freq_correlation\n",
    "    \n",
    "    print(f\"     Frequency correlation: {freq_correlation:.4f} (higher is better)\")\n",
    "    \n",
    "    # 5. Dynamic range\n",
    "    print(\"\\n  üìä Comparing dynamic range...\")\n",
    "    real_range = np.max(real_sample) - np.min(real_sample)\n",
    "    fake_range = np.max(fake_sample) - np.min(fake_sample)\n",
    "    \n",
    "    metrics['real_range'] = real_range\n",
    "    metrics['fake_range'] = fake_range\n",
    "    metrics['range_diff'] = abs(real_range - fake_range)\n",
    "    \n",
    "    print(f\"     Real range: {real_range:.4f}\")\n",
    "    print(f\"     Fake range: {fake_range:.4f}\")\n",
    "    print(f\"     Difference: {metrics['range_diff']:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def visualize_quality_comparison(real_specs, fake_specs, metrics, n_visual=3):\n",
    "    \"\"\"Visualize quality comparison between real and synthetic spectrograms.\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Top row: Sample spectrograms\n",
    "    for i in range(n_visual):\n",
    "        # Real spectrograms\n",
    "        ax = fig.add_subplot(gs[0, i])\n",
    "        ax.imshow(real_specs[i], aspect='auto', origin='lower', cmap='viridis')\n",
    "        ax.set_title(f'Real Sample {i+1}')\n",
    "        ax.set_xlabel('Time')\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Mel Frequency')\n",
    "        \n",
    "        # Fake spectrograms\n",
    "        ax = fig.add_subplot(gs[1, i])\n",
    "        ax.imshow(fake_specs[i], aspect='auto', origin='lower', cmap='viridis')\n",
    "        ax.set_title(f'Generated Sample {i+1}')\n",
    "        ax.set_xlabel('Time')\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Mel Frequency')\n",
    "    \n",
    "    # Middle row: Metrics visualization\n",
    "    ax = fig.add_subplot(gs[2, :])\n",
    "    metric_names = ['Frechet\\nDistance', 'Frequency\\nCorrelation', 'Smoothness\\nRatio']\n",
    "    metric_values = [\n",
    "        metrics['frechet_distance'],\n",
    "        metrics['frequency_correlation'],\n",
    "        metrics['smoothness_ratio']\n",
    "    ]\n",
    "    colors = ['#e74c3c' if v > 10 else '#3498db' if v > 5 else '#2ecc71' \n",
    "              for v in [metrics['frechet_distance'], \n",
    "                       1-metrics['frequency_correlation'], \n",
    "                       abs(1-metrics['smoothness_ratio'])]]\n",
    "    \n",
    "    bars = ax.bar(metric_names, metric_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_ylabel('Metric Value')\n",
    "    ax.set_title('GAN Quality Metrics')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{value:.3f}',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Bottom row: Temporal evolution comparison\n",
    "    ax = fig.add_subplot(gs[3, :])\n",
    "    real_temporal = np.mean(real_specs[:50], axis=(0, 1))\n",
    "    fake_temporal = np.mean(fake_specs[:50], axis=(0, 1))\n",
    "    ax.plot(real_temporal, label='Real', linewidth=2, alpha=0.8)\n",
    "    ax.plot(fake_temporal, label='Synthetic', linewidth=2, alpha=0.8)\n",
    "    ax.set_xlabel('Time Frame')\n",
    "    ax.set_ylabel('Average Amplitude')\n",
    "    ax.set_title('Temporal Evolution Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'gan_quality_evaluation.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Quality evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbbb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üé® Generating {NUM_SYNTHETIC} synthetic spectrograms...\\n\")\n",
    "\n",
    "generator.eval()\n",
    "synthetic_spectrograms = []\n",
    "synthetic_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    num_batches = NUM_SYNTHETIC // GAN_BATCH_SIZE\n",
    "    \n",
    "    for i in tqdm(range(num_batches), desc=\"Generating\"):\n",
    "        z = torch.randn(GAN_BATCH_SIZE, LATENT_DIM).to(DEVICE)\n",
    "        random_conditions = torch.FloatTensor(GAN_BATCH_SIZE, 2).uniform_(-1, 1).to(DEVICE)\n",
    "        \n",
    "        fake_specs = generator(z, random_conditions)\n",
    "        \n",
    "        synthetic_spectrograms.append(fake_specs.cpu().numpy())\n",
    "        synthetic_labels.append(random_conditions.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "synthetic_spectrograms = np.concatenate(synthetic_spectrograms, axis=0)\n",
    "synthetic_labels = np.concatenate(synthetic_labels, axis=0)\n",
    "\n",
    "# Remove channel dimension\n",
    "synthetic_spectrograms = synthetic_spectrograms.squeeze(1)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(synthetic_spectrograms)} synthetic spectrograms\")\n",
    "print(f\"Synthetic spectrogram shape: {synthetic_spectrograms.shape}\")\n",
    "print(f\"Synthetic labels shape: {synthetic_labels.shape}\")\n",
    "\n",
    "# ========== ROBUST DATA VALIDATION & CONVERSION ==========\n",
    "print(f\"\\nüîç Validating and preparing label data...\")\n",
    "\n",
    "def prepare_labels(labels, name=\"labels\"):\n",
    "    \"\"\"\n",
    "    Robust label preparation function that handles all edge cases.\n",
    "    Ensures output is numpy array with shape (N, 2).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Convert to numpy if tensor\n",
    "        if torch.is_tensor(labels):\n",
    "            print(f\"  - {name} is a tensor, converting to numpy...\")\n",
    "            if labels.is_cuda:\n",
    "                labels_np = labels.cpu().numpy()\n",
    "            else:\n",
    "                labels_np = labels.numpy()\n",
    "        else:\n",
    "            labels_np = np.array(labels)\n",
    "        \n",
    "        print(f\"  - {name} shape after conversion: {labels_np.shape}\")\n",
    "        \n",
    "        # Step 2: Handle shape issues\n",
    "        if len(labels_np.shape) == 1:\n",
    "            # 1D array - reshape to (N, 2)\n",
    "            print(f\"  - {name} is 1D, reshaping to (-1, 2)...\")\n",
    "            labels_np = labels_np.reshape(-1, 2)\n",
    "        elif len(labels_np.shape) == 2:\n",
    "            # 2D array - check if transposed\n",
    "            if labels_np.shape[0] == 2 and labels_np.shape[1] > 2:\n",
    "                # Likely transposed (2, N) -> (N, 2)\n",
    "                print(f\"  - {name} appears transposed {labels_np.shape}, fixing...\")\n",
    "                labels_np = labels_np.T\n",
    "            elif labels_np.shape[1] == 1:\n",
    "                # Shape is (N, 1) - might need to be (N//2, 2)\n",
    "                print(f\"  - {name} has shape {labels_np.shape}, reshaping...\")\n",
    "                labels_np = labels_np.reshape(-1, 2)\n",
    "        \n",
    "        # Step 3: Final validation\n",
    "        if labels_np.shape[1] != 2:\n",
    "            print(f\"  ‚ö†Ô∏è WARNING: {name} has unexpected shape {labels_np.shape}\")\n",
    "            print(f\"  Attempting to force reshape to (-1, 2)...\")\n",
    "            labels_np = labels_np.reshape(-1, 2)\n",
    "        \n",
    "        print(f\"  ‚úÖ {name} final shape: {labels_np.shape}\")\n",
    "        return labels_np\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå ERROR processing {name}: {e}\")\n",
    "        print(f\"  Returning original data as-is\")\n",
    "        return labels if not torch.is_tensor(labels) else labels.cpu().numpy()\n",
    "\n",
    "# Prepare real labels\n",
    "real_labels_np = prepare_labels(real_labels, \"real_labels\")\n",
    "\n",
    "# Prepare synthetic labels\n",
    "synthetic_labels = prepare_labels(synthetic_labels, \"synthetic_labels\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data preparation complete!\")\n",
    "print(f\"   Real labels: {real_labels_np.shape}\")\n",
    "print(f\"   Synthetic labels: {synthetic_labels.shape}\")\n",
    "\n",
    "# ========== VISUALIZATION WITH ERROR HANDLING ==========\n",
    "try:\n",
    "    print(f\"\\nüìä Creating visualizations...\")\n",
    "    \n",
    "    # Visualize synthetic vs real spectrograms\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    for i in range(3):\n",
    "        try:\n",
    "            axes[0, i].imshow(real_spectrograms[i], aspect='auto', origin='lower', cmap='viridis')\n",
    "            # Robust label access\n",
    "            v_real = real_labels_np[i, 0] if real_labels_np.shape[1] >= 1 else 0\n",
    "            a_real = real_labels_np[i, 1] if real_labels_np.shape[1] >= 2 else 0\n",
    "            axes[0, i].set_title(f'Real Spec {i+1}\\nV: {v_real:.2f}, A: {a_real:.2f}')\n",
    "            axes[0, i].set_xlabel('Time')\n",
    "            axes[0, i].set_ylabel('Mel Bins')\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Could not plot real spectrogram {i}: {e}\")\n",
    "            axes[0, i].text(0.5, 0.5, f'Error\\n{str(e)[:30]}', \n",
    "                          ha='center', va='center', transform=axes[0, i].transAxes)\n",
    "    \n",
    "    for i in range(3):\n",
    "        try:\n",
    "            axes[1, i].imshow(synthetic_spectrograms[i], aspect='auto', origin='lower', cmap='viridis')\n",
    "            # Robust label access\n",
    "            v_syn = synthetic_labels[i, 0] if synthetic_labels.shape[1] >= 1 else 0\n",
    "            a_syn = synthetic_labels[i, 1] if synthetic_labels.shape[1] >= 2 else 0\n",
    "            axes[1, i].set_title(f'Synthetic Spec {i+1}\\nV: {v_syn:.2f}, A: {a_syn:.2f}')\n",
    "            axes[1, i].set_xlabel('Time')\n",
    "            axes[1, i].set_ylabel('Mel Bins')\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Could not plot synthetic spectrogram {i}: {e}\")\n",
    "            axes[1, i].text(0.5, 0.5, f'Error\\n{str(e)[:30]}', \n",
    "                          ha='center', va='center', transform=axes[1, i].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'real_vs_synthetic.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"  ‚úÖ Spectrogram comparison plot saved\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Error creating spectrogram comparison plot: {e}\")\n",
    "    print(\"  Continuing execution...\")\n",
    "\n",
    "# ========== DISTRIBUTION COMPARISON WITH ERROR HANDLING ==========\n",
    "try:\n",
    "    print(f\"\\nüìà Creating distribution plots...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    try:\n",
    "        axes[0].scatter(real_labels_np[:, 0], real_labels_np[:, 1], \n",
    "                       alpha=0.5, label='Real', s=20)\n",
    "        axes[0].scatter(synthetic_labels[:, 0], synthetic_labels[:, 1], \n",
    "                       alpha=0.3, label='Synthetic', s=20)\n",
    "        axes[0].set_xlabel('Valence')\n",
    "        axes[0].set_ylabel('Arousal')\n",
    "        axes[0].set_title('Valence-Arousal Distribution')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].axhline(0, color='k', linewidth=0.5)\n",
    "        axes[0].axvline(0, color='k', linewidth=0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Warning: Could not create scatter plot: {e}\")\n",
    "        axes[0].text(0.5, 0.5, f'Scatter plot error\\n{str(e)[:30]}', \n",
    "                    ha='center', va='center', transform=axes[0].transAxes)\n",
    "    \n",
    "    # Bar plot\n",
    "    try:\n",
    "        sizes = [len(real_spectrograms), len(synthetic_spectrograms), \n",
    "                 len(real_spectrograms) + len(synthetic_spectrograms)]\n",
    "        labels = ['Real', 'Synthetic', 'Total']\n",
    "        colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "        axes[1].bar(labels, sizes, color=colors, alpha=0.7, edgecolor='black')\n",
    "        axes[1].set_ylabel('Number of Samples')\n",
    "        axes[1].set_title('Dataset Size Comparison')\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        for i, v in enumerate(sizes):\n",
    "            axes[1].text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Warning: Could not create bar plot: {e}\")\n",
    "        axes[1].text(0.5, 0.5, f'Bar plot error\\n{str(e)[:30]}', \n",
    "                    ha='center', va='center', transform=axes[1].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'augmented_dataset_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"  ‚úÖ Distribution comparison plot saved\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Error creating distribution plots: {e}\")\n",
    "    print(\"  Continuing execution...\")\n",
    "\n",
    "# ========== STATISTICS ==========\n",
    "try:\n",
    "    print(f\"\\nüìä Dataset Statistics:\")\n",
    "    print(f\"  - Real samples: {len(real_spectrograms)}\")\n",
    "    print(f\"  - Synthetic samples: {len(synthetic_spectrograms)}\")\n",
    "    print(f\"  - Total samples: {len(real_spectrograms) + len(synthetic_spectrograms)}\")\n",
    "    aug_factor = (len(real_spectrograms) + len(synthetic_spectrograms)) / len(real_spectrograms)\n",
    "    print(f\"  - Augmentation factor: {aug_factor:.2f}x\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è Warning: Could not compute statistics: {e}\")\n",
    "\n",
    "# ========== UPDATE GLOBAL VARIABLES ==========\n",
    "# Ensure real_labels is numpy array for downstream use\n",
    "real_labels = real_labels_np\n",
    "\n",
    "print(f\"\\n‚úÖ Synthetic data generation and visualization complete!\")\n",
    "print(f\"   Proceeding to next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2244e5a0",
   "metadata": {},
   "source": [
    "## 6.2Ô∏è‚É£ Evaluate GAN Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6303bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GAN quality evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî¨ GAN QUALITY EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate quality\n",
    "quality_metrics = evaluate_spectrogram_quality(\n",
    "    real_spectrograms[:500], \n",
    "    synthetic_spectrograms[:500]\n",
    ")\n",
    "\n",
    "# Visualize comparison\n",
    "visualize_quality_comparison(\n",
    "    real_spectrograms[:10], \n",
    "    synthetic_spectrograms[:10],\n",
    "    quality_metrics\n",
    ")\n",
    "\n",
    "# Overall quality score\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ OVERALL QUALITY ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute composite quality score (0-100)\n",
    "fd_score = max(0, 100 - quality_metrics['frechet_distance'] * 10)  # Lower FD is better\n",
    "freq_score = quality_metrics['frequency_correlation'] * 100\n",
    "smooth_score = max(0, 100 - abs(1.0 - quality_metrics['smoothness_ratio']) * 100)\n",
    "\n",
    "overall_score = (fd_score * 0.4 + freq_score * 0.4 + smooth_score * 0.2)\n",
    "\n",
    "print(f\"  Frechet Distance Score: {fd_score:.1f}/100\")\n",
    "print(f\"  Frequency Correlation Score: {freq_score:.1f}/100\")\n",
    "print(f\"  Smoothness Score: {smooth_score:.1f}/100\")\n",
    "print(f\"\\n  üìä Overall GAN Quality Score: {overall_score:.1f}/100\")\n",
    "\n",
    "if overall_score >= 70:\n",
    "    print(\"  ‚úÖ Excellent - GAN generates high-quality spectrograms\")\n",
    "elif overall_score >= 50:\n",
    "    print(\"  ‚ö†Ô∏è Good - GAN output is acceptable but could be improved\")\n",
    "else:\n",
    "    print(\"  ‚ùå Poor - GAN needs significant improvement (mostly noise)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cdcb4f",
   "metadata": {},
   "source": [
    "## 6.5Ô∏è‚É£ Audio Reconstruction - Listen to GAN Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a623ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "import soundfile as sf\n",
    "\n",
    "def spectrogram_to_audio(spec_normalized, sample_rate=SAMPLE_RATE, n_fft=N_FFT, \n",
    "                         hop_length=HOP_LENGTH, n_iter=32):\n",
    "    \"\"\"\n",
    "    Convert normalized mel spectrogram back to audio using Griffin-Lim algorithm.\n",
    "    \n",
    "    Args:\n",
    "        spec_normalized: Normalized spectrogram in range [-1, 1]\n",
    "        sample_rate: Audio sample rate\n",
    "        n_fft: FFT window size\n",
    "        hop_length: Hop length for STFT\n",
    "        n_iter: Number of Griffin-Lim iterations\n",
    "    \n",
    "    Returns:\n",
    "        audio: Reconstructed audio signal\n",
    "    \"\"\"\n",
    "    # Denormalize spectrogram\n",
    "    spec_db = spec_normalized * 40.0  # Approximate dB range\n",
    "    \n",
    "    # Convert from dB to power\n",
    "    spec_power = librosa.db_to_amplitude(spec_db)\n",
    "    \n",
    "    # Reconstruct audio using Griffin-Lim\n",
    "    audio = librosa.feature.inverse.mel_to_audio(\n",
    "        spec_power,\n",
    "        sr=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_iter=n_iter,\n",
    "        fmin=FMIN,\n",
    "        fmax=FMAX\n",
    "    )\n",
    "    \n",
    "    return audio\n",
    "\n",
    "\n",
    "def generate_and_listen_to_samples(generator, n_samples=5, emotions=None, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Generate synthetic spectrograms and convert them to audio for listening.\n",
    "    \n",
    "    Args:\n",
    "        generator: Trained GAN generator\n",
    "        n_samples: Number of samples to generate\n",
    "        emotions: List of (valence, arousal) tuples, or None for random\n",
    "        device: Torch device\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    print(f\"üéµ Generating {n_samples} audio samples from GAN...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(n_samples):\n",
    "            # Generate noise\n",
    "            z = torch.randn(1, LATENT_DIM).to(device)\n",
    "            \n",
    "            # Use provided emotions or generate random\n",
    "            if emotions and i < len(emotions):\n",
    "                valence, arousal = emotions[i]\n",
    "            else:\n",
    "                valence = np.random.uniform(-1, 1)\n",
    "                arousal = np.random.uniform(-1, 1)\n",
    "            \n",
    "            condition = torch.FloatTensor([[valence, arousal]]).to(device)\n",
    "            \n",
    "            # Generate spectrogram\n",
    "            fake_spec = generator(z, condition)\n",
    "            fake_spec_np = fake_spec.squeeze().cpu().numpy()\n",
    "            \n",
    "            # Convert to audio\n",
    "            print(f\"\\\\nüéß Sample {i+1}: Valence={valence:.2f}, Arousal={arousal:.2f}\")\n",
    "            audio = spectrogram_to_audio(fake_spec_np)\n",
    "            \n",
    "            # Normalize audio\n",
    "            audio = audio / (np.max(np.abs(audio)) + 1e-8) * 0.9\n",
    "            \n",
    "            # Save audio file\n",
    "            audio_path = os.path.join(OUTPUT_DIR, f'generated_sample_{i+1}_v{valence:.2f}_a{arousal:.2f}.wav')\n",
    "            sf.write(audio_path, audio, SAMPLE_RATE)\n",
    "            print(f\"   üíæ Saved: {audio_path}\")\n",
    "            \n",
    "            # Display audio player\n",
    "            display(Audio(audio, rate=SAMPLE_RATE))\n",
    "            \n",
    "            # Visualize spectrogram\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            librosa.display.specshow(\n",
    "                fake_spec_np,\n",
    "                sr=SAMPLE_RATE,\n",
    "                hop_length=HOP_LENGTH,\n",
    "                x_axis='time',\n",
    "                y_axis='mel',\n",
    "                fmax=FMAX,\n",
    "                cmap='viridis'\n",
    "            )\n",
    "            plt.colorbar(format='%+2.0f dB')\n",
    "            plt.title(f'Generated Spectrogram {i+1}\\\\nValence: {valence:.2f}, Arousal: {arousal:.2f}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(OUTPUT_DIR, f'generated_spec_{i+1}.png'), dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Define emotion targets to test\n",
    "test_emotions = [\n",
    "    (-0.8, -0.6),  # Sad, calm\n",
    "    (0.8, 0.7),    # Happy, energetic\n",
    "    (-0.3, 0.8),   # Angry, tense\n",
    "    (0.5, -0.5),   # Content, relaxed\n",
    "    (0.0, 0.0),    # Neutral\n",
    "]\n",
    "\n",
    "print(\"üé® Generating audio from synthetic spectrograms...\")\n",
    "print(\"This allows you to qualitatively assess GAN generation quality.\\\\n\")\n",
    "\n",
    "generate_and_listen_to_samples(\n",
    "    generator, \n",
    "    n_samples=5, \n",
    "    emotions=test_emotions,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Audio generation complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"üí° Tips for evaluation:\")\n",
    "print(\"  - Listen for musical structure vs pure noise\")\n",
    "print(\"  - Check if emotion patterns are perceptible\")\n",
    "print(\"  - Compare across different valence/arousal values\")\n",
    "print(\"  - Real music should have harmonic and temporal patterns\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d810471d",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ ViT Model Definition with Advanced Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb98d0e",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Prepare Augmented Dataset for ViT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine real and synthetic spectrograms\n",
    "print(\"üîÑ Preparing augmented dataset for ViT training...\")\n",
    "\n",
    "all_spectrograms = np.concatenate([real_spectrograms, synthetic_spectrograms], axis=0)\n",
    "all_labels = np.concatenate([real_labels, synthetic_labels], axis=0)\n",
    "\n",
    "print(f\"‚úÖ Total augmented dataset:\")\n",
    "print(f\"   - Total samples: {len(all_spectrograms)}\")\n",
    "print(f\"   - Shape: {all_spectrograms.shape}\")\n",
    "print(f\"   - Labels shape: {all_labels.shape}\")\n",
    "\n",
    "\n",
    "class SpectrogramDataset(Dataset):\n",
    "    \"\"\"Dataset for mel-spectrograms with ViT preprocessing.\"\"\"\n",
    "    \n",
    "    def __init__(self, spectrograms, labels, image_size=VIT_IMAGE_SIZE):\n",
    "        self.spectrograms = spectrograms\n",
    "        self.labels = labels\n",
    "        self.image_size = image_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.spectrograms)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get spectrogram and label\n",
    "        spec = self.spectrograms[idx]  # Shape: (n_mels, time_steps)\n",
    "        label = self.labels[idx]  # Shape: (2,)\n",
    "        \n",
    "        # Normalize spectrogram to [0, 1]\n",
    "        spec_min = spec.min()\n",
    "        spec_max = spec.max()\n",
    "        spec_norm = (spec - spec_min) / (spec_max - spec_min + 1e-8)\n",
    "        \n",
    "        # Resize to ViT input size (224x224)\n",
    "        spec_resized = torch.FloatTensor(spec_norm).unsqueeze(0)  # Add channel dim\n",
    "        spec_resized = F.interpolate(\n",
    "            spec_resized.unsqueeze(0), \n",
    "            size=(self.image_size, self.image_size), \n",
    "            mode='bilinear', \n",
    "            align_corners=False\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        # Convert to 3 channels (RGB) by triplicating\n",
    "        spec_rgb = spec_resized.repeat(3, 1, 1)\n",
    "        \n",
    "        # Apply ImageNet normalization\n",
    "        mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
    "        std = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n",
    "        spec_normalized = (spec_rgb - mean) / std\n",
    "        \n",
    "        return spec_normalized, torch.FloatTensor(label)\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "print(\"\\nüîÑ Creating dataset...\")\n",
    "full_dataset = SpectrogramDataset(all_spectrograms, all_labels)\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(TRAIN_SPLIT * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Datasets and dataloaders created:\")\n",
    "print(f\"   - Train samples: {len(train_dataset)}\")\n",
    "print(f\"   - Validation samples: {len(val_dataset)}\")\n",
    "print(f\"   - Train batches: {len(train_loader)}\")\n",
    "print(f\"   - Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d054daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTForEmotionRegression(nn.Module):\n",
    "    \"\"\"Vision Transformer for emotion regression with valence/arousal prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=VIT_MODEL_NAME, num_emotions=2, freeze_backbone=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_emotions = num_emotions\n",
    "        \n",
    "        print(f\"\\\\nü§ñ Initializing ViT Model: {model_name}\")\n",
    "        \n",
    "        # Load ViT model with robust error handling\n",
    "        self.vit_model = self._load_vit_model(model_name)\n",
    "        \n",
    "        # Get the hidden size from the model configuration\n",
    "        self.hidden_size = self.vit_model.config.hidden_size\n",
    "        print(f\"  Hidden Size: {self.hidden_size}\")\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            self._freeze_backbone()\n",
    "            print(\"  üßä Backbone frozen\")\n",
    "        else:\n",
    "            print(\"  üî• Backbone trainable\")\n",
    "        \n",
    "        # Add custom regression head\n",
    "        self.emotion_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.hidden_size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_size, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, self.num_emotions),\n",
    "            nn.Tanh()  # Output range [-1, 1] for valence/arousal\n",
    "        )\n",
    "        \n",
    "    def _load_vit_model(self, model_name):\n",
    "        \"\"\"Load ViT model with comprehensive error handling.\"\"\"\n",
    "        print(f\"  üì• Loading model from: {model_name}\")\n",
    "        \n",
    "        # Check if this is a local path or online model\n",
    "        if os.path.exists(model_name):\n",
    "            print(f\"  üóÇÔ∏è Loading from local path...\")\n",
    "            return self._load_local_model(model_name)\n",
    "        else:\n",
    "            print(f\"  üåê Loading from Hugging Face Hub...\")\n",
    "            return self._load_online_model(model_name)\n",
    "    \n",
    "    def _load_local_model(self, model_path):\n",
    "        \"\"\"Load model from local filesystem.\"\"\"\n",
    "        try:\n",
    "            print(f\"  üìÇ Checking local model at: {model_path}\")\n",
    "            \n",
    "            # Verify the path exists and contains model files\n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Model path does not exist: {model_path}\")\n",
    "            \n",
    "            # Check for required model files\n",
    "            required_files = ['config.json']\n",
    "            missing_files = [f for f in required_files if not os.path.exists(os.path.join(model_path, f))]\n",
    "            \n",
    "            if missing_files:\n",
    "                raise FileNotFoundError(f\"Missing model files: {missing_files}\")\n",
    "            \n",
    "            # Load the model\n",
    "            print(f\"  ‚ö° Loading ViT from local path...\")\n",
    "            model = ViTModel.from_pretrained(model_path, local_files_only=True)\n",
    "            print(f\"  ‚úÖ Successfully loaded local model!\")\n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Local model loading failed: {str(e)}\")\n",
    "            print(f\"  üîÑ Falling back to online download...\")\n",
    "            return self._load_online_model('google/vit-base-patch16-224-in21k')\n",
    "    \n",
    "    def _load_online_model(self, model_name):\n",
    "        \"\"\"Load model from Hugging Face Hub with retry logic.\"\"\"\n",
    "        max_retries = 3\n",
    "        retry_delays = [5, 10, 20]  # seconds\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"  üåê Download attempt {attempt + 1}/{max_retries}...\")\n",
    "                \n",
    "                # Try to load from cache first\n",
    "                model = ViTModel.from_pretrained(\n",
    "                    model_name,\n",
    "                    resume_download=True,\n",
    "                    force_download=False,\n",
    "                    cache_dir='/kaggle/working/model_cache'\n",
    "                )\n",
    "                \n",
    "                print(f\"  ‚úÖ Successfully loaded {model_name}\")\n",
    "                return model\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                \n",
    "                if attempt < max_retries - 1:\n",
    "                    delay = retry_delays[attempt]\n",
    "                    print(f\"  ‚è≥ Retrying in {delay} seconds...\")\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    print(f\"  üíÄ All download attempts failed!\")\n",
    "                    print(f\"  \udca1 SOLUTION: Download the model locally using the provided scripts:\")\n",
    "                    print(f\"     1. Run download_vit_model.py on your local machine\")\n",
    "                    print(f\"     2. Upload the model as a Kaggle dataset\")\n",
    "                    print(f\"     3. Update VIT_MODEL_NAME to your dataset path\")\n",
    "                    raise RuntimeError(f\"Failed to load ViT model after {max_retries} attempts: {str(e)}\")\n",
    "    \n",
    "    def _freeze_backbone(self):\n",
    "        \"\"\"Freeze the ViT backbone parameters.\"\"\"\n",
    "        for param in self.vit_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        \"\"\"Forward pass through ViT + emotion head.\"\"\"\n",
    "        # Get ViT outputs\n",
    "        outputs = self.vit_model(pixel_values=pixel_values)\n",
    "        \n",
    "        # Use the pooled output (CLS token representation)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Pass through emotion regression head\n",
    "        emotions = self.emotion_head(pooled_output)\n",
    "        \n",
    "        return emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f256c6",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Load Pre-trained Vision Transformer (ViT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77986193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-download and verify model availability\n",
    "print(\"üîç Checking model availability...\")\n",
    "\n",
    "from huggingface_hub import hf_hub_download, model_info\n",
    "import time\n",
    "\n",
    "def verify_model_download(model_name, max_retries=3):\n",
    "    \"\"\"\n",
    "    Verify model can be downloaded or is available locally\n",
    "    \"\"\"\n",
    "    print(f\"Model: {model_name}\")\n",
    "    \n",
    "    # Check if model exists in cache\n",
    "    try:\n",
    "        from transformers import ViTModel\n",
    "        \n",
    "        # Try loading from cache first\n",
    "        try:\n",
    "            print(\"  ‚è≥ Checking local cache...\")\n",
    "            ViTModel.from_pretrained(model_name, local_files_only=True)\n",
    "            print(\"  ‚úÖ Model found in local cache!\")\n",
    "            return True\n",
    "        except:\n",
    "            print(\"  ‚ÑπÔ∏è Model not in cache, will download...\")\n",
    "        \n",
    "        # Check model info\n",
    "        print(\"  ‚è≥ Verifying model on Hugging Face Hub...\")\n",
    "        info = model_info(model_name)\n",
    "        print(f\"  ‚úì Model exists: {info.modelId}\")\n",
    "        print(f\"  ‚úì Last modified: {info.lastModified}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Warning: {str(e)[:150]}\")\n",
    "        print(\"  üí° Will attempt to download during model initialization...\")\n",
    "        return False\n",
    "\n",
    "# Verify ViT model\n",
    "model_available = verify_model_download(VIT_MODEL_NAME)\n",
    "\n",
    "if not model_available:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Model verification failed!\")\n",
    "    print(\"The notebook will still attempt to download the model.\")\n",
    "    print(\"If download fails, the model will be initialized with random weights.\")\n",
    "    print(\"\\nAlternatives:\")\n",
    "    print(\"  1. Wait and retry (Hugging Face servers may be temporarily busy)\")\n",
    "    print(\"  2. Use a smaller model: 'google/vit-base-patch16-224'\")\n",
    "    print(\"  3. Continue without pre-training (train from scratch)\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41daa71b",
   "metadata": {},
   "source": [
    "### üîß Troubleshooting: Manual Model Download (Run if needed)\n",
    "\n",
    "If automatic download fails due to Hugging Face server issues, you can:\n",
    "\n",
    "**Option 1: Use Alternative Model**\n",
    "```python\n",
    "# Use the smaller, more stable version\n",
    "VIT_MODEL_NAME = 'google/vit-base-patch16-224'  # Instead of -in21k\n",
    "```\n",
    "\n",
    "**Option 2: Manual Download via Git**\n",
    "```bash\n",
    "# In a terminal or code cell with !\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/google/vit-base-patch16-224-in21k\n",
    "```\n",
    "\n",
    "**Option 3: Continue without Pre-training**\n",
    "- The notebook will automatically fall back to random initialization\n",
    "- Results will be similar to custom AST (no transfer learning benefits)\n",
    "\n",
    "**Option 4: Wait and Retry**\n",
    "- Hugging Face servers may be temporarily overloaded\n",
    "- Try again in 10-15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVE DOWNLOAD METHOD (Run this cell if automatic download fails)\n",
    "# This cell attempts to download the model using direct URLs\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file(url, filename):\n",
    "    \"\"\"Download file with progress bar\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        with open(filename, 'wb') as file, tqdm(\n",
    "            desc=filename,\n",
    "            total=total_size,\n",
    "            unit='iB',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as progress_bar:\n",
    "            for data in response.iter_content(chunk_size=1024):\n",
    "                size = file.write(data)\n",
    "                progress_bar.update(size)\n",
    "        \n",
    "        print(f\"‚úÖ Downloaded {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to download {filename}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Alternative: Try downloading with huggingface_hub's snapshot_download\n",
    "print(\"üîÑ Attempting alternative download method...\")\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import snapshot_download\n",
    "    \n",
    "    # Download entire model repository\n",
    "    cache_dir = \"/kaggle/working/model_cache\"\n",
    "    \n",
    "    print(f\"Downloading {VIT_MODEL_NAME} to {cache_dir}...\")\n",
    "    model_path = snapshot_download(\n",
    "        repo_id=VIT_MODEL_NAME,\n",
    "        cache_dir=cache_dir,\n",
    "        resume_download=True,\n",
    "        max_workers=1  # Use single worker to avoid 500 errors\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model downloaded successfully to: {model_path}\")\n",
    "    print(\"üí° Now update VIT_MODEL_NAME to use local path:\")\n",
    "    print(f\"    VIT_MODEL_NAME = '{model_path}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Alternative download also failed: {str(e)[:200]}\")\n",
    "    print(\"\\nüí° FALLBACK OPTIONS:\")\n",
    "    print(\"  1. Use smaller model: VIT_MODEL_NAME = 'google/vit-base-patch16-224'\")\n",
    "    print(\"  2. Train without pre-training (random initialization)\")\n",
    "    print(\"  3. Wait 15 minutes and retry\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a661a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class ViTForEmotionRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer for Emotion Regression\n",
    "    Uses pre-trained ViT from Hugging Face and adds regression head\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=VIT_MODEL_NAME, freeze_backbone=FREEZE_BACKBONE, dropout=DROPOUT):\n",
    "        super(ViTForEmotionRegression, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ViT model with retry logic\n",
    "        print(f\"Loading pre-trained ViT model: {model_name}...\")\n",
    "        \n",
    "        max_retries = 3\n",
    "        retry_delay = 5  # seconds\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Try loading with resume_download=True to handle interrupted downloads\n",
    "                self.vit = ViTModel.from_pretrained(\n",
    "                    model_name,\n",
    "                    resume_download=True,\n",
    "                    force_download=False,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "                print(f\"  ‚úì Model loaded successfully!\")\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"  ‚ö†Ô∏è Download attempt {attempt + 1} failed: {str(e)[:100]}\")\n",
    "                    print(f\"  ‚è≥ Retrying in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2  # Exponential backoff\n",
    "                else:\n",
    "                    print(f\"  ‚ùå All download attempts failed!\")\n",
    "                    print(f\"  üí° Trying alternative approach...\")\n",
    "                    \n",
    "                    # Fallback: Try to load from cache only\n",
    "                    try:\n",
    "                        self.vit = ViTModel.from_pretrained(\n",
    "                            model_name,\n",
    "                            local_files_only=True\n",
    "                        )\n",
    "                        print(f\"  ‚úì Loaded from local cache!\")\n",
    "                    except:\n",
    "                        # Last resort: Create model from config\n",
    "                        print(f\"  üîß Creating model from config (no pre-trained weights)...\")\n",
    "                        config = ViTConfig(\n",
    "                            hidden_size=768,\n",
    "                            num_hidden_layers=12,\n",
    "                            num_attention_heads=12,\n",
    "                            intermediate_size=3072,\n",
    "                            image_size=224,\n",
    "                            patch_size=16,\n",
    "                            num_channels=3\n",
    "                        )\n",
    "                        self.vit = ViTModel(config)\n",
    "                        print(f\"  ‚ö†Ô∏è WARNING: Using randomly initialized ViT (no transfer learning)\")\n",
    "        \n",
    "        # Freeze backbone if specified\n",
    "        if freeze_backbone:\n",
    "            for param in self.vit.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"  ‚úì ViT backbone frozen\")\n",
    "        else:\n",
    "            print(\"  ‚úì ViT backbone trainable\")\n",
    "        \n",
    "        # Get hidden size from ViT config\n",
    "        self.hidden_size = self.vit.config.hidden_size  # 768 for base model\n",
    "        \n",
    "        # Regression head for valence and arousal\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.hidden_size),\n",
    "            nn.Linear(self.hidden_size, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 2)  # Valence and Arousal\n",
    "        )\n",
    "        \n",
    "    def forward(self, pixel_values):\n",
    "        # Get ViT outputs\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        cls_output = outputs.last_hidden_state[:, 0]  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Regression head\n",
    "        emotion_output = self.regression_head(cls_output)  # (batch_size, 2)\n",
    "        \n",
    "        return emotion_output\n",
    "\n",
    "\n",
    "# Initialize model with error handling\n",
    "print(\"=\" * 60)\n",
    "print(\"ü§ñ INITIALIZING VISION TRANSFORMER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    model = ViTForEmotionRegression(\n",
    "        model_name=VIT_MODEL_NAME,\n",
    "        freeze_backbone=FREEZE_BACKBONE,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Print model summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ MODEL INITIALIZED SUCCESSFULLY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Model: {VIT_MODEL_NAME}\")\n",
    "    print(f\"Hidden size: {model.hidden_size}\")\n",
    "    print(f\"Freeze backbone: {FREEZE_BACKBONE}\")\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR: Failed to initialize model\")\n",
    "    print(f\"Error details: {str(e)}\")\n",
    "    print(\"\\nüí° TROUBLESHOOTING TIPS:\")\n",
    "    print(\"  1. Check your internet connection\")\n",
    "    print(\"  2. Try restarting the kernel and running again\")\n",
    "    print(\"  3. Manually download the model from: https://huggingface.co/google/vit-base-patch16-224-in21k\")\n",
    "    print(\"  4. Set local_files_only=True if model is already downloaded\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea0646c",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Train ViT Model on Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d3b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Weighted Emotion Loss Function\n",
    "# ===============================\n",
    "class WeightedEmotionLoss(nn.Module):\n",
    "    \"\"\"Weighted MSE loss for valence and arousal with different importance.\"\"\"\n",
    "    def __init__(self, valence_weight=0.6, arousal_weight=0.4):\n",
    "        super(WeightedEmotionLoss, self).__init__()\n",
    "        self.valence_weight = valence_weight\n",
    "        self.arousal_weight = arousal_weight\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        valence_loss = F.mse_loss(pred[:, 0], target[:, 0])\n",
    "        arousal_loss = F.mse_loss(pred[:, 1], target[:, 1])\n",
    "        return self.valence_weight * valence_loss + self.arousal_weight * arousal_loss\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Progressive Layer Unfreezing\n",
    "# ===============================\n",
    "def unfreeze_vit_layers(model, layers_to_unfreeze):\n",
    "    \"\"\"\n",
    "    Progressively unfreeze ViT encoder layers.\n",
    "    \n",
    "    Args:\n",
    "        model: ViTForEmotionRegression model\n",
    "        layers_to_unfreeze: List of layer indices to unfreeze (e.g., [10, 11] for last 2 layers)\n",
    "    \"\"\"\n",
    "    # First ensure everything in encoder is frozen\n",
    "    for param in model.vit.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze specified layers\n",
    "    if hasattr(model.vit, 'encoder') and hasattr(model.vit.encoder, 'layer'):\n",
    "        for layer_idx in layers_to_unfreeze:\n",
    "            if layer_idx < len(model.vit.encoder.layer):\n",
    "                for param in model.vit.encoder.layer[layer_idx].parameters():\n",
    "                    param.requires_grad = True\n",
    "                print(f\"   ‚úì Unfroze encoder layer {layer_idx}\")\n",
    "    \n",
    "    # Head is always trainable\n",
    "    for param in model.regression_head.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"   Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Enhanced Loss and Optimizer\n",
    "# ===============================\n",
    "criterion = WeightedEmotionLoss(valence_weight=0.6, arousal_weight=0.4)\n",
    "\n",
    "# Separate learning rates for backbone and head\n",
    "backbone_params = []\n",
    "head_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if 'regression_head' in name:\n",
    "            head_params.append(param)\n",
    "        else:\n",
    "            backbone_params.append(param)\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': head_params, 'lr': LEARNING_RATE},\n",
    "    {'params': backbone_params, 'lr': LEARNING_RATE * 0.1}  # Lower LR for backbone\n",
    "], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "print(\"\\\\nüéØ Using Weighted Emotion Loss:\")\n",
    "print(f\"   Valence weight: 60%\")\n",
    "print(f\"   Arousal weight: 40%\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Evaluation Metrics\n",
    "# ===============================\n",
    "def concordance_correlation_coefficient(y_true, y_pred):\n",
    "    \"\"\"Calculate CCC for emotion prediction evaluation\"\"\"\n",
    "    mean_true = torch.mean(y_true)\n",
    "    mean_pred = torch.mean(y_pred)\n",
    "    var_true = torch.var(y_true)\n",
    "    var_pred = torch.var(y_pred)\n",
    "    covariance = torch.mean((y_true - mean_true) * (y_pred - mean_pred))\n",
    "    \n",
    "    ccc = (2 * covariance) / (var_true + var_pred + (mean_true - mean_pred) ** 2 + 1e-8)\n",
    "    return ccc.item()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Training Function with Mixed Precision\n",
    "# ===============================\n",
    "scaler = torch.cuda.amp.GradScaler() if DEVICE.type == 'cuda' else None\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device, use_amp=True):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for specs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        specs, labels = specs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision training\n",
    "        if use_amp and scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(specs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(specs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.append(outputs.detach().cpu())\n",
    "        all_labels.append(labels.detach().cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    mae = F.l1_loss(all_preds, all_labels).item()\n",
    "    ccc_valence = concordance_correlation_coefficient(all_labels[:, 0], all_preds[:, 0])\n",
    "    ccc_arousal = concordance_correlation_coefficient(all_labels[:, 1], all_preds[:, 1])\n",
    "    \n",
    "    return avg_loss, mae, ccc_valence, ccc_arousal\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Validation Function\n",
    "# ===============================\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for specs, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(specs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_preds.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    mae = F.l1_loss(all_preds, all_labels).item()\n",
    "    ccc_valence = concordance_correlation_coefficient(all_labels[:, 0], all_preds[:, 0])\n",
    "    ccc_arousal = concordance_correlation_coefficient(all_labels[:, 1], all_preds[:, 1])\n",
    "    \n",
    "    return avg_loss, mae, ccc_valence, ccc_arousal, all_preds, all_labels\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Training Loop with Progressive Unfreezing\n",
    "# ===============================\n",
    "print(\"\\\\nüöÄ Starting ViT Training on Augmented Dataset...\\\\n\")\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'train_mae': [], 'train_ccc_v': [], 'train_ccc_a': [],\n",
    "    'val_loss': [], 'val_mae': [], 'val_ccc_v': [], 'val_ccc_a': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "# Progressive unfreezing schedule\n",
    "unfreezing_schedule = {\n",
    "    8: [11],      # Unfreeze last layer at epoch 8\n",
    "    12: [10, 11],  # Unfreeze last 2 layers at epoch 12\n",
    "    16: [9, 10, 11],  # Unfreeze last 3 layers at epoch 16\n",
    "}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Progressive unfreezing\n",
    "    if epoch in unfreezing_schedule:\n",
    "        print(f\"\\\\nüîì Progressive Unfreezing at Epoch {epoch + 1}:\")\n",
    "        unfreeze_vit_layers(model, unfreezing_schedule[epoch])\n",
    "        \n",
    "        # Update optimizer with newly unfrozen parameters\n",
    "        backbone_params = []\n",
    "        head_params = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'regression_head' in name:\n",
    "                    head_params.append(param)\n",
    "                else:\n",
    "                    backbone_params.append(param)\n",
    "        \n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': head_params, 'lr': LEARNING_RATE},\n",
    "            {'params': backbone_params, 'lr': LEARNING_RATE * 0.1}\n",
    "        ], weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS - epoch, eta_min=1e-6)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_mae, train_ccc_v, train_ccc_a = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_mae, val_ccc_v, val_ccc_a, val_preds, val_labels = validate(\n",
    "        model, val_loader, criterion, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_mae'].append(train_mae)\n",
    "    history['train_ccc_v'].append(train_ccc_v)\n",
    "    history['train_ccc_a'].append(train_ccc_a)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_mae'].append(val_mae)\n",
    "    history['val_ccc_v'].append(val_ccc_v)\n",
    "    history['val_ccc_a'].append(val_ccc_a)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\\\nüìä Training Metrics:\")\n",
    "    print(f\"  Loss: {train_loss:.4f} | MAE: {train_mae:.4f}\")\n",
    "    print(f\"  CCC Valence: {train_ccc_v:.4f} | CCC Arousal: {train_ccc_a:.4f}\")\n",
    "    \n",
    "    print(f\"\\\\nüìä Validation Metrics:\")\n",
    "    print(f\"  Loss: {val_loss:.4f} | MAE: {val_mae:.4f}\")\n",
    "    print(f\"  CCC Valence: {val_ccc_v:.4f} | CCC Arousal: {val_ccc_a:.4f}\")\n",
    "    print(f\"  Learning Rate: {current_lr:.2e}\")\n",
    "    \n",
    "    # Early stopping and model saving\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, 'best_vit_model.pth'))\n",
    "        print(f\"\\\\n‚úÖ Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\\\n‚ö†Ô∏è Early stopping triggered after {patience} epochs without improvement\")\n",
    "            break\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d656f62",
   "metadata": {},
   "source": [
    "## üîü Visualize Results & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('MSE Loss')\n",
    "axes[0, 0].set_title('Training & Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[0, 1].plot(history['train_mae'], label='Train MAE', linewidth=2)\n",
    "axes[0, 1].plot(history['val_mae'], label='Val MAE', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Mean Absolute Error')\n",
    "axes[0, 1].set_title('Training & Validation MAE')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# CCC Valence\n",
    "axes[1, 0].plot(history['train_ccc_v'], label='Train CCC', linewidth=2)\n",
    "axes[1, 0].plot(history['val_ccc_v'], label='Val CCC', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('CCC')\n",
    "axes[1, 0].set_title('Valence CCC')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "# CCC Arousal\n",
    "axes[1, 1].plot(history['train_ccc_a'], label='Train CCC', linewidth=2)\n",
    "axes[1, 1].plot(history['val_ccc_a'], label='Val CCC', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('CCC')\n",
    "axes[1, 1].set_title('Arousal CCC')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'vit_training_curves.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plots: Predicted vs Actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Valence\n",
    "axes[0].scatter(val_labels[:, 0], val_preds[:, 0], alpha=0.5, s=20)\n",
    "axes[0].plot([-1, 1], [-1, 1], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Valence')\n",
    "axes[0].set_ylabel('Predicted Valence')\n",
    "axes[0].set_title(f'Valence Prediction (CCC: {val_ccc_v:.4f})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim(-1.2, 1.2)\n",
    "axes[0].set_ylim(-1.2, 1.2)\n",
    "\n",
    "# Arousal\n",
    "axes[1].scatter(val_labels[:, 1], val_preds[:, 1], alpha=0.5, s=20)\n",
    "axes[1].plot([-1, 1], [-1, 1], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Arousal')\n",
    "axes[1].set_ylabel('Predicted Arousal')\n",
    "axes[1].set_title(f'Arousal Prediction (CCC: {val_ccc_a:.4f})')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(-1.2, 1.2)\n",
    "axes[1].set_ylim(-1.2, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'prediction_scatter.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2D Valence-Arousal Space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Ground Truth\n",
    "axes[0].scatter(val_labels[:, 0], val_labels[:, 1], alpha=0.6, s=50, c='blue', edgecolors='black')\n",
    "axes[0].set_xlabel('Valence')\n",
    "axes[0].set_ylabel('Arousal')\n",
    "axes[0].set_title('Ground Truth VA Space')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(0, color='k', linewidth=0.5)\n",
    "axes[0].axvline(0, color='k', linewidth=0.5)\n",
    "axes[0].set_xlim(-1.2, 1.2)\n",
    "axes[0].set_ylim(-1.2, 1.2)\n",
    "\n",
    "# Predictions\n",
    "axes[1].scatter(val_preds[:, 0], val_preds[:, 1], alpha=0.6, s=50, c='red', edgecolors='black')\n",
    "axes[1].set_xlabel('Valence')\n",
    "axes[1].set_ylabel('Arousal')\n",
    "axes[1].set_title('Predicted VA Space')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(0, color='k', linewidth=0.5)\n",
    "axes[1].axvline(0, color='k', linewidth=0.5)\n",
    "axes[1].set_xlim(-1.2, 1.2)\n",
    "axes[1].set_ylim(-1.2, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'va_space_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final summary\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\\\nüñºÔ∏è Model Architecture:\")\n",
    "print(f\"  - Base Model: {VIT_MODEL_NAME}\")\n",
    "print(f\"  - Total Parameters: {total_params:,}\")\n",
    "print(f\"  - Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "print(f\"\\\\nüé® GAN Augmentation:\")\n",
    "print(f\"  - Real samples: {len(real_spectrograms)}\")\n",
    "print(f\"  - Synthetic samples: {len(synthetic_spectrograms)}\")\n",
    "print(f\"  - Total samples: {len(all_spectrograms)}\")\n",
    "print(f\"  - Augmentation factor: {len(all_spectrograms)/len(real_spectrograms):.2f}x\")\n",
    "\n",
    "print(f\"\\\\nü§ñ ViT Model Performance:\")\n",
    "print(f\"  - Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"  - Final Val MAE: {val_mae:.4f}\")\n",
    "print(f\"  - Final Val CCC Valence: {val_ccc_v:.4f}\")\n",
    "print(f\"  - Final Val CCC Arousal: {val_ccc_a:.4f}\")\n",
    "\n",
    "print(f\"\\\\nüíæ Saved Outputs:\")\n",
    "print(f\"  - Generator model: generator.pth\")\n",
    "print(f\"  - Discriminator model: discriminator.pth\")\n",
    "print(f\"  - Best ViT model: best_vit_model.pth\")\n",
    "print(f\"  - Training curves: vit_training_curves.png\")\n",
    "print(f\"  - Prediction scatter: prediction_scatter.png\")\n",
    "print(f\"  - VA space comparison: va_space_comparison.png\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ffc9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archive outputs\n",
    "!zip -r /kaggle/working/vit_output.zip /kaggle/working/vit_augmented\n",
    "print(\"‚úÖ Outputs archived to vit_output.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9228f17f",
   "metadata": {},
   "source": [
    "## üß™ Comprehensive Model Testing\n",
    "\n",
    "Perform thorough testing of the trained ViT model including edge cases and robustness evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae684aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_robustness(model, test_loader, device=DEVICE):\n",
    "    \"\"\"Test model robustness with various edge cases and perturbations.\"\"\"\n",
    "    print(\"üß™ Testing model robustness...\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Test results storage\n",
    "    test_results = {\n",
    "        'normal_predictions': [],\n",
    "        'noisy_predictions': [],\n",
    "        'augmented_predictions': [],\n",
    "        'targets': [],\n",
    "        'confidence_scores': []\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_loader, desc='Robustness Testing')):\n",
    "            if i >= 10:  # Limit to first 10 batches for testing\n",
    "                break\n",
    "                \n",
    "            inputs = batch['pixel_values'].to(device)\n",
    "            targets = batch['emotions'].to(device)\n",
    "            \n",
    "            # 1. Normal prediction\n",
    "            normal_output = model(inputs)\n",
    "            \n",
    "            # 2. Add noise and test\n",
    "            noise = torch.randn_like(inputs) * 0.1\n",
    "            noisy_inputs = torch.clamp(inputs + noise, 0, 1)\n",
    "            noisy_output = model(noisy_inputs)\n",
    "            \n",
    "            # 3. Test with augmentation (random rotation)\n",
    "            augmented_inputs = torch.roll(inputs, shifts=10, dims=-1)\n",
    "            augmented_output = model(augmented_inputs)\n",
    "            \n",
    "            # Calculate confidence (inverse of prediction variance)\n",
    "            confidence = 1.0 / (torch.var(normal_output, dim=1) + 1e-6)\n",
    "            \n",
    "            # Store results\n",
    "            test_results['normal_predictions'].append(normal_output.cpu())\n",
    "            test_results['noisy_predictions'].append(noisy_output.cpu())\n",
    "            test_results['augmented_predictions'].append(augmented_output.cpu())\n",
    "            test_results['targets'].append(targets.cpu())\n",
    "            test_results['confidence_scores'].append(confidence.cpu())\n",
    "    \n",
    "    # Concatenate all results\n",
    "    for key in test_results:\n",
    "        if test_results[key]:\n",
    "            test_results[key] = torch.cat(test_results[key], dim=0).numpy()\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "def analyze_prediction_patterns(test_results):\n",
    "    \"\"\"Analyze prediction patterns and model behavior.\"\"\"\n",
    "    print(\"\\\\nüìä Analyzing prediction patterns...\")\n",
    "    \n",
    "    normal_pred = test_results['normal_predictions']\n",
    "    noisy_pred = test_results['noisy_predictions']\n",
    "    aug_pred = test_results['augmented_predictions']\n",
    "    targets = test_results['targets']\n",
    "    \n",
    "    # Calculate robustness metrics\n",
    "    noise_robustness = np.mean(np.abs(normal_pred - noisy_pred))\n",
    "    aug_robustness = np.mean(np.abs(normal_pred - aug_pred))\n",
    "    \n",
    "    print(f\"üîä Noise Robustness (MAE): {noise_robustness:.4f}\")\n",
    "    print(f\"üîÑ Augmentation Robustness (MAE): {aug_robustness:.4f}\")\n",
    "    \n",
    "    # Plot robustness analysis\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Prediction consistency\n",
    "    axes[0, 0].scatter(normal_pred[:, 0], noisy_pred[:, 0], alpha=0.6, color='blue')\n",
    "    axes[0, 0].plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "    axes[0, 0].set_xlabel('Normal Prediction (Valence)')\n",
    "    axes[0, 0].set_ylabel('Noisy Prediction (Valence)')\n",
    "    axes[0, 0].set_title('Noise Robustness - Valence')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].scatter(normal_pred[:, 1], noisy_pred[:, 1], alpha=0.6, color='red')\n",
    "    axes[0, 1].plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "    axes[0, 1].set_xlabel('Normal Prediction (Arousal)')\n",
    "    axes[0, 1].set_ylabel('Noisy Prediction (Arousal)')\n",
    "    axes[0, 1].set_title('Noise Robustness - Arousal')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Prediction distribution\n",
    "    axes[0, 2].hist(normal_pred.flatten(), bins=30, alpha=0.7, label='Normal', color='blue')\n",
    "    axes[0, 2].hist(noisy_pred.flatten(), bins=30, alpha=0.7, label='Noisy', color='orange')\n",
    "    axes[0, 2].set_xlabel('Prediction Value')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].set_title('Prediction Distribution')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error analysis\n",
    "    normal_error = np.abs(normal_pred - targets)\n",
    "    noisy_error = np.abs(noisy_pred - targets)\n",
    "    \n",
    "    axes[1, 0].hist(normal_error[:, 0], bins=20, alpha=0.7, label='Normal', color='blue')\n",
    "    axes[1, 0].hist(noisy_error[:, 0], bins=20, alpha=0.7, label='Noisy', color='orange')\n",
    "    axes[1, 0].set_xlabel('Absolute Error')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Valence Error Distribution')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].hist(normal_error[:, 1], bins=20, alpha=0.7, label='Normal', color='blue')\n",
    "    axes[1, 1].hist(noisy_error[:, 1], bins=20, alpha=0.7, label='Noisy', color='orange')\n",
    "    axes[1, 1].set_xlabel('Absolute Error')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Arousal Error Distribution')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confidence analysis\n",
    "    confidence = test_results['confidence_scores']\n",
    "    axes[1, 2].scatter(confidence, normal_error.mean(axis=1), alpha=0.6, color='green')\n",
    "    axes[1, 2].set_xlabel('Confidence Score')\n",
    "    axes[1, 2].set_ylabel('Prediction Error')\n",
    "    axes[1, 2].set_title('Confidence vs Error')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'noise_robustness': noise_robustness,\n",
    "        'augmentation_robustness': aug_robustness,\n",
    "        'mean_confidence': np.mean(confidence)\n",
    "    }\n",
    "\n",
    "def test_edge_cases(model, device=DEVICE):\n",
    "    \"\"\"Test model behavior on edge cases.\"\"\"\n",
    "    print(\"\\\\nüö® Testing edge cases...\")\n",
    "    \n",
    "    model.eval()\n",
    "    edge_cases = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Test with all zeros (silence)\n",
    "        zeros_input = torch.zeros(1, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE).to(device)\n",
    "        zeros_pred = model(zeros_input)\n",
    "        edge_cases['silence'] = zeros_pred.cpu().numpy()\n",
    "        \n",
    "        # Test with all ones (maximum intensity)\n",
    "        ones_input = torch.ones(1, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE).to(device)\n",
    "        ones_pred = model(ones_input)\n",
    "        edge_cases['maximum'] = ones_pred.cpu().numpy()\n",
    "        \n",
    "        # Test with random noise\n",
    "        noise_input = torch.randn(1, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE).to(device)\n",
    "        noise_input = torch.clamp(noise_input, 0, 1)\n",
    "        noise_pred = model(noise_input)\n",
    "        edge_cases['noise'] = noise_pred.cpu().numpy()\n",
    "        \n",
    "        # Test with checkerboard pattern\n",
    "        checker_input = torch.zeros(1, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE)\n",
    "        checker_input[:, :, ::2, ::2] = 1\n",
    "        checker_input[:, :, 1::2, 1::2] = 1\n",
    "        checker_input = checker_input.to(device)\n",
    "        checker_pred = model(checker_input)\n",
    "        edge_cases['checkerboard'] = checker_pred.cpu().numpy()\n",
    "    \n",
    "    print(\"Edge case predictions:\")\n",
    "    for case, pred in edge_cases.items():\n",
    "        valence, arousal = pred[0]\n",
    "        print(f\"  {case:12}: Valence={valence:.3f}, Arousal={arousal:.3f}\")\n",
    "    \n",
    "    return edge_cases\n",
    "\n",
    "def performance_benchmark(model, test_loader, device=DEVICE):\n",
    "    \"\"\"Benchmark model performance and timing.\"\"\"\n",
    "    print(\"\\\\n‚ö° Performance benchmarking...\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    dummy_input = torch.randn(1, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE).to(device)\n",
    "    for _ in range(5):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Timing test\n",
    "    import time\n",
    "    times = []\n",
    "    batch_sizes = [1, 4, 8, 16]\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        test_input = torch.randn(batch_size, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE).to(device)\n",
    "        \n",
    "        # Measure inference time\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):  # Average over 10 runs\n",
    "                _ = model(test_input)\n",
    "        \n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_time = (end_time - start_time) / 10\n",
    "        times.append(avg_time)\n",
    "        \n",
    "        print(f\"  Batch size {batch_size:2d}: {avg_time:.4f}s ({batch_size/avg_time:.1f} samples/s)\")\n",
    "    \n",
    "    # Memory usage\n",
    "    if device.type == 'cuda':\n",
    "        memory_usage = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "        print(f\"  Max GPU memory: {memory_usage:.1f} MB\")\n",
    "    \n",
    "    return {'batch_sizes': batch_sizes, 'inference_times': times}\n",
    "\n",
    "# Run comprehensive testing\n",
    "if 'best_vit_model' in locals() and 'test_loader' in locals():\n",
    "    print(\"üöÄ Starting comprehensive ViT model testing...\")\n",
    "    \n",
    "    # Load best model\n",
    "    try:\n",
    "        best_model_path = '/kaggle/working/vit_augmented/best_vit_model.pth'\n",
    "        if os.path.exists(best_model_path):\n",
    "            best_vit_model.load_state_dict(torch.load(best_model_path))\n",
    "            print(\"‚úÖ Best model loaded for testing\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Using current model state for testing\")\n",
    "    \n",
    "    # 1. Robustness testing\n",
    "    test_results = test_model_robustness(best_vit_model, test_loader)\n",
    "    robustness_metrics = analyze_prediction_patterns(test_results)\n",
    "    \n",
    "    # 2. Edge case testing\n",
    "    edge_results = test_edge_cases(best_vit_model)\n",
    "    \n",
    "    # 3. Performance benchmarking\n",
    "    perf_results = performance_benchmark(best_vit_model, test_loader)\n",
    "    \n",
    "    # Summary report\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"üìã COMPREHENSIVE TESTING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"‚úÖ Robustness Testing:\")\n",
    "    print(f\"   - Noise Robustness: {robustness_metrics['noise_robustness']:.4f}\")\n",
    "    print(f\"   - Augmentation Robustness: {robustness_metrics['augmentation_robustness']:.4f}\")\n",
    "    print(f\"   - Mean Confidence: {robustness_metrics['mean_confidence']:.4f}\")\n",
    "    print(f\"\\\\n‚úÖ Edge Cases: All {len(edge_results)} test cases completed\")\n",
    "    print(f\"\\\\n‚úÖ Performance: Benchmarked across {len(perf_results['batch_sizes'])} batch sizes\")\n",
    "    print(\"\\\\nüéâ All tests completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping comprehensive testing - model or test data not available\")\n",
    "    print(\"Please ensure the model is trained and test data is prepared.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
