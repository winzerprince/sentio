{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3fb8004",
   "metadata": {},
   "source": [
    "# üéµ Vision Transformer (ViT) Training with GAN-Based Data Augmentation\n",
    "\n",
    "## üìã Overview\n",
    "This notebook implements **Vision Transformer (ViT)** using the pre-trained `google/vit-base-patch16-224-in21k` model for music emotion recognition with **GAN-based data augmentation** to expand the DEAM dataset.\n",
    "\n",
    "### Key Features:\n",
    "- **Pre-trained ViT**: Uses `google/vit-base-patch16-224-in21k` trained on ImageNet-21k\n",
    "- **Transfer Learning**: Fine-tunes large vision model on audio spectrograms\n",
    "- **Conditional GAN**: Generates synthetic spectrograms conditioned on valence/arousal\n",
    "- **Data Expansion**: Increases dataset size from ~1800 to 5000+ samples\n",
    "- **Emotion Prediction**: Valence-Arousal (VA) continuous values\n",
    "\n",
    "### Pipeline:\n",
    "1. Load DEAM dataset and extract real spectrograms\n",
    "2. Train Conditional GAN to generate synthetic spectrograms\n",
    "3. Augment dataset with GAN-generated samples\n",
    "4. Fine-tune pre-trained ViT model on expanded dataset\n",
    "5. Evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb80459",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87dbfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from PIL import Image\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "root = Path('/kaggle/input').resolve()\n",
    "print(f\"Root exists: {root.exists()}\")\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeccfef",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cdd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# DATASET CONFIGURATION\n",
    "# ========================\n",
    "AUDIO_DIR = '/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/DEAM_audio/MEMD_audio/'\n",
    "ANNOTATIONS_DIR = '/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/DEAM_Annotations/annotations/annotations averaged per song/song_level/'\n",
    "\n",
    "# ========================\n",
    "# AUDIO PROCESSING CONFIG\n",
    "# ========================\n",
    "SAMPLE_RATE = 22050          # Audio sampling rate (Hz)\n",
    "DURATION = 30                # Audio clip duration (seconds)\n",
    "N_MELS = 128                 # Number of mel-frequency bins\n",
    "HOP_LENGTH = 512             # Hop length for STFT\n",
    "N_FFT = 2048                 # FFT window size\n",
    "FMIN = 20                    # Minimum frequency\n",
    "FMAX = 8000                  # Maximum frequency\n",
    "\n",
    "# ========================\n",
    "# VIT PREPROCESSING CONFIG\n",
    "# ========================\n",
    "VIT_IMAGE_SIZE = 224         # ViT expects 224x224 images\n",
    "VIT_CHANNELS = 3             # RGB channels (we'll triplicate grayscale)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]  # ImageNet normalization mean\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]   # ImageNet normalization std\n",
    "\n",
    "# ========================\n",
    "# GAN CONFIGURATION\n",
    "# ========================\n",
    "LATENT_DIM = 100             # Dimension of GAN noise vector\n",
    "CONDITION_DIM = 2            # Valence + Arousal\n",
    "GAN_LR = 0.0002              # GAN learning rate\n",
    "GAN_BETA1 = 0.5              # Adam beta1 for GAN\n",
    "GAN_BETA2 = 0.999            # Adam beta2 for GAN\n",
    "GAN_EPOCHS = 10              # GAN pre-training epochs\n",
    "GAN_BATCH_SIZE = 24          # GAN batch size (reduced from 32 to save memory)\n",
    "NUM_SYNTHETIC = 3200         # Number of synthetic samples to generate\n",
    "\n",
    "# ========================\n",
    "# VIT MODEL CONFIGURATION\n",
    "# ========================\n",
    "# OPTION 1: Use pre-downloaded model (recommended to avoid download issues)\n",
    "VIT_MODEL_NAME = '/kaggle/input/vit-model-kaggle/vit-model-for-kaggle'  # Update with your dataset path\n",
    "\n",
    "# OPTION 2: Fallback to online download (may fail with 500 errors)\n",
    "# VIT_MODEL_NAME = 'google/vit-base-patch16-224-in21k'\n",
    "\n",
    "# OPTION 3: Use smaller, more stable model\n",
    "# VIT_MODEL_NAME = 'google/vit-base-patch16-224'\n",
    "\n",
    "FREEZE_BACKBONE = False      # Whether to freeze ViT encoder layers\n",
    "DROPOUT = 0.1                # Dropout rate\n",
    "\n",
    "# ========================\n",
    "# TRAINING CONFIGURATION\n",
    "# ========================\n",
    "BATCH_SIZE = 12              # Training batch size (reduced from 16 to save memory)\n",
    "NUM_EPOCHS = 24              # Training epochs\n",
    "LEARNING_RATE = 1e-4         # Learning rate for fine-tuning\n",
    "WEIGHT_DECAY = 0.05          # AdamW weight decay\n",
    "TRAIN_SPLIT = 0.8            # Train/validation split ratio\n",
    "\n",
    "# ========================\n",
    "# MEMORY OPTIMIZATION\n",
    "# ========================\n",
    "# If you still encounter OOM errors, try these:\n",
    "# - Reduce GAN_BATCH_SIZE to 16\n",
    "# - Reduce BATCH_SIZE to 8\n",
    "# - Reduce NUM_SYNTHETIC to 2000\n",
    "# - Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "# ========================\n",
    "# SYSTEM CONFIGURATION\n",
    "# ========================\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "OUTPUT_DIR = '/kaggle/working/vit_augmented'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Enable memory efficient settings\n",
    "if torch.cuda.is_available():\n",
    "    # Enable TF32 for faster computation on Ampere GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    # Enable cudnn benchmarking for optimal performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"üöÄ CUDA optimizations enabled\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"Audio Duration: {DURATION}s @ {SAMPLE_RATE}Hz\")\n",
    "print(f\"Mel-Spectrogram: {N_MELS} bins\")\n",
    "print(f\"\\nüñºÔ∏è ViT Configuration:\")\n",
    "print(f\"  - Model Path: {VIT_MODEL_NAME}\")\n",
    "print(f\"  - Input Size: {VIT_IMAGE_SIZE}x{VIT_IMAGE_SIZE}x{VIT_CHANNELS}\")\n",
    "print(f\"  - Freeze Backbone: {FREEZE_BACKBONE}\")\n",
    "print(f\"\\nüé® GAN Configuration:\")\n",
    "print(f\"  - Latent Dim: {LATENT_DIM}\")\n",
    "print(f\"  - GAN Epochs: {GAN_EPOCHS}\")\n",
    "print(f\"  - GAN Batch Size: {GAN_BATCH_SIZE}\")\n",
    "print(f\"  - Synthetic Samples: {NUM_SYNTHETIC}\")\n",
    "print(f\"\\nüèãÔ∏è Training Configuration:\")\n",
    "print(f\"  - Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe6d571",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load DEAM Dataset & Extract Real Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc281fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "static_2000 = root / 'static-annotations-1-2000' / 'static_annotations_averaged_songs_1_2000.csv'\n",
    "static_2058 = root / 'static-annots-2058' / 'static_annots_2058.csv'\n",
    "\n",
    "try:\n",
    "    df1 = pd.read_csv(static_2000)\n",
    "    df2 = pd.read_csv(static_2058)\n",
    "    df_annotations = pd.concat([df1, df2], axis=0)\n",
    "    print(f\"‚úÖ Loaded annotations: {len(df_annotations)} songs\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading annotations: {e}\")\n",
    "    raise\n",
    "\n",
    "# Clean column names\n",
    "df_annotations.columns = df_annotations.columns.str.strip()\n",
    "\n",
    "print(\"\\\\nüìä Annotation Sample:\")\n",
    "print(df_annotations.head())\n",
    "print(f\"\\\\nColumns: {list(df_annotations.columns)}\")\n",
    "\n",
    "# Check for audio files\n",
    "audio_files = glob.glob(os.path.join(AUDIO_DIR, '*.mp3'))\n",
    "print(f\"\\\\nüéµ Found {len(audio_files)} audio files\")\n",
    "\n",
    "# Extract spectrograms with error logging\n",
    "print(\"\\\\nüîä Extracting spectrograms from real audio...\")\n",
    "\n",
    "error_log = []\n",
    "\n",
    "def extract_melspectrogram(audio_path, sr=SAMPLE_RATE, duration=DURATION):\n",
    "    \"\"\"Extract mel-spectrogram from audio file with error handling\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        y, _ = librosa.load(audio_path, sr=sr, duration=duration)\n",
    "        \n",
    "        # Compute mel-spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=y, sr=sr, n_mels=N_MELS, n_fft=N_FFT, \n",
    "            hop_length=HOP_LENGTH, fmin=FMIN, fmax=FMAX\n",
    "        )\n",
    "        \n",
    "        # Convert to log scale (dB)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        mel_spec_norm = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-8)\n",
    "        \n",
    "        return mel_spec_norm, None\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {os.path.basename(audio_path)}: {str(e)}\"\n",
    "        return None, error_msg\n",
    "\n",
    "# Extract spectrograms and labels\n",
    "real_spectrograms = []\n",
    "real_labels = []\n",
    "\n",
    "for idx, row in tqdm(df_annotations.iterrows(), total=len(df_annotations), desc=\"Extracting spectrograms\"):\n",
    "    song_id = str(int(row['song_id']))\n",
    "    audio_path = os.path.join(AUDIO_DIR, f\"{song_id}.mp3\")\n",
    "    \n",
    "    if not os.path.exists(audio_path):\n",
    "        error_log.append(f\"Missing audio file: {song_id}.mp3\")\n",
    "        continue\n",
    "    \n",
    "    # Extract spectrogram\n",
    "    spec, error = extract_melspectrogram(audio_path)\n",
    "    \n",
    "    if error is not None:\n",
    "        error_log.append(error)\n",
    "        continue\n",
    "        \n",
    "    if spec is not None:\n",
    "        real_spectrograms.append(spec)\n",
    "        \n",
    "        # Get valence and arousal\n",
    "        valence = row.get('valence_mean', row.get('valence', 0.5))\n",
    "        arousal = row.get('arousal_mean', row.get('arousal', 0.5))\n",
    "        \n",
    "        # Normalize to [-1, 1] range\n",
    "        valence_norm = (valence - 5.0) / 4.0\n",
    "        arousal_norm = (arousal - 5.0) / 4.0\n",
    "        \n",
    "        real_labels.append([valence_norm, arousal_norm])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "real_spectrograms = np.array(real_spectrograms)\n",
    "real_labels = np.array(real_labels)\n",
    "\n",
    "print(f\"\\\\n‚úÖ Extracted {len(real_spectrograms)} spectrograms\")\n",
    "print(f\"Spectrogram shape: {real_spectrograms.shape}\")\n",
    "print(f\"Labels shape: {real_labels.shape}\")\n",
    "print(f\"Spectrogram range: [{real_spectrograms.min():.2f}, {real_spectrograms.max():.2f}]\")\n",
    "print(f\"Labels range: [{real_labels.min():.2f}, {real_labels.max():.2f}]\")\n",
    "\n",
    "if error_log:\n",
    "    print(f\"\\\\n‚ö†Ô∏è {len(error_log)} errors occurred during extraction:\")\n",
    "    for i, error in enumerate(error_log[:10]):  # Show first 10 errors\n",
    "        print(f\"  {i+1}. {error}\")\n",
    "    if len(error_log) > 10:\n",
    "        print(f\"  ... and {len(error_log) - 10} more errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fddcfbb",
   "metadata": {},
   "source": [
    "### üìä Load Annotations and Extract Mel-Spectrograms\n",
    "\n",
    "Load the DEAM dataset annotations (valence/arousal ratings) and extract mel-spectrograms from audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a89a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample spectrogram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].imshow(real_spectrograms[0], aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0].set_title(f'Sample Spectrogram\\\\nValence: {real_labels[0][0]:.2f}, Arousal: {real_labels[0][1]:.2f}')\n",
    "axes[0].set_xlabel('Time Frames')\n",
    "axes[0].set_ylabel('Mel Frequency Bins')\n",
    "\n",
    "axes[1].scatter(real_labels[:, 0], real_labels[:, 1], alpha=0.5)\n",
    "axes[1].set_xlabel('Valence (normalized)')\n",
    "axes[1].set_ylabel('Arousal (normalized)')\n",
    "axes[1].set_title('Valence-Arousal Distribution (Real Data)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(0, color='k', linewidth=0.5)\n",
    "axes[1].axvline(0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'real_data_visualization.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be45baf",
   "metadata": {},
   "source": [
    "### üìà Visualize Real Data Distribution\n",
    "\n",
    "Display sample spectrograms and the distribution of valence-arousal values in the real DEAM dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d39a406",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Conditional GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d55f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Memory-efficient channel attention instead of spatial self-attention.\n",
    "    Reduces memory from O(H*W * H*W) to O(C*C).\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, channels, _, _ = x.size()\n",
    "        \n",
    "        # Channel attention via global pooling\n",
    "        avg_out = self.fc(self.avg_pool(x).view(batch_size, channels))\n",
    "        max_out = self.fc(self.max_pool(x).view(batch_size, channels))\n",
    "        \n",
    "        # Combine and apply attention\n",
    "        attention = (avg_out + max_out).view(batch_size, channels, 1, 1)\n",
    "        return x * attention\n",
    "\n",
    "\n",
    "class ImprovedSpectrogramGenerator(nn.Module):\n",
    "    \"\"\"Enhanced Conditional GAN Generator with Channel Attention (memory-efficient)\"\"\"\n",
    "    def __init__(self, latent_dim=LATENT_DIM, condition_dim=CONDITION_DIM, \n",
    "                 n_mels=N_MELS, time_steps=1292):\n",
    "        super(ImprovedSpectrogramGenerator, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.condition_dim = condition_dim\n",
    "        self.n_mels = n_mels\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        # Improved condition embedding\n",
    "        self.condition_embed = nn.Sequential(\n",
    "            nn.Linear(condition_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # Initial projection with condition\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 256, 256 * 16 * 20),\n",
    "            nn.BatchNorm1d(256 * 16 * 20),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # Convolutional upsampling with channel attention\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # Channel attention module (memory-efficient)\n",
    "        self.attention = ChannelAttention(64, reduction=8)\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=(1, 8), stride=(1, 8), padding=0),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, c):\n",
    "        # Embed condition\n",
    "        c_embed = self.condition_embed(c)\n",
    "        \n",
    "        # Concatenate noise and embedded condition\n",
    "        x = torch.cat([z, c_embed], dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 256, 16, 20)\n",
    "        \n",
    "        # Upsampling with attention\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.attention(x)  # Apply channel attention (much more memory efficient)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        # Ensure correct output size\n",
    "        if x.shape[-1] != self.time_steps or x.shape[-2] != self.n_mels:\n",
    "            x = F.interpolate(x, size=(self.n_mels, self.time_steps), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class ImprovedSpectrogramDiscriminator(nn.Module):\n",
    "    \"\"\"Enhanced Conditional GAN Discriminator with Spectral Normalization\"\"\"\n",
    "    def __init__(self, condition_dim=CONDITION_DIM, n_mels=N_MELS, time_steps=1292):\n",
    "        super(ImprovedSpectrogramDiscriminator, self).__init__()\n",
    "        \n",
    "        self.n_mels = n_mels\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        # Simplified condition embedding (reduce memory)\n",
    "        self.condition_embed = nn.Sequential(\n",
    "            nn.Linear(condition_dim, 64),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # Convolutional layers with spectral normalization for stability\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.utils.spectral_norm(nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.utils.spectral_norm(nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.utils.spectral_norm(nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        conv_output_size = 256 * 8 * 80\n",
    "        \n",
    "        # Fully connected layers with dropout and condition\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_output_size + 64, 256),  # Concatenate with condition\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "            # No sigmoid - will use BCEWithLogitsLoss for better stability\n",
    "        )\n",
    "        \n",
    "    def forward(self, spec, c):\n",
    "        # Embed condition (reduced dimensionality for memory)\n",
    "        batch_size = spec.size(0)\n",
    "        c_embed = self.condition_embed(c)\n",
    "        \n",
    "        # Apply conv layers\n",
    "        features = self.conv_layers(spec)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # Concatenate with condition and classify\n",
    "        x = torch.cat([features, c_embed], dim=1)\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Initialize improved GAN models\n",
    "time_steps = real_spectrograms.shape[2]\n",
    "generator = ImprovedSpectrogramGenerator(\n",
    "    latent_dim=LATENT_DIM, \n",
    "    condition_dim=CONDITION_DIM, \n",
    "    n_mels=N_MELS, \n",
    "    time_steps=time_steps\n",
    ").to(DEVICE)\n",
    "\n",
    "discriminator = ImprovedSpectrogramDiscriminator(\n",
    "    condition_dim=CONDITION_DIM, \n",
    "    n_mels=N_MELS, \n",
    "    time_steps=time_steps\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üé® IMPROVED GAN ARCHITECTURE (Memory-Efficient)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚ú® Generator Features:\")\n",
    "print(f\"   - Channel attention (memory-efficient)\")\n",
    "print(f\"   - Enhanced condition embedding\")\n",
    "print(f\"   - Parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "print(f\"\\n‚ú® Discriminator Features:\")\n",
    "print(f\"   - Spectral normalization for stability\")\n",
    "print(f\"   - Compact condition embedding\")\n",
    "print(f\"   - Parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clear cache after model initialization\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"üíæ GPU memory after models: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abedf47",
   "metadata": {},
   "source": [
    "### üé® Define GAN Generator with Channel Attention\n",
    "\n",
    "Improved conditional GAN generator that creates synthetic spectrograms based on valence/arousal conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d9314",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Train Conditional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a435ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéÆ BALANCED GAN TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# GAN Training Hyperparameters\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))  # Lower LR for discriminator\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Adaptive training parameters\n",
    "D_STEPS_THRESHOLD = 0.8  # Train discriminator more if accuracy < 80%\n",
    "G_STEPS_THRESHOLD = 0.2  # Train generator more if discriminator accuracy > 80%\n",
    "\n",
    "print(f\"‚öôÔ∏è  Optimizer Configuration:\")\n",
    "print(f\"   Generator LR: {0.0002} (Adam, Œ≤1=0.5, Œ≤2=0.999)\")\n",
    "print(f\"   Discriminator LR: {0.0001} (Adam, Œ≤1=0.5, Œ≤2=0.999)\")\n",
    "print(f\"   Loss Function: BCEWithLogitsLoss\")\n",
    "print(f\"\\nüéØ Adaptive Training:\")\n",
    "print(f\"   D steps if D_acc < 80%: 1-2 steps\")\n",
    "print(f\"   G steps if D_acc > 80%: 2-3 steps\")\n",
    "print(f\"   Gradient clipping: max_norm = 1.0\")\n",
    "print(f\"\\nüíæ Memory Optimization:\")\n",
    "print(f\"   Gradient accumulation: 2 steps (effective batch = {GAN_BATCH_SIZE * 2})\")\n",
    "print(f\"   Pin memory: enabled\")\n",
    "print(f\"   Periodic cache clearing: every 5 batches\")\n",
    "print(f\"   Data on CPU, batch transfer to GPU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract conditions from real labels (valence, arousal)\n",
    "real_conditions = real_labels.copy()  # Shape: (N, 2) - valence and arousal\n",
    "print(f\"\\nüìä Data prepared for GAN training:\")\n",
    "print(f\"   Real spectrograms: {real_spectrograms.shape}\")\n",
    "print(f\"   Real conditions: {real_conditions.shape}\")\n",
    "print(f\"   Condition range: [{real_conditions.min():.2f}, {real_conditions.max():.2f}]\")\n",
    "\n",
    "# Create memory-efficient DataLoader (data on CPU, transfer batches to GPU)\n",
    "real_specs_tensor = torch.FloatTensor(real_spectrograms).unsqueeze(1)  # Keep on CPU\n",
    "real_conditions_tensor = torch.FloatTensor(real_conditions)  # Keep on CPU\n",
    "\n",
    "gan_dataset = torch.utils.data.TensorDataset(real_specs_tensor, real_conditions_tensor)\n",
    "gan_loader = torch.utils.data.DataLoader(\n",
    "    gan_dataset, \n",
    "    batch_size=GAN_BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    pin_memory=True,  # Fast CPU->GPU transfer\n",
    "    num_workers=0  # Avoid multiprocessing overhead\n",
    ")\n",
    "\n",
    "# Clear cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"üßπ Cleared GPU cache\")\n",
    "    print(f\"üíæ Initial GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüöÄ Starting Balanced GAN Training...\\n\")\n",
    "\n",
    "# Training loop\n",
    "gan_losses = {'g_loss': [], 'd_loss': [], 'd_real_acc': [], 'd_fake_acc': []}\n",
    "GRADIENT_ACCUMULATION_STEPS = 2  # Effective batch size = GAN_BATCH_SIZE * 2\n",
    "\n",
    "for epoch in range(GAN_EPOCHS):\n",
    "    epoch_g_loss = 0\n",
    "    epoch_d_loss = 0\n",
    "    d_real_correct = 0\n",
    "    d_fake_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Reset gradient accumulation\n",
    "    g_optimizer.zero_grad()\n",
    "    d_optimizer.zero_grad()\n",
    "    \n",
    "    for i, (real_specs, conditions) in enumerate(tqdm(gan_loader, desc=f\"Epoch {epoch+1}/{GAN_EPOCHS}\")):\n",
    "        # Move batch to GPU (lazy loading)\n",
    "        real_specs = real_specs.to(DEVICE)\n",
    "        conditions = conditions.to(DEVICE)\n",
    "        batch_size = real_specs.size(0)\n",
    "        \n",
    "        # Discriminator labels (don't confuse with emotion labels)\n",
    "        d_real_labels = torch.ones(batch_size, 1).to(DEVICE)\n",
    "        d_fake_labels = torch.zeros(batch_size, 1).to(DEVICE)\n",
    "        \n",
    "        # ========== Train Discriminator ==========\n",
    "        # Calculate discriminator accuracy for adaptive training\n",
    "        with torch.no_grad():\n",
    "            z_temp = torch.randn(batch_size, LATENT_DIM).to(DEVICE)\n",
    "            fake_specs_temp = generator(z_temp, conditions)\n",
    "            d_real_out = discriminator(real_specs, conditions)\n",
    "            d_fake_out = discriminator(fake_specs_temp, conditions)\n",
    "            \n",
    "            d_real_acc = ((torch.sigmoid(d_real_out) > 0.5).float().mean()).item()\n",
    "            d_fake_acc = ((torch.sigmoid(d_fake_out) < 0.5).float().mean()).item()\n",
    "            \n",
    "        # Adaptive discriminator steps\n",
    "        d_steps = 1 if d_real_acc > D_STEPS_THRESHOLD and d_fake_acc > D_STEPS_THRESHOLD else 2\n",
    "        \n",
    "        for _ in range(d_steps):\n",
    "            # Real spectrograms\n",
    "            real_output = discriminator(real_specs, conditions)\n",
    "            d_real_loss = criterion(real_output, d_real_labels)\n",
    "            \n",
    "            # Fake spectrograms\n",
    "            z = torch.randn(batch_size, LATENT_DIM).to(DEVICE)\n",
    "            fake_specs = generator(z, conditions).detach()\n",
    "            fake_output = discriminator(fake_specs, conditions)\n",
    "            d_fake_loss = criterion(fake_output, d_fake_labels)\n",
    "            \n",
    "            # Total discriminator loss (scaled for gradient accumulation)\n",
    "            d_loss = (d_real_loss + d_fake_loss) / (2 * GRADIENT_ACCUMULATION_STEPS)\n",
    "            d_loss.backward()\n",
    "            \n",
    "            # Update discriminator (every GRADIENT_ACCUMULATION_STEPS batches)\n",
    "            if (i + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "                d_optimizer.step()\n",
    "                d_optimizer.zero_grad()\n",
    "        \n",
    "        # ========== Train Generator ==========\n",
    "        # Adaptive generator steps\n",
    "        g_steps = 3 if d_real_acc > D_STEPS_THRESHOLD else 1\n",
    "        \n",
    "        for _ in range(g_steps):\n",
    "            z = torch.randn(batch_size, LATENT_DIM).to(DEVICE)\n",
    "            fake_specs = generator(z, conditions)\n",
    "            fake_output = discriminator(fake_specs, conditions)\n",
    "            \n",
    "            # Generator loss (scaled for gradient accumulation)\n",
    "            g_loss = criterion(fake_output, d_real_labels) / GRADIENT_ACCUMULATION_STEPS\n",
    "            g_loss.backward()\n",
    "            \n",
    "            # Update generator (every GRADIENT_ACCUMULATION_STEPS batches)\n",
    "            if (i + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "                g_optimizer.step()\n",
    "                g_optimizer.zero_grad()\n",
    "        \n",
    "        # Track losses and accuracy\n",
    "        epoch_g_loss += g_loss.item() * GRADIENT_ACCUMULATION_STEPS * g_steps\n",
    "        epoch_d_loss += d_loss.item() * GRADIENT_ACCUMULATION_STEPS * 2 * d_steps\n",
    "        d_real_correct += d_real_acc * batch_size\n",
    "        d_fake_correct += d_fake_acc * batch_size\n",
    "        total_samples += batch_size\n",
    "        \n",
    "        # Periodic cache clearing (every 5 batches)\n",
    "        if i % 5 == 0 and i > 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Epoch statistics\n",
    "    avg_g_loss = epoch_g_loss / len(gan_loader)\n",
    "    avg_d_loss = epoch_d_loss / len(gan_loader)\n",
    "    avg_d_real_acc = d_real_correct / total_samples\n",
    "    avg_d_fake_acc = d_fake_correct / total_samples\n",
    "    \n",
    "    gan_losses['g_loss'].append(avg_g_loss)\n",
    "    gan_losses['d_loss'].append(avg_d_loss)\n",
    "    gan_losses['d_real_acc'].append(avg_d_real_acc)\n",
    "    gan_losses['d_fake_acc'].append(avg_d_fake_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{GAN_EPOCHS}]\")\n",
    "    print(f\"  G Loss: {avg_g_loss:.4f} | D Loss: {avg_d_loss:.4f}\")\n",
    "    print(f\"  D Real Acc: {avg_d_real_acc:.2%} | D Fake Acc: {avg_d_fake_acc:.2%}\")\n",
    "    \n",
    "    # GPU memory monitoring\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  üíæ GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "        if (epoch + 1) % 3 == 0:  # Deep clean every 3 epochs\n",
    "            torch.cuda.empty_cache()\n",
    "    print()\n",
    "\n",
    "print(\"\\n‚úÖ GAN Training Complete!\")\n",
    "print(f\"Final Generator Loss: {gan_losses['g_loss'][-1]:.4f}\")\n",
    "print(f\"Final Discriminator Loss: {gan_losses['d_loss'][-1]:.4f}\")\n",
    "print(f\"Final D Real Accuracy: {gan_losses['d_real_acc'][-1]:.2%}\")\n",
    "print(f\"Final D Fake Accuracy: {gan_losses['d_fake_acc'][-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed9d17d",
   "metadata": {},
   "source": [
    "### üèãÔ∏è GAN Training Loop\n",
    "\n",
    "Train the conditional GAN to generate realistic spectrograms conditioned on valence/arousal values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02b793",
   "metadata": {},
   "source": [
    "## 5.5Ô∏è‚É£ GAN Quality Metrics (Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "\n",
    "def calculate_statistics(spectrograms):\n",
    "    \"\"\"Calculate mean and covariance of spectrograms (FID-style)\"\"\"\n",
    "    # Flatten spectrograms\n",
    "    specs_flat = spectrograms.reshape(spectrograms.shape[0], -1)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mu = np.mean(specs_flat, axis=0)\n",
    "    sigma = np.cov(specs_flat, rowvar=False)\n",
    "    \n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate Frechet Distance (similar to FID score).\n",
    "    Lower is better - indicates generated data is closer to real data.\n",
    "    \"\"\"\n",
    "    # Calculate mean difference\n",
    "    diff = mu1 - mu2\n",
    "    \n",
    "    # Product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    \n",
    "    # Numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    # Calculate FD\n",
    "    fd = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    \n",
    "    return fd\n",
    "\n",
    "\n",
    "def evaluate_spectrogram_quality(real_specs, fake_specs, n_samples=500):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of generated spectrogram quality.\n",
    "    \n",
    "    Returns various metrics comparing real vs synthetic spectrograms.\n",
    "    \"\"\"\n",
    "    print(\"üìä Evaluating GAN Generation Quality...\\n\")\n",
    "    \n",
    "    # Subsample for efficiency\n",
    "    n_samples = min(n_samples, len(real_specs), len(fake_specs))\n",
    "    real_sample = real_specs[:n_samples]\n",
    "    fake_sample = fake_specs[:n_samples]\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Frechet Distance (FID-style)\n",
    "    print(\"  üî¢ Computing Frechet Distance...\")\n",
    "    mu_real, sigma_real = calculate_statistics(real_sample)\n",
    "    mu_fake, sigma_fake = calculate_statistics(fake_sample)\n",
    "    fd = calculate_frechet_distance(mu_real, sigma_real, mu_fake, sigma_fake)\n",
    "    metrics['frechet_distance'] = fd\n",
    "    print(f\"     Frechet Distance: {fd:.4f} (lower is better)\")\n",
    "    \n",
    "    # 2. Statistical moments comparison\n",
    "    print(\"\\n  üìà Computing statistical moments...\")\n",
    "    real_mean = np.mean(real_sample)\n",
    "    fake_mean = np.mean(fake_sample)\n",
    "    real_std = np.std(real_sample)\n",
    "    fake_std = np.std(fake_sample)\n",
    "    \n",
    "    metrics['mean_diff'] = abs(real_mean - fake_mean)\n",
    "    metrics['std_diff'] = abs(real_std - fake_std)\n",
    "    \n",
    "    print(f\"     Mean - Real: {real_mean:.4f}, Fake: {fake_mean:.4f}, Diff: {metrics['mean_diff']:.4f}\")\n",
    "    print(f\"     Std  - Real: {real_std:.4f}, Fake: {fake_std:.4f}, Diff: {metrics['std_diff']:.4f}\")\n",
    "    \n",
    "    # 3. Spectrogram smoothness (measure of noise)\n",
    "    print(\"\\n  üé® Evaluating smoothness (temporal consistency)...\")\n",
    "    real_smoothness = np.mean([np.mean(np.abs(np.diff(spec, axis=1))) for spec in real_sample])\n",
    "    fake_smoothness = np.mean([np.mean(np.abs(np.diff(spec, axis=1))) for spec in fake_sample])\n",
    "    \n",
    "    metrics['real_smoothness'] = real_smoothness\n",
    "    metrics['fake_smoothness'] = fake_smoothness\n",
    "    metrics['smoothness_ratio'] = fake_smoothness / (real_smoothness + 1e-8)\n",
    "    \n",
    "    print(f\"     Real smoothness: {real_smoothness:.4f}\")\n",
    "    print(f\"     Fake smoothness: {fake_smoothness:.4f}\")\n",
    "    print(f\"     Ratio: {metrics['smoothness_ratio']:.4f} (closer to 1.0 is better)\")\n",
    "    \n",
    "    # 4. Frequency distribution analysis\n",
    "    print(\"\\n  üéµ Analyzing frequency content...\")\n",
    "    real_freq_mean = np.mean(real_sample, axis=(0, 2))  # Average across batch and time\n",
    "    fake_freq_mean = np.mean(fake_sample, axis=(0, 2))\n",
    "    \n",
    "    freq_correlation = np.corrcoef(real_freq_mean, fake_freq_mean)[0, 1]\n",
    "    metrics['frequency_correlation'] = freq_correlation\n",
    "    \n",
    "    print(f\"     Frequency correlation: {freq_correlation:.4f} (higher is better)\")\n",
    "    \n",
    "    # 5. Dynamic range\n",
    "    print(\"\\n  üìä Comparing dynamic range...\")\n",
    "    real_range = np.max(real_sample) - np.min(real_sample)\n",
    "    fake_range = np.max(fake_sample) - np.min(fake_sample)\n",
    "    \n",
    "    metrics['real_range'] = real_range\n",
    "    metrics['fake_range'] = fake_range\n",
    "    metrics['range_diff'] = abs(real_range - fake_range)\n",
    "    \n",
    "    print(f\"     Real range: {real_range:.4f}\")\n",
    "    print(f\"     Fake range: {fake_range:.4f}\")\n",
    "    print(f\"     Difference: {metrics['range_diff']:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def visualize_quality_comparison(real_specs, fake_specs, metrics, n_visual=3):\n",
    "    \"\"\"Visualize quality comparison between real and synthetic spectrograms.\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Top row: Sample spectrograms\n",
    "    for i in range(n_visual):\n",
    "        # Real spectrograms\n",
    "        ax = fig.add_subplot(gs[0, i])\n",
    "        ax.imshow(real_specs[i], aspect='auto', origin='lower', cmap='viridis')\n",
    "        ax.set_title(f'Real Sample {i+1}')\n",
    "        ax.set_xlabel('Time')\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Mel Frequency')\n",
    "        \n",
    "        # Fake spectrograms\n",
    "        ax = fig.add_subplot(gs[1, i])\n",
    "        ax.imshow(fake_specs[i], aspect='auto', origin='lower', cmap='viridis')\n",
    "        ax.set_title(f'Generated Sample {i+1}')\n",
    "        ax.set_xlabel('Time')\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Mel Frequency')\n",
    "    \n",
    "    # Middle row: Metrics visualization\n",
    "    ax = fig.add_subplot(gs[2, :])\n",
    "    metric_names = ['Frechet\\nDistance', 'Frequency\\nCorrelation', 'Smoothness\\nRatio']\n",
    "    metric_values = [\n",
    "        metrics['frechet_distance'],\n",
    "        metrics['frequency_correlation'],\n",
    "        metrics['smoothness_ratio']\n",
    "    ]\n",
    "    colors = ['#e74c3c' if v > 10 else '#3498db' if v > 5 else '#2ecc71' \n",
    "              for v in [metrics['frechet_distance'], \n",
    "                       1-metrics['frequency_correlation'], \n",
    "                       abs(1-metrics['smoothness_ratio'])]]\n",
    "    \n",
    "    bars = ax.bar(metric_names, metric_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_ylabel('Metric Value')\n",
    "    ax.set_title('GAN Quality Metrics')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{value:.3f}',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Bottom row: Temporal evolution comparison\n",
    "    ax = fig.add_subplot(gs[3, :])\n",
    "    real_temporal = np.mean(real_specs[:50], axis=(0, 1))\n",
    "    fake_temporal = np.mean(fake_specs[:50], axis=(0, 1))\n",
    "    ax.plot(real_temporal, label='Real', linewidth=2, alpha=0.8)\n",
    "    ax.plot(fake_temporal, label='Synthetic', linewidth=2, alpha=0.8)\n",
    "    ax.set_xlabel('Time Frame')\n",
    "    ax.set_ylabel('Average Amplitude')\n",
    "    ax.set_title('Temporal Evolution Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'gan_quality_evaluation.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Quality evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ff0fd",
   "metadata": {},
   "source": [
    "### üìä Define Quality Evaluation Functions\n",
    "\n",
    "Functions to evaluate GAN generation quality using Fr√©chet Distance, correlation metrics, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbbb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üé® Generating {NUM_SYNTHETIC} synthetic spectrograms...\\n\")\n",
    "\n",
    "generator.eval()\n",
    "synthetic_spectrograms = []\n",
    "synthetic_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    num_batches = NUM_SYNTHETIC // GAN_BATCH_SIZE\n",
    "    \n",
    "    for i in tqdm(range(num_batches), desc=\"Generating\"):\n",
    "        z = torch.randn(GAN_BATCH_SIZE, LATENT_DIM).to(DEVICE)\n",
    "        random_conditions = torch.FloatTensor(GAN_BATCH_SIZE, 2).uniform_(-1, 1).to(DEVICE)\n",
    "        \n",
    "        fake_specs = generator(z, random_conditions)\n",
    "        \n",
    "        synthetic_spectrograms.append(fake_specs.cpu().numpy())\n",
    "        synthetic_labels.append(random_conditions.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "synthetic_spectrograms = np.concatenate(synthetic_spectrograms, axis=0)\n",
    "synthetic_labels = np.concatenate(synthetic_labels, axis=0)\n",
    "\n",
    "# Remove channel dimension\n",
    "synthetic_spectrograms = synthetic_spectrograms.squeeze(1)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(synthetic_spectrograms)} synthetic spectrograms\")\n",
    "print(f\"Synthetic spectrogram shape: {synthetic_spectrograms.shape}\")\n",
    "print(f\"Synthetic labels shape: {synthetic_labels.shape}\")\n",
    "\n",
    "# ========== ROBUST DATA VALIDATION & CONVERSION ==========\n",
    "print(f\"\\nüîç Validating and preparing label data...\")\n",
    "print(f\"‚úÖ Using 'real_conditions' for original emotion labels (valence/arousal)\")\n",
    "print(f\"   (This contains the original DEAM annotations)\")\n",
    "\n",
    "def prepare_labels(labels, name=\"labels\"):\n",
    "    \"\"\"\n",
    "    Robust label preparation function that handles all edge cases.\n",
    "    Ensures output is numpy array with shape (N, 2).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Convert to numpy if tensor\n",
    "        if torch.is_tensor(labels):\n",
    "            print(f\"  - {name} is a tensor, converting to numpy...\")\n",
    "            if labels.is_cuda:\n",
    "                labels_np = labels.cpu().numpy()\n",
    "            else:\n",
    "                labels_np = labels.numpy()\n",
    "        else:\n",
    "            labels_np = np.array(labels)\n",
    "        \n",
    "        print(f\"  - {name} shape after conversion: {labels_np.shape}\")\n",
    "        \n",
    "        # Step 2: Handle shape issues\n",
    "        if len(labels_np.shape) == 1:\n",
    "            # 1D array - reshape to (N, 2)\n",
    "            print(f\"  - {name} is 1D, reshaping to (-1, 2)...\")\n",
    "            labels_np = labels_np.reshape(-1, 2)\n",
    "        elif len(labels_np.shape) == 2:\n",
    "            # 2D array - check if transposed\n",
    "            if labels_np.shape[0] == 2 and labels_np.shape[1] > 2:\n",
    "                # Likely transposed (2, N) -> (N, 2)\n",
    "                print(f\"  - {name} appears transposed {labels_np.shape}, fixing...\")\n",
    "                labels_np = labels_np.T\n",
    "            elif labels_np.shape[1] == 1:\n",
    "                # Shape is (N, 1) - might need to be (N//2, 2)\n",
    "                print(f\"  - {name} has shape {labels_np.shape}, reshaping...\")\n",
    "                labels_np = labels_np.reshape(-1, 2)\n",
    "        \n",
    "        # Step 3: Final validation\n",
    "        if labels_np.shape[1] != 2:\n",
    "            print(f\"  ‚ö†Ô∏è WARNING: {name} has unexpected shape {labels_np.shape}\")\n",
    "            print(f\"  Attempting to force reshape to (-1, 2)...\")\n",
    "            labels_np = labels_np.reshape(-1, 2)\n",
    "        \n",
    "        print(f\"  ‚úÖ {name} final shape: {labels_np.shape}\")\n",
    "        return labels_np\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå ERROR processing {name}: {e}\")\n",
    "        print(f\"  Returning original data as-is\")\n",
    "        return labels if not torch.is_tensor(labels) else labels.cpu().numpy()\n",
    "\n",
    "# Prepare emotion labels for dataset\n",
    "# 'real_conditions' contains the original valence/arousal values from DEAM\n",
    "print(f\"\\nüìä Original emotion labels info:\")\n",
    "print(f\"   real_conditions shape: {real_conditions.shape}\")\n",
    "print(f\"   real_conditions type: {type(real_conditions)}\")\n",
    "\n",
    "# Prepare real emotion labels from real_conditions  \n",
    "emotion_labels_real = prepare_labels(real_conditions, \"real_conditions\")\n",
    "\n",
    "# Prepare synthetic labels\n",
    "emotion_labels_synthetic = prepare_labels(synthetic_labels, \"synthetic_labels\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data preparation complete!\")\n",
    "print(f\"   Real emotion labels: {emotion_labels_real.shape}\")\n",
    "print(f\"   Synthetic emotion labels: {emotion_labels_synthetic.shape}\")\n",
    "\n",
    "# ========== VISUALIZATION WITH ERROR HANDLING ==========\n",
    "try:\n",
    "    print(f\"\\nüìä Creating visualizations...\")\n",
    "    \n",
    "    # Visualize synthetic vs real spectrograms\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    for i in range(3):\n",
    "        try:\n",
    "            axes[0, i].imshow(real_spectrograms[i], aspect='auto', origin='lower', cmap='viridis')\n",
    "            # Robust label access\n",
    "            v_real = emotion_labels_real[i, 0] if emotion_labels_real.shape[1] >= 1 else 0\n",
    "            a_real = emotion_labels_real[i, 1] if emotion_labels_real.shape[1] >= 2 else 0\n",
    "            axes[0, i].set_title(f'Real Spec {i+1}\\nV: {v_real:.2f}, A: {a_real:.2f}')\n",
    "            axes[0, i].set_xlabel('Time')\n",
    "            axes[0, i].set_ylabel('Mel Bins')\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Could not plot real spectrogram {i}: {e}\")\n",
    "            axes[0, i].text(0.5, 0.5, f'Error\\n{str(e)[:30]}', \n",
    "                          ha='center', va='center', transform=axes[0, i].transAxes)\n",
    "    \n",
    "    for i in range(3):\n",
    "        try:\n",
    "            axes[1, i].imshow(synthetic_spectrograms[i], aspect='auto', origin='lower', cmap='viridis')\n",
    "            # Robust label access\n",
    "            v_syn = emotion_labels_synthetic[i, 0] if emotion_labels_synthetic.shape[1] >= 1 else 0\n",
    "            a_syn = emotion_labels_synthetic[i, 1] if emotion_labels_synthetic.shape[1] >= 2 else 0\n",
    "            axes[1, i].set_title(f'Synthetic Spec {i+1}\\nV: {v_syn:.2f}, A: {a_syn:.2f}')\n",
    "            axes[1, i].set_xlabel('Time')\n",
    "            axes[1, i].set_ylabel('Mel Bins')\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Could not plot synthetic spectrogram {i}: {e}\")\n",
    "            axes[1, i].text(0.5, 0.5, f'Error\\n{str(e)[:30]}', \n",
    "                          ha='center', va='center', transform=axes[1, i].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'real_vs_synthetic.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"  ‚úÖ Spectrogram comparison plot saved\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Error creating spectrogram comparison plot: {e}\")\n",
    "    print(\"  Continuing execution...\")\n",
    "\n",
    "# ========== DISTRIBUTION COMPARISON WITH ERROR HANDLING ==========\n",
    "try:\n",
    "    print(f\"\\nüìà Creating distribution plots...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    try:\n",
    "        axes[0].scatter(emotion_labels_real[:, 0], emotion_labels_real[:, 1], \n",
    "                       alpha=0.5, label='Real', s=20)\n",
    "        axes[0].scatter(emotion_labels_synthetic[:, 0], emotion_labels_synthetic[:, 1], \n",
    "                       alpha=0.3, label='Synthetic', s=20)\n",
    "        axes[0].set_xlabel('Valence')\n",
    "        axes[0].set_ylabel('Arousal')\n",
    "        axes[0].set_title('Valence-Arousal Distribution')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].axhline(0, color='k', linewidth=0.5)\n",
    "        axes[0].axvline(0, color='k', linewidth=0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Warning: Could not create scatter plot: {e}\")\n",
    "        axes[0].text(0.5, 0.5, f'Scatter plot error\\n{str(e)[:30]}', \n",
    "                    ha='center', va='center', transform=axes[0].transAxes)\n",
    "    \n",
    "    # Bar plot\n",
    "    try:\n",
    "        sizes = [len(real_spectrograms), len(synthetic_spectrograms), \n",
    "                 len(real_spectrograms) + len(synthetic_spectrograms)]\n",
    "        labels = ['Real', 'Synthetic', 'Total']\n",
    "        colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "        axes[1].bar(labels, sizes, color=colors, alpha=0.7, edgecolor='black')\n",
    "        axes[1].set_ylabel('Number of Samples')\n",
    "        axes[1].set_title('Dataset Size Comparison')\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        for i, v in enumerate(sizes):\n",
    "            axes[1].text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Warning: Could not create bar plot: {e}\")\n",
    "        axes[1].text(0.5, 0.5, f'Bar plot error\\n{str(e)[:30]}', \n",
    "                    ha='center', va='center', transform=axes[1].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'augmented_dataset_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"  ‚úÖ Distribution comparison plot saved\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Error creating distribution plots: {e}\")\n",
    "    print(\"  Continuing execution...\")\n",
    "\n",
    "# ========== STATISTICS ==========\n",
    "print(f\"\\nüìä Augmented Dataset Statistics:\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"Real Data:\")\n",
    "print(f\"  Samples: {len(real_spectrograms)}\")\n",
    "print(f\"  Valence: mean={emotion_labels_real[:, 0].mean():.3f}, std={emotion_labels_real[:, 0].std():.3f}\")\n",
    "print(f\"  Arousal: mean={emotion_labels_real[:, 1].mean():.3f}, std={emotion_labels_real[:, 1].std():.3f}\")\n",
    "print(f\"\\nSynthetic Data:\")\n",
    "print(f\"  Samples: {len(synthetic_spectrograms)}\")\n",
    "print(f\"  Valence: mean={emotion_labels_synthetic[:, 0].mean():.3f}, std={emotion_labels_synthetic[:, 0].std():.3f}\")\n",
    "print(f\"  Arousal: mean={emotion_labels_synthetic[:, 1].mean():.3f}, std={emotion_labels_synthetic[:, 1].std():.3f}\")\n",
    "print(f\"\\nüìà Data Augmentation Ratio: {len(synthetic_spectrograms) / len(real_spectrograms):.2f}x\")\n",
    "print(f\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b9ab4",
   "metadata": {},
   "source": [
    "### üé® Generate Synthetic Spectrograms\n",
    "\n",
    "Use the trained GAN generator to create synthetic spectrograms with random valence/arousal conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2244e5a0",
   "metadata": {},
   "source": [
    "## 6.2Ô∏è‚É£ Evaluate GAN Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6303bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== OPTIONAL GAN QUALITY EVALUATION ==========\n",
    "# This step can consume 4-6 GB of RAM due to covariance matrix computation\n",
    "# Skip if memory is limited\n",
    "\n",
    "import gc\n",
    "\n",
    "# Check if we should run quality evaluation\n",
    "SKIP_QUALITY_EVAL = True  # Set to False if you have >20GB RAM available\n",
    "\n",
    "if SKIP_QUALITY_EVAL:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ö†Ô∏è SKIPPING GAN QUALITY EVALUATION (Memory Optimization)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"üìä Reason: Quality evaluation requires ~4-6 GB RAM for covariance computation\")\n",
    "    print(\"üí° To enable: Set SKIP_QUALITY_EVAL = False in this cell\")\n",
    "    print(\"\\n‚úÖ GAN training completed successfully!\")\n",
    "    print(\"   Moving on to ViT training with augmented dataset...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üî¨ GAN QUALITY EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚ö†Ô∏è Warning: This may consume 4-6 GB of memory\")\n",
    "    \n",
    "    try:\n",
    "        # Clear memory before evaluation\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Evaluate quality (with smaller sample size to reduce memory)\n",
    "        quality_metrics = evaluate_spectrogram_quality(\n",
    "            real_spectrograms[:200],  # Reduced from 500 to 200\n",
    "            synthetic_spectrograms[:200]\n",
    "        )\n",
    "        \n",
    "        # Visualize comparison\n",
    "        visualize_quality_comparison(\n",
    "            real_spectrograms[:5],  # Reduced from 10 to 5\n",
    "            synthetic_spectrograms[:5],\n",
    "            quality_metrics\n",
    "        )\n",
    "        \n",
    "        # Overall quality score\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üéØ OVERALL QUALITY ASSESSMENT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Compute composite quality score (0-100)\n",
    "        fd_score = max(0, 100 - quality_metrics['frechet_distance'] * 10)\n",
    "        freq_score = quality_metrics['frequency_correlation'] * 100\n",
    "        smooth_score = max(0, 100 - abs(1.0 - quality_metrics['smoothness_ratio']) * 100)\n",
    "        \n",
    "        overall_score = (fd_score * 0.4 + freq_score * 0.4 + smooth_score * 0.2)\n",
    "        \n",
    "        print(f\"  Frechet Distance Score: {fd_score:.1f}/100\")\n",
    "        print(f\"  Frequency Correlation Score: {freq_score:.1f}/100\")\n",
    "        print(f\"  Smoothness Score: {smooth_score:.1f}/100\")\n",
    "        print(f\"\\n  üìä Overall GAN Quality Score: {overall_score:.1f}/100\")\n",
    "        \n",
    "        if overall_score >= 70:\n",
    "            print(\"  ‚úÖ Excellent - GAN generates high-quality spectrograms\")\n",
    "        elif overall_score >= 50:\n",
    "            print(\"  ‚ö†Ô∏è Good - GAN output is acceptable but could be improved\")\n",
    "        else:\n",
    "            print(\"  ‚ùå Poor - GAN needs significant improvement (mostly noise)\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Clear memory after evaluation\n",
    "        del quality_metrics\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Quality evaluation failed: {e}\")\n",
    "        print(\"‚ö†Ô∏è Continuing without quality metrics...\")\n",
    "        print(\"üí° This does not affect ViT training\")\n",
    "\n",
    "# Final cleanup before moving to ViT training\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\nüíæ GPU memory before ViT prep: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready to proceed with ViT dataset preparation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e5a17",
   "metadata": {},
   "source": [
    "### üî¨ [Optional] Evaluate GAN Quality Metrics\n",
    "\n",
    "Optional step to evaluate synthetic spectrogram quality (requires ~4-6 GB RAM). Can be skipped for memory efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cdcb4f",
   "metadata": {},
   "source": [
    "## 6.5Ô∏è‚É£ Audio Reconstruction - Listen to GAN Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a623ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "import soundfile as sf\n",
    "\n",
    "def spectrogram_to_audio(spec_normalized, sample_rate=SAMPLE_RATE, n_fft=N_FFT, \n",
    "                         hop_length=HOP_LENGTH, n_iter=32):\n",
    "    \"\"\"\n",
    "    Convert normalized mel spectrogram back to audio using Griffin-Lim algorithm.\n",
    "    \n",
    "    Args:\n",
    "        spec_normalized: Normalized spectrogram in range [-1, 1]\n",
    "        sample_rate: Audio sample rate\n",
    "        n_fft: FFT window size\n",
    "        hop_length: Hop length for STFT\n",
    "        n_iter: Number of Griffin-Lim iterations\n",
    "    \n",
    "    Returns:\n",
    "        audio: Reconstructed audio signal\n",
    "    \"\"\"\n",
    "    # Denormalize spectrogram\n",
    "    spec_db = spec_normalized * 40.0  # Approximate dB range\n",
    "    \n",
    "    # Convert from dB to power\n",
    "    spec_power = librosa.db_to_amplitude(spec_db)\n",
    "    \n",
    "    # Reconstruct audio using Griffin-Lim\n",
    "    audio = librosa.feature.inverse.mel_to_audio(\n",
    "        spec_power,\n",
    "        sr=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_iter=n_iter,\n",
    "        fmin=FMIN,\n",
    "        fmax=FMAX\n",
    "    )\n",
    "    \n",
    "    return audio\n",
    "\n",
    "\n",
    "def generate_and_listen_to_samples(generator, n_samples=5, emotions=None, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Generate synthetic spectrograms and convert them to audio for listening.\n",
    "    \n",
    "    Args:\n",
    "        generator: Trained GAN generator\n",
    "        n_samples: Number of samples to generate\n",
    "        emotions: List of (valence, arousal) tuples, or None for random\n",
    "        device: Torch device\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    print(f\"üéµ Generating {n_samples} audio samples from GAN...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(n_samples):\n",
    "            # Generate noise\n",
    "            z = torch.randn(1, LATENT_DIM).to(device)\n",
    "            \n",
    "            # Use provided emotions or generate random\n",
    "            if emotions and i < len(emotions):\n",
    "                valence, arousal = emotions[i]\n",
    "            else:\n",
    "                valence = np.random.uniform(-1, 1)\n",
    "                arousal = np.random.uniform(-1, 1)\n",
    "            \n",
    "            condition = torch.FloatTensor([[valence, arousal]]).to(device)\n",
    "            \n",
    "            # Generate spectrogram\n",
    "            fake_spec = generator(z, condition)\n",
    "            fake_spec_np = fake_spec.squeeze().cpu().numpy()\n",
    "            \n",
    "            # Convert to audio\n",
    "            print(f\"\\\\nüéß Sample {i+1}: Valence={valence:.2f}, Arousal={arousal:.2f}\")\n",
    "            audio = spectrogram_to_audio(fake_spec_np)\n",
    "            \n",
    "            # Normalize audio\n",
    "            audio = audio / (np.max(np.abs(audio)) + 1e-8) * 0.9\n",
    "            \n",
    "            # Save audio file\n",
    "            audio_path = os.path.join(OUTPUT_DIR, f'generated_sample_{i+1}_v{valence:.2f}_a{arousal:.2f}.wav')\n",
    "            sf.write(audio_path, audio, SAMPLE_RATE)\n",
    "            print(f\"   üíæ Saved: {audio_path}\")\n",
    "            \n",
    "            # Display audio player\n",
    "            display(Audio(audio, rate=SAMPLE_RATE))\n",
    "            \n",
    "            # Visualize spectrogram\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            librosa.display.specshow(\n",
    "                fake_spec_np,\n",
    "                sr=SAMPLE_RATE,\n",
    "                hop_length=HOP_LENGTH,\n",
    "                x_axis='time',\n",
    "                y_axis='mel',\n",
    "                fmax=FMAX,\n",
    "                cmap='viridis'\n",
    "            )\n",
    "            plt.colorbar(format='%+2.0f dB')\n",
    "            plt.title(f'Generated Spectrogram {i+1}\\\\nValence: {valence:.2f}, Arousal: {arousal:.2f}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(OUTPUT_DIR, f'generated_spec_{i+1}.png'), dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Define emotion targets to test\n",
    "test_emotions = [\n",
    "    (-0.8, -0.6),  # Sad, calm\n",
    "    (0.8, 0.7),    # Happy, energetic\n",
    "    (-0.3, 0.8),   # Angry, tense\n",
    "    (0.5, -0.5),   # Content, relaxed\n",
    "    (0.0, 0.0),    # Neutral\n",
    "]\n",
    "\n",
    "print(\"üé® Generating audio from synthetic spectrograms...\")\n",
    "print(\"This allows you to qualitatively assess GAN generation quality.\\\\n\")\n",
    "\n",
    "generate_and_listen_to_samples(\n",
    "    generator, \n",
    "    n_samples=5, \n",
    "    emotions=test_emotions,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Audio generation complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"üí° Tips for evaluation:\")\n",
    "print(\"  - Listen for musical structure vs pure noise\")\n",
    "print(\"  - Check if emotion patterns are perceptible\")\n",
    "print(\"  - Compare across different valence/arousal values\")\n",
    "print(\"  - Real music should have harmonic and temporal patterns\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6707c26a",
   "metadata": {},
   "source": [
    "### üéµ [Optional] Convert Spectrograms to Audio\n",
    "\n",
    "Optional: Convert generated spectrograms back to audio for qualitative listening evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d810471d",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Prepare Augmented Dataset for ViT Training\n",
    "\n",
    "Now that we have trained the GAN and generated synthetic spectrograms, we combine the real and synthetic data to create an expanded dataset for training the Vision Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ba87b",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Define ViT Model Architecture for Emotion Regression\n",
    "\n",
    "We define a custom Vision Transformer model for emotion prediction, using a pre-trained ViT backbone with a custom regression head for valence/arousal prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== MEMORY-EFFICIENT DATASET PREPARATION ==========\n",
    "import gc\n",
    "\n",
    "print(\"üîÑ Preparing augmented dataset for ViT training...\")\n",
    "print(f\"üíæ Memory before concatenation: {torch.cuda.memory_allocated()/1024**3:.2f} GB\" if torch.cuda.is_available() else \"\")\n",
    "\n",
    "# Clear any unused memory before concatenation\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Combine real and synthetic spectrograms\n",
    "print(f\"\\nüì¶ Combining datasets:\")\n",
    "print(f\"   Real spectrograms: {real_spectrograms.shape}\")\n",
    "print(f\"   Synthetic spectrograms: {synthetic_spectrograms.shape}\")\n",
    "print(f\"   Real emotion labels: {emotion_labels_real.shape}\")\n",
    "print(f\"   Synthetic emotion labels: {emotion_labels_synthetic.shape}\")\n",
    "\n",
    "try:\n",
    "    all_spectrograms = np.concatenate([real_spectrograms, synthetic_spectrograms], axis=0)\n",
    "    all_emotion_labels = np.concatenate([emotion_labels_real, emotion_labels_synthetic], axis=0)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total augmented dataset:\")\n",
    "    print(f\"   - Total samples: {len(all_spectrograms)}\")\n",
    "    print(f\"   - Spectrograms shape: {all_spectrograms.shape}\")\n",
    "    print(f\"   - Emotion labels shape: {all_emotion_labels.shape}\")\n",
    "    print(f\"   - Memory usage: ~{all_spectrograms.nbytes / 1024**3:.2f} GB\")\n",
    "    \n",
    "except MemoryError as e:\n",
    "    print(f\"\\n‚ùå MemoryError during concatenation: {e}\")\n",
    "    print(f\"üîß Reducing synthetic samples to fit in memory...\")\n",
    "    \n",
    "    # Reduce synthetic samples if OOM\n",
    "    max_synthetic = 2000  # Reduce from 3192 to 2000\n",
    "    synthetic_spectrograms = synthetic_spectrograms[:max_synthetic]\n",
    "    emotion_labels_synthetic = emotion_labels_synthetic[:max_synthetic]\n",
    "    \n",
    "    print(f\"   Reduced synthetic samples to: {max_synthetic}\")\n",
    "    \n",
    "    # Try again\n",
    "    all_spectrograms = np.concatenate([real_spectrograms, synthetic_spectrograms], axis=0)\n",
    "    all_emotion_labels = np.concatenate([emotion_labels_real, emotion_labels_synthetic], axis=0)\n",
    "    \n",
    "    print(f\"‚úÖ Reduced dataset created: {len(all_spectrograms)} samples\")\n",
    "\n",
    "# Delete intermediate arrays to free memory\n",
    "print(f\"\\nüßπ Freeing intermediate memory...\")\n",
    "del synthetic_spectrograms  # Delete synthetic spectrograms (we have all_spectrograms now)\n",
    "del emotion_labels_synthetic  # Delete synthetic labels (we have all_emotion_labels now)\n",
    "\n",
    "# Don't delete real_spectrograms yet - needed for evaluation\n",
    "gc.collect()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"üíæ GPU memory after cleanup: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"‚úÖ Memory cleanup complete\\n\")\n",
    "\n",
    "\n",
    "# ========== MEMORY-EFFICIENT DATASET CLASS ==========\n",
    "class SpectrogramDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Memory-efficient dataset for mel-spectrograms with ViT preprocessing.\n",
    "    Performs preprocessing on-the-fly instead of storing preprocessed data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spectrograms, labels, image_size=VIT_IMAGE_SIZE):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spectrograms: numpy array of shape (N, n_mels, time_steps)\n",
    "            labels: numpy array of shape (N, 2)\n",
    "            image_size: target image size for ViT (default 224)\n",
    "        \"\"\"\n",
    "        # Validate input shapes\n",
    "        assert len(spectrograms) == len(labels), \\\n",
    "            f\"Spectrogram count ({len(spectrograms)}) must match label count ({len(labels)})\"\n",
    "        assert labels.shape[1] == 2, \\\n",
    "            f\"Labels must have shape (N, 2), got {labels.shape}\"\n",
    "        \n",
    "        # Store as numpy arrays (more memory efficient than tensors)\n",
    "        self.spectrograms = spectrograms\n",
    "        self.labels = labels\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        print(f\"  üìä Dataset created: {len(self.spectrograms)} samples\")\n",
    "        print(f\"     Spectrograms: {self.spectrograms.shape}\")\n",
    "        print(f\"     Labels: {self.labels.shape}\")\n",
    "        \n",
    "        # Precompute normalization constants\n",
    "        self.imagenet_mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
    "        self.imagenet_std = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.spectrograms)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get spectrogram and label with on-the-fly preprocessing.\n",
    "        This saves memory by not storing preprocessed tensors.\n",
    "        \"\"\"\n",
    "        # Get spectrogram and label (as numpy arrays)\n",
    "        spec = self.spectrograms[idx]  # Shape: (n_mels, time_steps)\n",
    "        label = self.labels[idx]  # Shape: (2,)\n",
    "        \n",
    "        # Normalize spectrogram to [0, 1]\n",
    "        spec_min = spec.min()\n",
    "        spec_max = spec.max()\n",
    "        spec_norm = (spec - spec_min) / (spec_max - spec_min + 1e-8)\n",
    "        \n",
    "        # Convert to tensor and resize to ViT input size (224x224)\n",
    "        spec_tensor = torch.FloatTensor(spec_norm).unsqueeze(0)  # Add channel dim: (1, H, W)\n",
    "        spec_resized = F.interpolate(\n",
    "            spec_tensor.unsqueeze(0),  # Add batch dim: (1, 1, H, W)\n",
    "            size=(self.image_size, self.image_size), \n",
    "            mode='bilinear', \n",
    "            align_corners=False\n",
    "        ).squeeze(0)  # Remove batch dim: (1, 224, 224)\n",
    "        \n",
    "        # Convert to 3 channels (RGB) by triplicating\n",
    "        spec_rgb = spec_resized.repeat(3, 1, 1)  # (3, 224, 224)\n",
    "        \n",
    "        # Apply ImageNet normalization\n",
    "        spec_normalized = (spec_rgb - self.imagenet_mean) / self.imagenet_std\n",
    "        \n",
    "        return spec_normalized, torch.FloatTensor(label)\n",
    "\n",
    "\n",
    "# ========== CREATE DATASETS AND DATALOADERS ==========\n",
    "print(\"üîÑ Creating dataset and dataloaders...\")\n",
    "\n",
    "try:\n",
    "    # Create full dataset\n",
    "    full_dataset = SpectrogramDataset(all_spectrograms, all_emotion_labels)\n",
    "    print(f\"‚úÖ Created dataset with {len(full_dataset)} samples\")\n",
    "    \n",
    "    # Split into train and validation\n",
    "    train_size = int(TRAIN_SPLIT * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset split:\")\n",
    "    print(f\"   - Train: {len(train_dataset)} samples\")\n",
    "    print(f\"   - Validation: {len(val_dataset)} samples\")\n",
    "    \n",
    "    # Create dataloaders with memory-efficient settings\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=0,  # Set to 0 to avoid multiprocessing memory overhead\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False  # Don't keep workers alive\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Set to 0 to avoid multiprocessing memory overhead\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Dataloaders created:\")\n",
    "    print(f\"   - Train batches: {len(train_loader)}\")\n",
    "    print(f\"   - Validation batches: {len(val_loader)}\")\n",
    "    print(f\"   - Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    # Final memory cleanup\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\nüíæ Final GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset preparation complete! Ready for ViT training.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during dataset creation: {e}\")\n",
    "    print(f\"üí° Suggestion: Reduce NUM_SYNTHETIC or BATCH_SIZE in configuration\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032986a9",
   "metadata": {},
   "source": [
    "### üì¶ Combine Real and Synthetic Data\n",
    "\n",
    "Concatenate real and synthetic spectrograms to create the augmented training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d054daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTForEmotionRegression(nn.Module):\n",
    "    \"\"\"Vision Transformer for emotion regression with valence/arousal prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=VIT_MODEL_NAME, num_emotions=2, freeze_backbone=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_emotions = num_emotions\n",
    "        \n",
    "        print(f\"\\nü§ñ Initializing ViT Model: {model_name}\")\n",
    "        \n",
    "        # Load ViT model with 3-tier fallback strategy\n",
    "        self.vit_model = self._load_vit_model_with_fallback()\n",
    "        \n",
    "        # Get the hidden size from the model configuration\n",
    "        self.hidden_size = self.vit_model.config.hidden_size\n",
    "        print(f\"  Hidden Size: {self.hidden_size}\")\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            self._freeze_backbone()\n",
    "            print(\"  üßä Backbone frozen\")\n",
    "        else:\n",
    "            print(\"  üî• Backbone trainable\")\n",
    "        \n",
    "        # Add custom regression head\n",
    "        self.emotion_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.hidden_size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_size, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, self.num_emotions),\n",
    "            nn.Tanh()  # Output range [-1, 1] for valence/arousal\n",
    "        )\n",
    "        \n",
    "    def _load_vit_model_with_fallback(self):\n",
    "        \"\"\"\n",
    "        3-Tier Model Loading Strategy:\n",
    "        1. Try loading from Kaggle dataset input folder\n",
    "        2. Try downloading from Hugging Face\n",
    "        3. Fall back to base model if all else fails\n",
    "        \"\"\"\n",
    "        print(f\"\\n\udd04 Starting 3-Tier Model Loading Strategy...\")\n",
    "        \n",
    "        # ========== TIER 1: Kaggle Dataset Input ==========\n",
    "        kaggle_model_paths = [\n",
    "            '/kaggle/input/vit-model-for-kaggle/vit-model-for-kaggle',\n",
    "            '/kaggle/input/vit-model-for-kaggle',\n",
    "        ]\n",
    "        \n",
    "        for kaggle_path in kaggle_model_paths:\n",
    "            try:\n",
    "                print(f\"\\nüì¶ TIER 1: Trying Kaggle dataset...\")\n",
    "                print(f\"  Path: {kaggle_path}\")\n",
    "                \n",
    "                if os.path.exists(kaggle_path):\n",
    "                    print(f\"  ‚úÖ Path exists!\")\n",
    "                    \n",
    "                    # List contents for debugging\n",
    "                    if os.path.isdir(kaggle_path):\n",
    "                        contents = os.listdir(kaggle_path)\n",
    "                        print(f\"  üìÇ Contents: {contents[:5]}...\" if len(contents) > 5 else f\"  üìÇ Contents: {contents}\")\n",
    "                    \n",
    "                    # Check for required files\n",
    "                    config_path = os.path.join(kaggle_path, 'config.json')\n",
    "                    if os.path.exists(config_path):\n",
    "                        print(f\"  ‚úÖ Found config.json\")\n",
    "                        \n",
    "                        # Try to load the model\n",
    "                        print(f\"  ‚ö° Loading ViT from Kaggle dataset...\")\n",
    "                        model = ViTModel.from_pretrained(kaggle_path, local_files_only=True)\n",
    "                        print(f\"  ‚úÖ SUCCESS! Loaded model from Kaggle dataset\")\n",
    "                        return model\n",
    "                    else:\n",
    "                        print(f\"  ‚ö†Ô∏è Missing config.json at {config_path}\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Path does not exist\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Kaggle dataset loading failed: {str(e)[:100]}\")\n",
    "        \n",
    "        # ========== TIER 2: Download from Hugging Face ==========\n",
    "        try:\n",
    "            print(f\"\\nüåê TIER 2: Trying Hugging Face download...\")\n",
    "            model = self._download_from_huggingface()\n",
    "            print(f\"  ‚úÖ SUCCESS! Downloaded model from Hugging Face\")\n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Hugging Face download failed: {str(e)[:100]}\")\n",
    "        \n",
    "        # ========== TIER 3: Base Model Fallback ==========\n",
    "        try:\n",
    "            print(f\"\\nüîß TIER 3: Falling back to base model...\")\n",
    "            base_model_name = 'google/vit-base-patch16-224-in21k'\n",
    "            print(f\"  Loading: {base_model_name}\")\n",
    "            model = ViTModel.from_pretrained(base_model_name)\n",
    "            print(f\"  ‚úÖ SUCCESS! Loaded base model\")\n",
    "            print(f\"  ‚ö†Ô∏è WARNING: Using base model without fine-tuning\")\n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Base model loading failed: {str(e)}\")\n",
    "            raise RuntimeError(\n",
    "                \"All 3 tiers of model loading failed!\\n\"\n",
    "                \"SOLUTION:\\n\"\n",
    "                \"1. Download the model using: python download_vit_model.py\\n\"\n",
    "                \"2. Upload to Kaggle as dataset: vit-model-for-kaggle\\n\"\n",
    "                \"3. Add dataset to notebook inputs\\n\"\n",
    "                \"4. Verify path: /kaggle/input/vit-model-for-kaggle/vit-model-for-kaggle\"\n",
    "            )\n",
    "    \n",
    "    def _download_from_huggingface(self):\n",
    "        \"\"\"Download model from Hugging Face with retry logic.\"\"\"\n",
    "        model_name = 'google/vit-base-patch16-224-in21k'\n",
    "        max_retries = 2\n",
    "        retry_delays = [5, 10]  # seconds\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"  üåê Download attempt {attempt + 1}/{max_retries}...\")\n",
    "                \n",
    "                # Try to load from cache first\n",
    "                model = ViTModel.from_pretrained(\n",
    "                    model_name,\n",
    "                    resume_download=True,\n",
    "                    force_download=False,\n",
    "                    cache_dir='/kaggle/working/model_cache'\n",
    "                )\n",
    "                \n",
    "                return model\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Attempt {attempt + 1} failed: {str(e)[:80]}\")\n",
    "                \n",
    "                if attempt < max_retries - 1:\n",
    "                    delay = retry_delays[attempt]\n",
    "                    print(f\"  ‚è≥ Retrying in {delay} seconds...\")\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    raise e\n",
    "    \n",
    "    def _freeze_backbone(self):\n",
    "        \"\"\"Freeze the ViT backbone parameters.\"\"\"\n",
    "        for param in self.vit_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        \"\"\"Forward pass through ViT + emotion head.\"\"\"\n",
    "        # Get ViT outputs\n",
    "        outputs = self.vit_model(pixel_values=pixel_values)\n",
    "        \n",
    "        # Use [CLS] token representation (first token)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Pass through emotion prediction head\n",
    "        emotion_predictions = self.emotion_head(cls_output)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        return emotion_predictions\n",
    "\n",
    "\n",
    "print(\"‚úÖ ViT Model class defined with 3-tier loading strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c512f3e2",
   "metadata": {},
   "source": [
    "### ü§ñ Define ViT Regression Model Class\n",
    "\n",
    "Create a custom ViT model with a regression head for predicting continuous valence and arousal values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f256c6",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Load Pre-trained Vision Transformer Model\n",
    "\n",
    "Load the pre-trained ViT model with a 3-tier fallback strategy: Kaggle dataset ‚Üí Hugging Face API ‚Üí Local fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77986193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-download and verify model availability\n",
    "print(\"üîç Checking model availability...\")\n",
    "\n",
    "from huggingface_hub import hf_hub_download, model_info\n",
    "import time\n",
    "\n",
    "def verify_model_download(model_name, max_retries=3):\n",
    "    \"\"\"\n",
    "    Verify model can be downloaded or is available locally\n",
    "    \"\"\"\n",
    "    print(f\"Model: {model_name}\")\n",
    "    \n",
    "    # Check if model exists in cache\n",
    "    try:\n",
    "        from transformers import ViTModel\n",
    "        \n",
    "        # Try loading from cache first\n",
    "        try:\n",
    "            print(\"  ‚è≥ Checking local cache...\")\n",
    "            ViTModel.from_pretrained(model_name, local_files_only=True)\n",
    "            print(\"  ‚úÖ Model found in local cache!\")\n",
    "            return True\n",
    "        except:\n",
    "            print(\"  ‚ÑπÔ∏è Model not in cache, will download...\")\n",
    "        \n",
    "        # Check model info\n",
    "        print(\"  ‚è≥ Verifying model on Hugging Face Hub...\")\n",
    "        info = model_info(model_name)\n",
    "        print(f\"  ‚úì Model exists: {info.modelId}\")\n",
    "        print(f\"  ‚úì Last modified: {info.lastModified}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Warning: {str(e)[:150]}\")\n",
    "        print(\"  üí° Will attempt to download during model initialization...\")\n",
    "        return False\n",
    "\n",
    "# Verify ViT model\n",
    "model_available = verify_model_download(VIT_MODEL_NAME)\n",
    "\n",
    "if not model_available:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Model verification failed!\")\n",
    "    print(\"The notebook will still attempt to download the model.\")\n",
    "    print(\"If download fails, the model will be initialized with random weights.\")\n",
    "    print(\"\\nAlternatives:\")\n",
    "    print(\"  1. Wait and retry (Hugging Face servers may be temporarily busy)\")\n",
    "    print(\"  2. Use a smaller model: 'google/vit-base-patch16-224'\")\n",
    "    print(\"  3. Continue without pre-training (train from scratch)\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b0dfd",
   "metadata": {},
   "source": [
    "### üöÄ Instantiate ViT Model\n",
    "\n",
    "Create an instance of the ViT model with the custom emotion regression head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41daa71b",
   "metadata": {},
   "source": [
    "## \udd1f Train ViT Model on Augmented Dataset\n",
    "\n",
    "Train the Vision Transformer on the combined real + synthetic dataset using the AdamW optimizer and CosineAnnealing learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVE DOWNLOAD METHOD (Run this cell if automatic download fails)\n",
    "# This cell attempts to download the model using direct URLs\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file(url, filename):\n",
    "    \"\"\"Download file with progress bar\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        with open(filename, 'wb') as file, tqdm(\n",
    "            desc=filename,\n",
    "            total=total_size,\n",
    "            unit='iB',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as progress_bar:\n",
    "            for data in response.iter_content(chunk_size=1024):\n",
    "                size = file.write(data)\n",
    "                progress_bar.update(size)\n",
    "        \n",
    "        print(f\"‚úÖ Downloaded {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to download {filename}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Alternative: Try downloading with huggingface_hub's snapshot_download\n",
    "print(\"üîÑ Attempting alternative download method...\")\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import snapshot_download\n",
    "    \n",
    "    # Download entire model repository\n",
    "    cache_dir = \"/kaggle/working/model_cache\"\n",
    "    \n",
    "    print(f\"Downloading {VIT_MODEL_NAME} to {cache_dir}...\")\n",
    "    model_path = snapshot_download(\n",
    "        repo_id=VIT_MODEL_NAME,\n",
    "        cache_dir=cache_dir,\n",
    "        resume_download=True,\n",
    "        max_workers=1  # Use single worker to avoid 500 errors\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model downloaded successfully to: {model_path}\")\n",
    "    print(\"üí° Now update VIT_MODEL_NAME to use local path:\")\n",
    "    print(f\"    VIT_MODEL_NAME = '{model_path}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Alternative download also failed: {str(e)[:200]}\")\n",
    "    print(\"\\nüí° FALLBACK OPTIONS:\")\n",
    "    print(\"  1. Use smaller model: VIT_MODEL_NAME = 'google/vit-base-patch16-224'\")\n",
    "    print(\"  2. Train without pre-training (random initialization)\")\n",
    "    print(\"  3. Wait 15 minutes and retry\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a6fc3",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Setup Training Configuration\n",
    "\n",
    "Define loss function (MSE), optimizer (AdamW), and learning rate scheduler for ViT training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a661a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class ViTForEmotionRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer for Emotion Regression\n",
    "    Uses pre-trained ViT from Hugging Face and adds regression head\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=VIT_MODEL_NAME, freeze_backbone=FREEZE_BACKBONE, dropout=DROPOUT):\n",
    "        super(ViTForEmotionRegression, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ViT model with retry logic\n",
    "        print(f\"Loading pre-trained ViT model: {model_name}...\")\n",
    "        \n",
    "        max_retries = 3\n",
    "        retry_delay = 5  # seconds\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Try loading with resume_download=True to handle interrupted downloads\n",
    "                self.vit = ViTModel.from_pretrained(\n",
    "                    model_name,\n",
    "                    resume_download=True,\n",
    "                    force_download=False,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "                print(f\"  ‚úì Model loaded successfully!\")\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"  ‚ö†Ô∏è Download attempt {attempt + 1} failed: {str(e)[:100]}\")\n",
    "                    print(f\"  ‚è≥ Retrying in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2  # Exponential backoff\n",
    "                else:\n",
    "                    print(f\"  ‚ùå All download attempts failed!\")\n",
    "                    print(f\"  üí° Trying alternative approach...\")\n",
    "                    \n",
    "                    # Fallback: Try to load from cache only\n",
    "                    try:\n",
    "                        self.vit = ViTModel.from_pretrained(\n",
    "                            model_name,\n",
    "                            local_files_only=True\n",
    "                        )\n",
    "                        print(f\"  ‚úì Loaded from local cache!\")\n",
    "                    except:\n",
    "                        # Last resort: Create model from config\n",
    "                        print(f\"  üîß Creating model from config (no pre-trained weights)...\")\n",
    "                        config = ViTConfig(\n",
    "                            hidden_size=768,\n",
    "                            num_hidden_layers=12,\n",
    "                            num_attention_heads=12,\n",
    "                            intermediate_size=3072,\n",
    "                            image_size=224,\n",
    "                            patch_size=16,\n",
    "                            num_channels=3\n",
    "                        )\n",
    "                        self.vit = ViTModel(config)\n",
    "                        print(f\"  ‚ö†Ô∏è WARNING: Using randomly initialized ViT (no transfer learning)\")\n",
    "        \n",
    "        # Freeze backbone if specified\n",
    "        if freeze_backbone:\n",
    "            for param in self.vit.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"  ‚úì ViT backbone frozen\")\n",
    "        else:\n",
    "            print(\"  ‚úì ViT backbone trainable\")\n",
    "        \n",
    "        # Get hidden size from ViT config\n",
    "        self.hidden_size = self.vit.config.hidden_size  # 768 for base model\n",
    "        \n",
    "        # Regression head for valence and arousal\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.hidden_size),\n",
    "            nn.Linear(self.hidden_size, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 2)  # Valence and Arousal\n",
    "        )\n",
    "        \n",
    "    def forward(self, pixel_values):\n",
    "        # Get ViT outputs\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        cls_output = outputs.last_hidden_state[:, 0]  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Regression head\n",
    "        emotion_output = self.regression_head(cls_output)  # (batch_size, 2)\n",
    "        \n",
    "        return emotion_output\n",
    "\n",
    "\n",
    "# Initialize model with error handling\n",
    "print(\"=\" * 60)\n",
    "print(\"ü§ñ INITIALIZING VISION TRANSFORMER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    model = ViTForEmotionRegression(\n",
    "        model_name=VIT_MODEL_NAME,\n",
    "        freeze_backbone=FREEZE_BACKBONE,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Print model summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ MODEL INITIALIZED SUCCESSFULLY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Model: {VIT_MODEL_NAME}\")\n",
    "    print(f\"Hidden size: {model.hidden_size}\")\n",
    "    print(f\"Freeze backbone: {FREEZE_BACKBONE}\")\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR: Failed to initialize model\")\n",
    "    print(f\"Error details: {str(e)}\")\n",
    "    print(\"\\nüí° TROUBLESHOOTING TIPS:\")\n",
    "    print(\"  1. Check your internet connection\")\n",
    "    print(\"  2. Try restarting the kernel and running again\")\n",
    "    print(\"  3. Manually download the model from: https://huggingface.co/google/vit-base-patch16-224-in21k\")\n",
    "    print(\"  4. Set local_files_only=True if model is already downloaded\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8236f22c",
   "metadata": {},
   "source": [
    "### üìä Define Evaluation Metrics\n",
    "\n",
    "Define Concordance Correlation Coefficient (CCC) for measuring prediction quality on valence and arousal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9766538",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Visualize Training Results & Analysis\n",
    "\n",
    "Visualize training curves, create scatter plots of predictions vs actual values, and analyze model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== FINAL RESULTS SUMMARY ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model Architecture Summary\n",
    "print(f\"\\nüñºÔ∏è Model Architecture:\")\n",
    "if 'best_vit_model' in locals():\n",
    "    total_params = sum(p.numel() for p in best_vit_model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in best_vit_model.parameters() if p.requires_grad)\n",
    "    print(f\"  - Base Model: {VIT_MODEL_NAME}\")\n",
    "    print(f\"  - Total Parameters: {total_params:,}\")\n",
    "    print(f\"  - Trainable Parameters: {trainable_params:,}\")\n",
    "    if FREEZE_BACKBONE:\n",
    "        print(f\"  - Backbone: Frozen\")\n",
    "    else:\n",
    "        print(f\"  - Backbone: Fine-tuned\")\n",
    "else:\n",
    "    print(f\"  - Model not initialized\")\n",
    "\n",
    "# GAN Augmentation Summary (using cached counts)\n",
    "print(f\"\\nüé® GAN Augmentation:\")\n",
    "if 'DATASET_COUNTS' in locals():\n",
    "    print(f\"  - Real samples: {DATASET_COUNTS['real_count']}\")\n",
    "    print(f\"  - Synthetic samples: {DATASET_COUNTS['synthetic_count']}\")\n",
    "    print(f\"  - Total samples: {DATASET_COUNTS['total_count']}\")\n",
    "    print(f\"  - Augmentation factor: {DATASET_COUNTS['augmentation_factor']:.2f}x\")\n",
    "else:\n",
    "    print(f\"  - Dataset counts not available\")\n",
    "\n",
    "# Dataset Split Summary\n",
    "print(f\"\\nüîÄ Dataset Split:\")\n",
    "if 'DATASET_COUNTS' in locals() and 'train_count' in DATASET_COUNTS:\n",
    "    train_pct = (DATASET_COUNTS['train_count'] / DATASET_COUNTS['total_count']) * 100\n",
    "    val_pct = (DATASET_COUNTS['val_count'] / DATASET_COUNTS['total_count']) * 100\n",
    "    test_pct = (DATASET_COUNTS['test_count'] / DATASET_COUNTS['total_count']) * 100\n",
    "    \n",
    "    print(f\"  - Train:      {DATASET_COUNTS['train_count']:5d} samples ({train_pct:.1f}%)\")\n",
    "    print(f\"  - Validation: {DATASET_COUNTS['val_count']:5d} samples ({val_pct:.1f}%)\")\n",
    "    print(f\"  - Test:       {DATASET_COUNTS['test_count']:5d} samples ({test_pct:.1f}%)\")\n",
    "else:\n",
    "    print(f\"  - Split information not available\")\n",
    "\n",
    "# Training Configuration\n",
    "print(f\"\\n‚öôÔ∏è  Training Configuration:\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Weight Decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  - Image Size: {VIT_IMAGE_SIZE}x{VIT_IMAGE_SIZE}\")\n",
    "\n",
    "# Training Results Summary\n",
    "print(f\"\\nüìà Training Results:\")\n",
    "if 'training_history' in locals() and len(training_history['train_loss']) > 0:\n",
    "    best_val_ccc = max([max(v, a) for v, a in zip(training_history['val_ccc_valence'], training_history['val_ccc_arousal'])])\n",
    "    best_val_mae = min(training_history['val_mae'])\n",
    "    final_train_loss = training_history['train_loss'][-1]\n",
    "    final_val_loss = training_history['val_loss'][-1]\n",
    "    \n",
    "    print(f\"  - Best Validation CCC: {best_val_ccc:.4f}\")\n",
    "    print(f\"  - Best Validation MAE: {best_val_mae:.4f}\")\n",
    "    print(f\"  - Final Train Loss: {final_train_loss:.4f}\")\n",
    "    print(f\"  - Final Val Loss: {final_val_loss:.4f}\")\n",
    "    print(f\"  - Total Epochs Trained: {len(training_history['train_loss'])}\")\n",
    "else:\n",
    "    print(f\"  - Training history not available\")\n",
    "\n",
    "# Test Results (if available)\n",
    "if 'test_results' in locals():\n",
    "    print(f\"\\nüß™ Test Set Performance:\")\n",
    "    print(f\"  - Test Loss: {test_results.get('test_loss', 'N/A')}\")\n",
    "    print(f\"  - Test MAE: {test_results.get('test_mae', 'N/A')}\")\n",
    "    print(f\"  - Test CCC (Valence): {test_results.get('test_ccc_valence', 'N/A')}\")\n",
    "    print(f\"  - Test CCC (Arousal): {test_results.get('test_ccc_arousal', 'N/A')}\")\n",
    "\n",
    "# Memory Usage\n",
    "print(f\"\\nüíæ Memory Usage:\")\n",
    "if torch.cuda.is_available():\n",
    "    current_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "    max_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    print(f\"  - Current GPU Memory: {current_memory:.2f} GB\")\n",
    "    print(f\"  - Peak GPU Memory: {max_memory:.2f} GB\")\n",
    "    print(f\"  - Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(f\"  - GPU: Not available\")\n",
    "    print(f\"  - Using CPU\")\n",
    "\n",
    "# Output Files\n",
    "print(f\"\\nüìÅ Output Files:\")\n",
    "print(f\"  - Output Directory: {OUTPUT_DIR}\")\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    output_files = os.listdir(OUTPUT_DIR)\n",
    "    print(f\"  - Files Created: {len(output_files)}\")\n",
    "    \n",
    "    # Key files\n",
    "    key_files = [\n",
    "        'best_vit_model.pth',\n",
    "        'training_history.npy',\n",
    "        'training_curves.png',\n",
    "        'confusion_matrix.png'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n  Key Output Files:\")\n",
    "    for file in key_files:\n",
    "        file_path = os.path.join(OUTPUT_DIR, file)\n",
    "        if os.path.exists(file_path):\n",
    "            file_size = os.path.getsize(file_path) / 1024**2  # MB\n",
    "            print(f\"    ‚úÖ {file} ({file_size:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"    ‚ùå {file} (not found)\")\n",
    "else:\n",
    "    print(f\"  - Output directory not found\")\n",
    "\n",
    "# Completion Status\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚úÖ Notebook Execution Complete!\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# Next Steps\n",
    "print(f\"\\nüí° Next Steps:\")\n",
    "print(f\"  1. Review training curves and metrics\")\n",
    "print(f\"  2. Analyze test set performance\")\n",
    "print(f\"  3. Examine edge case predictions\")\n",
    "print(f\"  4. Export model for deployment\")\n",
    "print(f\"  5. Document findings and insights\")\n",
    "\n",
    "print(f\"\\nüéâ Thank you for using the ViT + GAN Emotion Prediction Pipeline!\")\n",
    "print(f\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f3bf30",
   "metadata": {},
   "source": [
    "### üèãÔ∏è Define Training and Validation Functions\n",
    "\n",
    "Implement the training loop and validation function with CCC metric tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea0646c",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Train ViT Model on Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d3b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== MEMORY-EFFICIENT DATASET PREPARATION ==========\n",
    "import gc\n",
    "\n",
    "print(\"üîÑ Preparing augmented dataset for ViT training...\")\n",
    "print(f\"üíæ Memory before concatenation: {torch.cuda.memory_allocated()/1024**3:.2f} GB\" if torch.cuda.is_available() else \"\")\n",
    "\n",
    "# Clear any unused memory before concatenation\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Combine real and synthetic spectrograms\n",
    "print(f\"\\nüì¶ Combining datasets:\")\n",
    "print(f\"   Real spectrograms: {real_spectrograms.shape}\")\n",
    "print(f\"   Synthetic spectrograms: {synthetic_spectrograms.shape}\")\n",
    "print(f\"   Real labels: {real_labels_np.shape}\")  # Use prepared numpy version\n",
    "print(f\"   Synthetic labels: {synthetic_labels.shape}\")\n",
    "\n",
    "# ========== STORE COUNTS BEFORE DELETION ==========\n",
    "# These will be used in final summary since we'll delete the arrays\n",
    "DATASET_COUNTS = {\n",
    "    'real_count': len(real_spectrograms),\n",
    "    'synthetic_count': len(synthetic_spectrograms),\n",
    "    'real_label_count': len(real_labels_np),\n",
    "    'synthetic_label_count': len(synthetic_labels)\n",
    "}\n",
    "\n",
    "try:\n",
    "    all_spectrograms = np.concatenate([real_spectrograms, synthetic_spectrograms], axis=0)\n",
    "    all_labels = np.concatenate([real_labels_np, synthetic_labels], axis=0)  # FIX: Use real_labels_np\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total augmented dataset:\")\n",
    "    print(f\"   - Total samples: {len(all_spectrograms)}\")\n",
    "    print(f\"   - Spectrograms shape: {all_spectrograms.shape}\")\n",
    "    print(f\"   - Labels shape: {all_labels.shape}\")\n",
    "    print(f\"   - Memory usage: ~{all_spectrograms.nbytes / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Update counts with final total\n",
    "    DATASET_COUNTS['total_count'] = len(all_spectrograms)\n",
    "    DATASET_COUNTS['augmentation_factor'] = len(all_spectrograms) / DATASET_COUNTS['real_count']\n",
    "    \n",
    "except MemoryError as e:\n",
    "    print(f\"\\n‚ùå MemoryError during concatenation: {e}\")\n",
    "    print(f\"üîß Reducing synthetic samples to fit in memory...\")\n",
    "    \n",
    "    # Reduce synthetic samples if OOM\n",
    "    max_synthetic = 2000  # Reduce from 3192 to 2000\n",
    "    synthetic_spectrograms = synthetic_spectrograms[:max_synthetic]\n",
    "    synthetic_labels = synthetic_labels[:max_synthetic]\n",
    "    \n",
    "    print(f\"   Reduced synthetic samples to: {max_synthetic}\")\n",
    "    \n",
    "    # Update counts\n",
    "    DATASET_COUNTS['synthetic_count'] = max_synthetic\n",
    "    DATASET_COUNTS['synthetic_label_count'] = max_synthetic\n",
    "    \n",
    "    # Try again\n",
    "    all_spectrograms = np.concatenate([real_spectrograms, synthetic_spectrograms], axis=0)\n",
    "    all_emotion_labels = np.concatenate([emotion_labels_real, emotion_labels_synthetic], axis=0)\n",
    "    \n",
    "    DATASET_COUNTS['total_count'] = len(all_spectrograms)\n",
    "    DATASET_COUNTS['augmentation_factor'] = len(all_spectrograms) / DATASET_COUNTS['real_count']\n",
    "    \n",
    "    print(f\"‚úÖ Reduced dataset created: {len(all_spectrograms)} samples\")\n",
    "\n",
    "# Delete intermediate arrays to free memory\n",
    "print(f\"\\nüßπ Freeing intermediate memory...\")\n",
    "del synthetic_spectrograms  # Delete synthetic spectrograms (we have all_spectrograms now)\n",
    "del emotion_labels_synthetic  # Delete synthetic labels (we have all_emotion_labels now)\n",
    "\n",
    "# Don't delete real_spectrograms yet - needed for evaluation\n",
    "gc.collect()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"üíæ GPU memory after cleanup: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "print(f\"‚úÖ Memory cleanup complete\\n\")\n",
    "\n",
    "\n",
    "# ========== TRAIN/TEST/VALIDATION SPLIT ==========\n",
    "print(\"üîÄ Splitting dataset into train/test/validation sets...\")\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train+val (80%) and test (20%)\n",
    "train_val_specs, test_specs, train_val_labels, test_labels = train_test_split(\n",
    "    all_spectrograms, \n",
    "    all_emotion_labels, \n",
    "    test_size=0.2,  # 20% for test\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Second split: Split train+val into 80% train, 20% val (of the 80%)\n",
    "# This gives us: 64% train, 16% val, 20% test\n",
    "train_specs, val_specs, train_labels, val_labels = train_test_split(\n",
    "    train_val_specs,\n",
    "    train_val_labels,\n",
    "    test_size=0.2,  # 20% of train+val = 16% of total\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Store split counts\n",
    "DATASET_COUNTS['train_count'] = len(train_specs)\n",
    "DATASET_COUNTS['val_count'] = len(val_specs)\n",
    "DATASET_COUNTS['test_count'] = len(test_specs)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset split complete:\")\n",
    "print(f\"   üéì Train:      {len(train_specs):5d} samples ({len(train_specs)/len(all_spectrograms)*100:.1f}%)\")\n",
    "print(f\"   üìä Validation: {len(val_specs):5d} samples ({len(val_specs)/len(all_spectrograms)*100:.1f}%)\")\n",
    "print(f\"   üß™ Test:       {len(test_specs):5d} samples ({len(test_specs)/len(all_spectrograms)*100:.1f}%)\")\n",
    "print(f\"   üì¶ Total:      {len(all_spectrograms):5d} samples\")\n",
    "\n",
    "# Free the full dataset now that we have splits\n",
    "del all_spectrograms\n",
    "del all_emotion_labels\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# ========== MEMORY-EFFICIENT DATASET CLASS ==========\n",
    "class SpectrogramDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Memory-efficient dataset for mel-spectrograms with ViT preprocessing.\n",
    "    Performs preprocessing on-the-fly instead of storing preprocessed data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spectrograms, labels, image_size=VIT_IMAGE_SIZE):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spectrograms: numpy array of shape (N, n_mels, time_steps)\n",
    "            labels: numpy array of shape (N, 2)\n",
    "            image_size: target image size for ViT (default 224)\n",
    "        \"\"\"\n",
    "        # Validate input shapes\n",
    "        assert len(spectrograms) == len(labels), \\\n",
    "            f\"Spectrogram count ({len(spectrograms)}) must match label count ({len(labels)})\"\n",
    "        assert labels.shape[1] == 2, \\\n",
    "            f\"Labels must have shape (N, 2), got {labels.shape}\"\n",
    "        \n",
    "        # Store as numpy arrays (more memory efficient than tensors)\n",
    "        self.spectrograms = spectrograms\n",
    "        self.labels = labels\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        print(f\"  üìä Dataset created: {len(self.spectrograms)} samples\")\n",
    "        print(f\"     Spectrograms: {self.spectrograms.shape}\")\n",
    "        print(f\"     Labels: {self.labels.shape}\")\n",
    "        \n",
    "        # Precompute normalization constants\n",
    "        self.imagenet_mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
    "        self.imagenet_std = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.spectrograms)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get spectrogram and label with on-the-fly preprocessing.\n",
    "        This saves memory by not storing preprocessed tensors.\n",
    "        \"\"\"\n",
    "        # Get spectrogram and label (as numpy arrays)\n",
    "        spec = self.spectrograms[idx]  # Shape: (n_mels, time_steps)\n",
    "        label = self.labels[idx]  # Shape: (2,)\n",
    "        \n",
    "        # Normalize spectrogram to [0, 1]\n",
    "        spec_min = spec.min()\n",
    "        spec_max = spec.max()\n",
    "        spec_norm = (spec - spec_min) / (spec_max - spec_min + 1e-8)\n",
    "        \n",
    "        # Convert to tensor and resize to ViT input size (224x224)\n",
    "        spec_tensor = torch.FloatTensor(spec_norm).unsqueeze(0)  # Add channel dim: (1, H, W)\n",
    "        spec_resized = F.interpolate(\n",
    "            spec_tensor.unsqueeze(0),  # Add batch dim: (1, 1, H, W)\n",
    "            size=(self.image_size, self.image_size), \n",
    "            mode='bilinear', \n",
    "            align_corners=False\n",
    "        ).squeeze(0)  # Remove batch dim: (1, 224, 224)\n",
    "        \n",
    "        # Convert to 3 channels (RGB) by triplicating\n",
    "        spec_rgb = spec_resized.repeat(3, 1, 1)  # (3, 224, 224)\n",
    "        \n",
    "        # Apply ImageNet normalization\n",
    "        spec_normalized = (spec_rgb - self.imagenet_mean) / self.imagenet_std\n",
    "        \n",
    "        return spec_normalized, torch.FloatTensor(label)\n",
    "\n",
    "\n",
    "# ========== CREATE DATASETS AND DATALOADERS ==========\n",
    "print(\"\\nüîÑ Creating datasets and dataloaders...\")\n",
    "\n",
    "try:\n",
    "    # Create datasets for train, validation, and test\n",
    "    print(\"\\nüì¶ Creating datasets:\")\n",
    "    train_dataset = SpectrogramDataset(train_specs, train_labels)\n",
    "    val_dataset = SpectrogramDataset(val_specs, val_labels)\n",
    "    test_dataset = SpectrogramDataset(test_specs, test_labels)\n",
    "    \n",
    "    print(f\"\\n‚úÖ All datasets created successfully\")\n",
    "    \n",
    "    # Create dataloaders with memory-efficient settings\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=0,  # Set to 0 to avoid multiprocessing memory overhead\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False  # Don't keep workers alive\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataloaders created:\")\n",
    "    print(f\"   üéì Train:      {len(train_loader):4d} batches\")\n",
    "    print(f\"   üìä Validation: {len(val_loader):4d} batches\")\n",
    "    print(f\"   üß™ Test:       {len(test_loader):4d} batches\")\n",
    "    print(f\"   üì¶ Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    # Final memory cleanup\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\nüíæ Final GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset preparation complete! Ready for ViT training.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during dataset creation: {e}\")\n",
    "    print(f\"üí° Suggestion: Reduce NUM_SYNTHETIC or BATCH_SIZE in configuration\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f794ecff",
   "metadata": {},
   "source": [
    "### üöÄ Execute Training Loop\n",
    "\n",
    "Run the complete training process for the specified number of epochs with validation after each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d656f62",
   "metadata": {},
   "source": [
    "## üîü Visualize Results & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('MSE Loss')\n",
    "axes[0, 0].set_title('Training & Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[0, 1].plot(history['train_mae'], label='Train MAE', linewidth=2)\n",
    "axes[0, 1].plot(history['val_mae'], label='Val MAE', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Mean Absolute Error')\n",
    "axes[0, 1].set_title('Training & Validation MAE')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# CCC Valence\n",
    "axes[1, 0].plot(history['train_ccc_v'], label='Train CCC', linewidth=2)\n",
    "axes[1, 0].plot(history['val_ccc_v'], label='Val CCC', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('CCC')\n",
    "axes[1, 0].set_title('Valence CCC')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "# CCC Arousal\n",
    "axes[1, 1].plot(history['train_ccc_a'], label='Train CCC', linewidth=2)\n",
    "axes[1, 1].plot(history['val_ccc_a'], label='Val CCC', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('CCC')\n",
    "axes[1, 1].set_title('Arousal CCC')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'vit_training_curves.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plots: Predicted vs Actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Valence\n",
    "axes[0].scatter(val_labels[:, 0], val_preds[:, 0], alpha=0.5, s=20)\n",
    "axes[0].plot([-1, 1], [-1, 1], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Valence')\n",
    "axes[0].set_ylabel('Predicted Valence')\n",
    "axes[0].set_title(f'Valence Prediction (CCC: {val_ccc_v:.4f})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim(-1.2, 1.2)\n",
    "axes[0].set_ylim(-1.2, 1.2)\n",
    "\n",
    "# Arousal\n",
    "axes[1].scatter(val_labels[:, 1], val_preds[:, 1], alpha=0.5, s=20)\n",
    "axes[1].plot([-1, 1], [-1, 1], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Arousal')\n",
    "axes[1].set_ylabel('Predicted Arousal')\n",
    "axes[1].set_title(f'Arousal Prediction (CCC: {val_ccc_a:.4f})')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(-1.2, 1.2)\n",
    "axes[1].set_ylim(-1.2, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'prediction_scatter.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2D Valence-Arousal Space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Ground Truth\n",
    "axes[0].scatter(val_labels[:, 0], val_labels[:, 1], alpha=0.6, s=50, c='blue', edgecolors='black')\n",
    "axes[0].set_xlabel('Valence')\n",
    "axes[0].set_ylabel('Arousal')\n",
    "axes[0].set_title('Ground Truth VA Space')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(0, color='k', linewidth=0.5)\n",
    "axes[0].axvline(0, color='k', linewidth=0.5)\n",
    "axes[0].set_xlim(-1.2, 1.2)\n",
    "axes[0].set_ylim(-1.2, 1.2)\n",
    "\n",
    "# Predictions\n",
    "axes[1].scatter(val_preds[:, 0], val_preds[:, 1], alpha=0.6, s=50, c='red', edgecolors='black')\n",
    "axes[1].set_xlabel('Valence')\n",
    "axes[1].set_ylabel('Arousal')\n",
    "axes[1].set_title('Predicted VA Space')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(0, color='k', linewidth=0.5)\n",
    "axes[1].axvline(0, color='k', linewidth=0.5)\n",
    "axes[1].set_xlim(-1.2, 1.2)\n",
    "axes[1].set_ylim(-1.2, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'va_space_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final summary\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\\\nüñºÔ∏è Model Architecture:\")\n",
    "print(f\"  - Base Model: {VIT_MODEL_NAME}\")\n",
    "print(f\"  - Total Parameters: {total_params:,}\")\n",
    "print(f\"  - Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "print(f\"\\\\nüé® GAN Augmentation:\")\n",
    "print(f\"  - Real samples: {len(real_spectrograms)}\")\n",
    "print(f\"  - Synthetic samples: {len(synthetic_spectrograms)}\")\n",
    "print(f\"  - Total samples: {len(all_spectrograms)}\")\n",
    "print(f\"  - Augmentation factor: {len(all_spectrograms)/len(real_spectrograms):.2f}x\")\n",
    "\n",
    "print(f\"\\\\nü§ñ ViT Model Performance:\")\n",
    "print(f\"  - Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"  - Final Val MAE: {val_mae:.4f}\")\n",
    "print(f\"  - Final Val CCC Valence: {val_ccc_v:.4f}\")\n",
    "print(f\"  - Final Val CCC Arousal: {val_ccc_a:.4f}\")\n",
    "\n",
    "print(f\"\\\\nüíæ Saved Outputs:\")\n",
    "print(f\"  - Generator model: generator.pth\")\n",
    "print(f\"  - Discriminator model: discriminator.pth\")\n",
    "print(f\"  - Best ViT model: best_vit_model.pth\")\n",
    "print(f\"  - Training curves: vit_training_curves.png\")\n",
    "print(f\"  - Prediction scatter: prediction_scatter.png\")\n",
    "print(f\"  - VA space comparison: va_space_comparison.png\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac8fb7a",
   "metadata": {},
   "source": [
    "### üìà Plot Training and Validation Curves\n",
    "\n",
    "Visualize loss, MAE, and CCC metrics over training epochs to assess model convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5fe25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 24\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "PATIENCE = 5  # Early stopping patience\n",
    "\n",
    "# Initialize model, criterion, optimizer\n",
    "print(\"üöÄ Initializing ViT training...\")\n",
    "vit_model = EmotionViT().to(device)\n",
    "criterion = WeightedEmotionLoss(valence_weight=0.6, arousal_weight=0.4)\n",
    "optimizer = torch.optim.AdamW(vit_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'train_valence_loss': [], 'train_arousal_loss': [],\n",
    "    'val_loss': [], 'val_valence_loss': [], 'val_arousal_loss': [],\n",
    "    'val_valence_ccc': [], 'val_arousal_ccc': []\n",
    "}\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_path = 'best_vit_emotion_model.pth'\n",
    "\n",
    "print(f\"üìä Training for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"üìà Dataset sizes: Train={DATASET_COUNTS['train']}, Val={DATASET_COUNTS['val']}, Test={DATASET_COUNTS['test']}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_val_loss, train_aro_loss = train_epoch(\n",
    "        vit_model, train_loader, criterion, optimizer, device, epoch\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_val_loss, val_aro_loss, val_valence_ccc, val_arousal_ccc = evaluate_epoch(\n",
    "        vit_model, val_loader, criterion, device, epoch, phase='Val'\n",
    "    )\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_valence_loss'].append(train_val_loss)\n",
    "    history['train_arousal_loss'].append(train_aro_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_valence_loss'].append(val_val_loss)\n",
    "    history['val_arousal_loss'].append(val_aro_loss)\n",
    "    history['val_valence_ccc'].append(val_valence_ccc)\n",
    "    history['val_arousal_ccc'].append(val_arousal_ccc)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Valence CCC: {val_valence_ccc:.4f} | Arousal CCC: {val_arousal_ccc:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Early stopping and model saving\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': vit_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'history': history\n",
    "        }, best_model_path)\n",
    "        print(f\"‚úÖ Saved best model (val_loss={val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"‚è≥ Patience: {patience_counter}/{PATIENCE}\")\n",
    "        \n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"üõë Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "print(\"\\nüéâ Training completed!\")\n",
    "print(f\"‚úÖ Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cef83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test set\n",
    "print(\"üß™ Evaluating on test set...\")\n",
    "checkpoint = torch.load(best_model_path)\n",
    "vit_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "test_loss, test_val_loss, test_aro_loss, test_valence_ccc, test_arousal_ccc = evaluate_epoch(\n",
    "    vit_model, test_loader, criterion, device, epoch=0, phase='Test'\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìä FINAL TEST SET RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  Valence Loss: {test_val_loss:.4f} | CCC: {test_valence_ccc:.4f}\")\n",
    "print(f\"  Arousal Loss: {test_aro_loss:.4f} | CCC: {test_arousal_ccc:.4f}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationLoss(nn.Module):\n",
    "    \"\"\"Combined distillation loss: MSE + Diffusion denoising\"\"\"\n",
    "    def __init__(self, alpha=0.5, temperature=3.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Balance between MSE and denoising\n",
    "        self.temperature = temperature\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, student_pred, teacher_pred, noisy_teacher, student_noise_pred, noise_target):\n",
    "        \"\"\"\n",
    "        student_pred: Student's direct prediction\n",
    "        teacher_pred: Teacher's clean prediction\n",
    "        noisy_teacher: Teacher prediction with added noise\n",
    "        student_noise_pred: Student's attempt to denoise\n",
    "        noise_target: Actual noise that was added\n",
    "        \"\"\"\n",
    "        # Direct prediction loss (MSE between student and teacher)\n",
    "        pred_loss = self.mse(student_pred, teacher_pred)\n",
    "        \n",
    "        # Denoising loss (student learns to predict the noise)\n",
    "        denoise_loss = self.mse(student_noise_pred, noise_target)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = self.alpha * pred_loss + (1 - self.alpha) * denoise_loss\n",
    "        \n",
    "        return total_loss, pred_loss, denoise_loss\n",
    "\n",
    "distillation_criterion = DistillationLoss(alpha=0.6)  # 60% direct, 40% denoising\n",
    "print(\"‚úÖ Distillation loss initialized (60% MSE, 40% denoising)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distillation curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Total loss\n",
    "axes[0].plot(distill_history['train_loss'], marker='o', color='purple')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Distillation Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Component losses\n",
    "axes[1].plot(distill_history['train_pred_loss'], label='Prediction Loss', marker='o')\n",
    "axes[1].plot(distill_history['train_denoise_loss'], label='Denoising Loss', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Loss Components')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distillation_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Distillation curves saved to 'distillation_curves.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de29e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Teacher - Valence\n",
    "axes[0, 0].scatter(true_labels[:, 0], teacher_predictions[:, 0], alpha=0.5, s=20)\n",
    "axes[0, 0].plot([true_labels[:, 0].min(), true_labels[:, 0].max()], \n",
    "                [true_labels[:, 0].min(), true_labels[:, 0].max()], \n",
    "                'r--', lw=2, label='Perfect prediction')\n",
    "axes[0, 0].set_xlabel('True Valence')\n",
    "axes[0, 0].set_ylabel('Predicted Valence')\n",
    "axes[0, 0].set_title(f'Teacher Model - Valence (CCC={teacher_val_ccc:.4f})')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Teacher - Arousal\n",
    "axes[0, 1].scatter(true_labels[:, 1], teacher_predictions[:, 1], alpha=0.5, s=20, color='orange')\n",
    "axes[0, 1].plot([true_labels[:, 1].min(), true_labels[:, 1].max()], \n",
    "                [true_labels[:, 1].min(), true_labels[:, 1].max()], \n",
    "                'r--', lw=2, label='Perfect prediction')\n",
    "axes[0, 1].set_xlabel('True Arousal')\n",
    "axes[0, 1].set_ylabel('Predicted Arousal')\n",
    "axes[0, 1].set_title(f'Teacher Model - Arousal (CCC={teacher_aro_ccc:.4f})')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Student - Valence\n",
    "axes[1, 0].scatter(true_labels[:, 0], student_predictions[:, 0], alpha=0.5, s=20, color='green')\n",
    "axes[1, 0].plot([true_labels[:, 0].min(), true_labels[:, 0].max()], \n",
    "                [true_labels[:, 0].min(), true_labels[:, 0].max()], \n",
    "                'r--', lw=2, label='Perfect prediction')\n",
    "axes[1, 0].set_xlabel('True Valence')\n",
    "axes[1, 0].set_ylabel('Predicted Valence')\n",
    "axes[1, 0].set_title(f'Student Model - Valence (CCC={student_val_ccc:.4f})')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Student - Arousal\n",
    "axes[1, 1].scatter(true_labels[:, 1], student_predictions[:, 1], alpha=0.5, s=20, color='purple')\n",
    "axes[1, 1].plot([true_labels[:, 1].min(), true_labels[:, 1].max()], \n",
    "                [true_labels[:, 1].min(), true_labels[:, 1].max()], \n",
    "                'r--', lw=2, label='Perfect prediction')\n",
    "axes[1, 1].set_xlabel('True Arousal')\n",
    "axes[1, 1].set_ylabel('Predicted Arousal')\n",
    "axes[1, 1].set_title(f'Student Model - Arousal (CCC={student_aro_ccc:.4f})')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('teacher_vs_student_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Comparison plots saved to 'teacher_vs_student_predictions.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ffd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on original songs\n",
    "if os.path.exists(deam_audio_dir) and len(ground_truth) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Valence comparison\n",
    "    x = np.arange(len(ground_truth))\n",
    "    width = 0.25\n",
    "    \n",
    "    axes[0].bar(x - width, ground_truth[:, 0], width, label='Ground Truth', alpha=0.8)\n",
    "    axes[0].bar(x, teacher_results[:, 0], width, label='Teacher', alpha=0.8)\n",
    "    axes[0].bar(x + width, student_results[:, 0], width, label='Student', alpha=0.8)\n",
    "    axes[0].set_xlabel('Song Sample')\n",
    "    axes[0].set_ylabel('Valence')\n",
    "    axes[0].set_title('Valence Predictions on Original Songs')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Arousal comparison\n",
    "    axes[1].bar(x - width, ground_truth[:, 1], width, label='Ground Truth', alpha=0.8)\n",
    "    axes[1].bar(x, teacher_results[:, 1], width, label='Teacher', alpha=0.8)\n",
    "    axes[1].bar(x + width, student_results[:, 1], width, label='Student', alpha=0.8)\n",
    "    axes[1].set_xlabel('Song Sample')\n",
    "    axes[1].set_ylabel('Arousal')\n",
    "    axes[1].set_title('Arousal Predictions on Original Songs')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('original_songs_predictions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Original songs visualization saved to 'original_songs_predictions.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbebf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\" \" * 35 + \"üéâ COMPLETE PIPELINE SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nüìä DATASET STATISTICS:\")\n",
    "print(f\"  ‚Ä¢ Real DEAM samples: {DATASET_COUNTS['real']}\")\n",
    "print(f\"  ‚Ä¢ GAN-generated samples: {DATASET_COUNTS['synthetic']}\")\n",
    "print(f\"  ‚Ä¢ Total samples: {DATASET_COUNTS['total']}\")\n",
    "print(f\"  ‚Ä¢ Train set: {DATASET_COUNTS['train']} (64%)\")\n",
    "print(f\"  ‚Ä¢ Validation set: {DATASET_COUNTS['val']} (16%)\")\n",
    "print(f\"  ‚Ä¢ Test set: {DATASET_COUNTS['test']} (20%)\")\n",
    "\n",
    "print(\"\\nüéì TEACHER MODEL (Full Vision Transformer):\")\n",
    "print(f\"  ‚Ä¢ Architecture: google/vit-base-patch16-224-in21k\")\n",
    "print(f\"  ‚Ä¢ Parameters: {count_parameters(vit_model):,}\")\n",
    "print(f\"  ‚Ä¢ Test Valence CCC: {test_valence_ccc:.4f}\")\n",
    "print(f\"  ‚Ä¢ Test Arousal CCC: {test_arousal_ccc:.4f}\")\n",
    "print(f\"  ‚Ä¢ Model saved: {best_model_path}\")\n",
    "\n",
    "print(\"\\nüî¨ STUDENT MODEL (Diffusion-Compressed):\")\n",
    "print(f\"  ‚Ä¢ Architecture: Lightweight ViT (6 layers, 384 hidden)\")\n",
    "print(f\"  ‚Ä¢ Parameters: {count_parameters(student_model):,}\")\n",
    "print(f\"  ‚Ä¢ Compression: {count_parameters(vit_model) / count_parameters(student_model):.1f}x smaller\")\n",
    "print(f\"  ‚Ä¢ Test Valence CCC: {student_val_ccc:.4f} ({val_ccc_drop:.2f}% drop)\")\n",
    "print(f\"  ‚Ä¢ Test Arousal CCC: {student_aro_ccc:.4f} ({aro_ccc_drop:.2f}% drop)\")\n",
    "print(f\"  ‚Ä¢ Model saved: {student_model_path}\")\n",
    "\n",
    "print(\"\\nüí° KEY INSIGHTS:\")\n",
    "print(f\"  ‚Ä¢ GAN augmentation provided {DATASET_COUNTS['synthetic']/DATASET_COUNTS['real']:.1f}x more training data\")\n",
    "print(f\"  ‚Ä¢ Diffusion-based distillation achieved {count_parameters(vit_model) / count_parameters(student_model):.1f}x compression\")\n",
    "print(f\"  ‚Ä¢ Student model maintains {100 - max(val_ccc_drop, aro_ccc_drop):.1f}% of teacher performance\")\n",
    "print(f\"  ‚Ä¢ Memory savings: ~{(count_parameters(vit_model) - count_parameters(student_model)) * 4 / 1e6:.1f} MB\")\n",
    "\n",
    "print(\"\\nüìà TRAINING DETAILS:\")\n",
    "print(f\"  ‚Ä¢ ViT Epochs: {len(history['train_loss'])} / {NUM_EPOCHS}\")\n",
    "print(f\"  ‚Ä¢ Distillation Epochs: {DISTILL_EPOCHS}\")\n",
    "print(f\"  ‚Ä¢ Best Validation Loss: {best_val_loss:.4f}\")\n",
    "print(f\"  ‚Ä¢ Loss Function: Weighted (60% valence, 40% arousal)\")\n",
    "\n",
    "print(\"\\n‚úÖ DELIVERABLES:\")\n",
    "print(f\"  ‚Ä¢ Trained Teacher Model: {best_model_path}\")\n",
    "print(f\"  ‚Ä¢ Compressed Student Model: {student_model_path}\")\n",
    "print(f\"  ‚Ä¢ Training Curves: vit_training_curves.png\")\n",
    "print(f\"  ‚Ä¢ Distillation Curves: distillation_curves.png\")\n",
    "print(f\"  ‚Ä¢ Prediction Comparisons: teacher_vs_student_predictions.png\")\n",
    "if os.path.exists(deam_audio_dir):\n",
    "    print(f\"  ‚Ä¢ Original Songs Test: original_songs_predictions.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\" \" * 30 + \"üöÄ Pipeline Execution Complete!\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb7a8ff",
   "metadata": {},
   "source": [
    "## üéØ Final Summary\n",
    "\n",
    "Complete pipeline summary with all metrics and model information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62943c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find original DEAM audio files\n",
    "deam_audio_dir = '/kaggle/input/deam-dataset/DEAM_audio/MEMD_audio'\n",
    "import os\n",
    "\n",
    "if os.path.exists(deam_audio_dir):\n",
    "    audio_files = [f for f in os.listdir(deam_audio_dir) if f.endswith('.mp3')]\n",
    "    print(f\"üìÅ Found {len(audio_files)} DEAM audio files\")\n",
    "    \n",
    "    # Test on random sample of 20 songs\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    sample_files = random.sample(audio_files, min(20, len(audio_files)))\n",
    "    \n",
    "    print(f\"üéµ Testing on {len(sample_files)} random songs...\")\n",
    "    \n",
    "    teacher_results = []\n",
    "    student_results = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    vit_model.eval()\n",
    "    student_model.eval()\n",
    "    \n",
    "    for audio_file in tqdm(sample_files, desc='Processing songs'):\n",
    "        audio_path = os.path.join(deam_audio_dir, audio_file)\n",
    "        \n",
    "        # Extract song ID from filename\n",
    "        song_id = int(audio_file.split('.')[0])\n",
    "        \n",
    "        # Get ground truth labels\n",
    "        if song_id in deam_annotations_df['song_id'].values:\n",
    "            true_valence = deam_annotations_df[deam_annotations_df['song_id'] == song_id]['valence_mean'].values[0]\n",
    "            true_arousal = deam_annotations_df[deam_annotations_df['song_id'] == song_id]['arousal_mean'].values[0]\n",
    "            \n",
    "            # Process audio to spectrogram\n",
    "            spec = audio_to_spectrogram(audio_path)\n",
    "            if spec is not None:\n",
    "                spec = spec.unsqueeze(0).to(device)\n",
    "                \n",
    "                # Get predictions from both models\n",
    "                with torch.no_grad():\n",
    "                    teacher_pred = vit_model(spec).cpu().numpy()[0]\n",
    "                    student_pred = student_model(spec).cpu().numpy()[0]\n",
    "                \n",
    "                teacher_results.append(teacher_pred)\n",
    "                student_results.append(student_pred)\n",
    "                ground_truth.append([true_valence, true_arousal])\n",
    "    \n",
    "    # Convert to arrays\n",
    "    teacher_results = np.array(teacher_results)\n",
    "    student_results = np.array(student_results)\n",
    "    ground_truth = np.array(ground_truth)\n",
    "    \n",
    "    # Calculate metrics on original songs\n",
    "    orig_teacher_val_ccc = concordance_correlation_coefficient(ground_truth[:, 0], teacher_results[:, 0])\n",
    "    orig_teacher_aro_ccc = concordance_correlation_coefficient(ground_truth[:, 1], teacher_results[:, 1])\n",
    "    orig_student_val_ccc = concordance_correlation_coefficient(ground_truth[:, 0], student_results[:, 0])\n",
    "    orig_student_aro_ccc = concordance_correlation_coefficient(ground_truth[:, 1], student_results[:, 1])\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üéµ RESULTS ON ORIGINAL DEAM SONGS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Teacher Model:\")\n",
    "    print(f\"  Valence CCC: {orig_teacher_val_ccc:.4f} | Arousal CCC: {orig_teacher_aro_ccc:.4f}\")\n",
    "    print(f\"Student Model:\")\n",
    "    print(f\"  Valence CCC: {orig_student_val_ccc:.4f} | Arousal CCC: {orig_student_aro_ccc:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå DEAM audio directory not found at {deam_audio_dir}\")\n",
    "    print(\"üí° This section requires the original DEAM audio files\")\n",
    "    print(\"üí° Skip if only spectrograms are available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4799f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_spectrogram(audio_path, target_length=431):\n",
    "    \"\"\"Convert audio file to mel-spectrogram matching DEAM format\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        y, sr = librosa.load(audio_path, sr=22050, duration=45.0)\n",
    "        \n",
    "        # Extract mel-spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=y, sr=sr, n_mels=128, n_fft=2048, hop_length=512\n",
    "        )\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Pad or truncate to target length\n",
    "        if mel_spec_db.shape[1] < target_length:\n",
    "            pad_width = target_length - mel_spec_db.shape[1]\n",
    "            mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            mel_spec_db = mel_spec_db[:, :target_length]\n",
    "        \n",
    "        # Normalize\n",
    "        mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-6)\n",
    "        \n",
    "        # Convert to 3-channel (RGB-like) format for ViT\n",
    "        mel_spec_rgb = np.stack([mel_spec_db, mel_spec_db, mel_spec_db], axis=0)\n",
    "        \n",
    "        # Resize to 224x224 for ViT\n",
    "        mel_spec_rgb = torch.from_numpy(mel_spec_rgb).float()\n",
    "        mel_spec_rgb = torch.nn.functional.interpolate(\n",
    "            mel_spec_rgb.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False\n",
    "        )\n",
    "        \n",
    "        return mel_spec_rgb.squeeze(0)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Audio processing function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7508d253",
   "metadata": {},
   "source": [
    "## üéµ Testing on Original DEAM Songs\n",
    "\n",
    "Let's test both models on original DEAM audio files (not pre-computed spectrograms) to validate real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab428be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare teacher vs student on test set\n",
    "print(\"üî¨ Comparing Teacher vs Student models on test set...\")\n",
    "\n",
    "vit_model.eval()\n",
    "student_model.eval()\n",
    "\n",
    "teacher_predictions = []\n",
    "student_predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for spectrograms, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        \n",
    "        # Teacher predictions\n",
    "        teacher_pred = vit_model(spectrograms)\n",
    "        teacher_predictions.append(teacher_pred.cpu().numpy())\n",
    "        \n",
    "        # Student predictions\n",
    "        student_pred = student_model(spectrograms)\n",
    "        student_predictions.append(student_pred.cpu().numpy())\n",
    "        \n",
    "        true_labels.append(labels.numpy())\n",
    "\n",
    "# Concatenate all predictions\n",
    "teacher_predictions = np.concatenate(teacher_predictions, axis=0)\n",
    "student_predictions = np.concatenate(student_predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Calculate metrics for both models\n",
    "teacher_val_ccc = concordance_correlation_coefficient(true_labels[:, 0], teacher_predictions[:, 0])\n",
    "teacher_aro_ccc = concordance_correlation_coefficient(true_labels[:, 1], teacher_predictions[:, 1])\n",
    "teacher_val_mae = np.mean(np.abs(true_labels[:, 0] - teacher_predictions[:, 0]))\n",
    "teacher_aro_mae = np.mean(np.abs(true_labels[:, 1] - teacher_predictions[:, 1]))\n",
    "\n",
    "student_val_ccc = concordance_correlation_coefficient(true_labels[:, 0], student_predictions[:, 0])\n",
    "student_aro_ccc = concordance_correlation_coefficient(true_labels[:, 1], student_predictions[:, 1])\n",
    "student_val_mae = np.mean(np.abs(true_labels[:, 0] - student_predictions[:, 0]))\n",
    "student_aro_mae = np.mean(np.abs(true_labels[:, 1] - student_predictions[:, 1]))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìä TEACHER MODEL (Full ViT - 86M params)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Valence - CCC: {teacher_val_ccc:.4f} | MAE: {teacher_val_mae:.4f}\")\n",
    "print(f\"  Arousal - CCC: {teacher_aro_ccc:.4f} | MAE: {teacher_aro_mae:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìä STUDENT MODEL (Lightweight - 20M params)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Valence - CCC: {student_val_ccc:.4f} | MAE: {student_val_mae:.4f}\")\n",
    "print(f\"  Arousal - CCC: {student_aro_ccc:.4f} | MAE: {student_aro_mae:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìâ PERFORMANCE DROP\")\n",
    "print(f\"{'='*80}\")\n",
    "val_ccc_drop = ((teacher_val_ccc - student_val_ccc) / teacher_val_ccc) * 100\n",
    "aro_ccc_drop = ((teacher_aro_ccc - student_aro_ccc) / teacher_aro_ccc) * 100\n",
    "print(f\"  Valence CCC: {val_ccc_drop:.2f}% drop\")\n",
    "print(f\"  Arousal CCC: {aro_ccc_drop:.2f}% drop\")\n",
    "print(f\"  Size Reduction: {count_parameters(vit_model) / count_parameters(student_model):.1f}x smaller\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation training configuration\n",
    "DISTILL_EPOCHS = 15\n",
    "DISTILL_LR = 1e-4\n",
    "\n",
    "# Freeze teacher model\n",
    "for param in vit_model.parameters():\n",
    "    param.requires_grad = False\n",
    "vit_model.eval()\n",
    "\n",
    "# Initialize student optimizer\n",
    "student_optimizer = torch.optim.AdamW(student_model.parameters(), lr=DISTILL_LR, weight_decay=0.01)\n",
    "student_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(student_optimizer, T_max=DISTILL_EPOCHS)\n",
    "\n",
    "# Distillation history\n",
    "distill_history = {\n",
    "    'train_loss': [],\n",
    "    'train_pred_loss': [],\n",
    "    'train_denoise_loss': []\n",
    "}\n",
    "\n",
    "print(\"üéì Starting diffusion-based distillation...\")\n",
    "print(f\"üìä Training for {DISTILL_EPOCHS} epochs\")\n",
    "print(f\"üéØ Teacher: {count_parameters(vit_model):,} params (frozen)\")\n",
    "print(f\"üéØ Student: {count_parameters(student_model):,} params (training)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for epoch in range(DISTILL_EPOCHS):\n",
    "    # Train student\n",
    "    loss, pred_loss, denoise_loss = train_distillation_epoch(\n",
    "        student_model, vit_model, train_loader, \n",
    "        distillation_criterion, student_optimizer, diffusion, device, epoch\n",
    "    )\n",
    "    \n",
    "    # Update scheduler\n",
    "    student_scheduler.step()\n",
    "    \n",
    "    # Store history\n",
    "    distill_history['train_loss'].append(loss)\n",
    "    distill_history['train_pred_loss'].append(pred_loss)\n",
    "    distill_history['train_denoise_loss'].append(denoise_loss)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{DISTILL_EPOCHS}:\")\n",
    "    print(f\"  Loss: {loss:.4f} | Pred: {pred_loss:.4f} | Denoise: {denoise_loss:.4f}\")\n",
    "\n",
    "# Save student model\n",
    "student_model_path = 'lightweight_vit_student.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': student_model.state_dict(),\n",
    "    'distill_history': distill_history\n",
    "}, student_model_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Distillation completed! Student model saved to '{student_model_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ff7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distillation_epoch(student, teacher, dataloader, criterion, optimizer, diffusion, device, epoch):\n",
    "    \"\"\"Train student model with diffusion-based distillation\"\"\"\n",
    "    student.train()\n",
    "    teacher.eval()  # Teacher is frozen\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_pred_loss = 0.0\n",
    "    running_denoise_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f'Distillation Epoch {epoch+1}')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        teacher_frozen = True  # Ensure teacher doesn't update\n",
    "    \n",
    "    for batch_idx, (spectrograms, _) in enumerate(progress_bar):\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        \n",
    "        # Get teacher predictions (frozen, no gradients)\n",
    "        with torch.no_grad():\n",
    "            teacher_pred = teacher(spectrograms)\n",
    "        \n",
    "        # Sample timesteps for diffusion\n",
    "        t = diffusion.get_timestep(spectrograms.shape[0], device)\n",
    "        \n",
    "        # Add noise to teacher predictions\n",
    "        noisy_teacher, noise_target = diffusion.add_noise(teacher_pred, t, device)\n",
    "        \n",
    "        # Student makes two predictions:\n",
    "        # 1. Direct prediction from input\n",
    "        student_pred = student(spectrograms)\n",
    "        \n",
    "        # 2. Denoising prediction (predicting the noise in noisy teacher output)\n",
    "        # Here we use a simple approach: predict noise as difference from noisy input\n",
    "        student_noise_pred = student_pred - noisy_teacher.detach()\n",
    "        \n",
    "        # Calculate distillation loss\n",
    "        optimizer.zero_grad()\n",
    "        loss, pred_loss, denoise_loss = criterion(\n",
    "            student_pred, teacher_pred.detach(), \n",
    "            noisy_teacher, student_noise_pred, noise_target\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        running_loss += loss.item()\n",
    "        running_pred_loss += pred_loss.item()\n",
    "        running_denoise_loss += denoise_loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "                'pred': f'{running_pred_loss/(batch_idx+1):.4f}',\n",
    "                'denoise': f'{running_denoise_loss/(batch_idx+1):.4f}'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_pred_loss = running_pred_loss / len(dataloader)\n",
    "    epoch_denoise_loss = running_denoise_loss / len(dataloader)\n",
    "    \n",
    "    return epoch_loss, epoch_pred_loss, epoch_denoise_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionDistillation:\n",
    "    \"\"\"Diffusion-based knowledge distillation\"\"\"\n",
    "    def __init__(self, num_timesteps=100, beta_start=0.0001, beta_end=0.02):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        \n",
    "        # Linear beta schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "    def add_noise(self, predictions, t, device):\n",
    "        \"\"\"Add noise to teacher predictions based on timestep t\"\"\"\n",
    "        batch_size = predictions.shape[0]\n",
    "        \n",
    "        # Get alpha for this timestep\n",
    "        alpha_t = self.alphas_cumprod[t].to(device)\n",
    "        alpha_t = alpha_t.view(-1, 1)  # Shape: (batch, 1)\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(predictions)\n",
    "        \n",
    "        # Add noise: x_t = sqrt(alpha_t) * x_0 + sqrt(1 - alpha_t) * noise\n",
    "        noisy_predictions = torch.sqrt(alpha_t) * predictions + torch.sqrt(1 - alpha_t) * noise\n",
    "        \n",
    "        return noisy_predictions, noise\n",
    "    \n",
    "    def get_timestep(self, batch_size, device):\n",
    "        \"\"\"Sample random timesteps for batch\"\"\"\n",
    "        return torch.randint(0, self.num_timesteps, (batch_size,), device=device)\n",
    "\n",
    "diffusion = DiffusionDistillation(num_timesteps=100)\n",
    "print(\"‚úÖ Diffusion scheduler initialized with 100 timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c9d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightweightViT(nn.Module):\n",
    "    \"\"\"Lightweight ViT for student model (~20M parameters)\"\"\"\n",
    "    def __init__(self):\n",
    "        super(LightweightViT, self).__init__()\n",
    "        \n",
    "        # Use smaller ViT variant\n",
    "        self.vit = ViTModel.from_pretrained(\n",
    "            \"google/vit-base-patch16-224-in21k\",\n",
    "            num_hidden_layers=6,  # Reduce from 12 to 6 layers\n",
    "            hidden_size=384,      # Reduce from 768 to 384\n",
    "            num_attention_heads=6, # Reduce from 12 to 6\n",
    "            intermediate_size=1536 # Reduce from 3072 to 1536\n",
    "        )\n",
    "        \n",
    "        # Emotion prediction head\n",
    "        self.emotion_head = nn.Sequential(\n",
    "            nn.LayerNorm(384),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(384, 192),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(192, 2)  # valence, arousal\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = self.vit(pixel_values=x)\n",
    "        pooled = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "        emotions = self.emotion_head(pooled)\n",
    "        return emotions\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"üìè Teacher Model (Full ViT): {count_parameters(vit_model):,} parameters\")\n",
    "student_model = LightweightViT().to(device)\n",
    "print(f\"üìè Student Model (Lightweight): {count_parameters(student_model):,} parameters\")\n",
    "print(f\"üéØ Compression Ratio: {count_parameters(vit_model) / count_parameters(student_model):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dcb0bf",
   "metadata": {},
   "source": [
    "## üî¨ Diffusion-Based Model Compression\n",
    "\n",
    "We'll use a diffusion-inspired knowledge distillation approach to compress the ViT model:\n",
    "- **Teacher Model**: Full ViT (86M parameters)\n",
    "- **Student Model**: Lightweight ViT (20M parameters)\n",
    "- **Approach**: Student learns to denoise teacher predictions + match outputs\n",
    "- **Goal**: 4-5x compression with <5% performance drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d606f69",
   "metadata": {},
   "source": [
    "## üéØ Knowledge Distillation for Mobile Deployment\n",
    "\n",
    "### Goal: Create a Lightweight Model for Android Phones\n",
    "\n",
    "We'll use **attention transfer distillation** to compress the full ViT model:\n",
    "\n",
    "**Teacher Model (Full ViT)**\n",
    "- Architecture: `google/vit-base-patch16-224-in21k`\n",
    "- Parameters: ~86M\n",
    "- Performance: High accuracy but requires ~350MB memory\n",
    "\n",
    "**Student Model (MobileViT)**\n",
    "- Architecture: Lightweight ViT with reduced layers\n",
    "- Parameters: ~5-8M (target)\n",
    "- Performance: >90% of teacher with <40MB memory\n",
    "- Target: Runnable on Android phones with 4GB RAM\n",
    "\n",
    "### Distillation Method: Response-Based + Feature-Based\n",
    "\n",
    "1. **Response Distillation**: Student mimics teacher's emotion predictions\n",
    "2. **Feature Distillation**: Student learns intermediate representations\n",
    "3. **Attention Transfer**: Student learns where teacher \"looks\"\n",
    "\n",
    "### Expected Results\n",
    "- Model size: 10-15x smaller (~25-40MB)\n",
    "- Inference speed: 3-5x faster\n",
    "- Performance retention: >90% of teacher CCC\n",
    "- Memory usage: <200MB during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9994914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileViTBlock(nn.Module):\n",
    "    \"\"\"Efficient ViT block for mobile deployment\"\"\"\n",
    "    def __init__(self, dim, num_heads=4, mlp_ratio=2.0, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=drop, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        \n",
    "        # Smaller MLP expansion ratio for mobile\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pre-norm architecture for better training\n",
    "        attn_out, attn_weights = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "class MobileViTStudent(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight Vision Transformer optimized for mobile deployment\n",
    "    \n",
    "    Architecture choices for Android phones:\n",
    "    - Smaller patch size (16x16) for better feature extraction\n",
    "    - Fewer transformer layers (4 instead of 12)\n",
    "    - Smaller hidden dimension (192 instead of 768)\n",
    "    - Efficient attention mechanism\n",
    "    - ~5-8M parameters (vs 86M in full ViT)\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 image_size=224,\n",
    "                 patch_size=16,\n",
    "                 num_classes=2,  # valence, arousal\n",
    "                 hidden_dim=192,  # Reduced from 768\n",
    "                 num_layers=4,    # Reduced from 12\n",
    "                 num_heads=4,     # Reduced from 12\n",
    "                 mlp_ratio=2.0,   # Reduced from 4.0\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        # Patch embedding with depthwise separable convolution (mobile-friendly)\n",
    "        self.patch_embed = nn.Sequential(\n",
    "            # Depthwise conv\n",
    "            nn.Conv2d(3, 3, kernel_size=patch_size, stride=patch_size, groups=3, bias=False),\n",
    "            nn.BatchNorm2d(3),\n",
    "            # Pointwise conv\n",
    "            nn.Conv2d(3, hidden_dim, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Learnable position embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, hidden_dim))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer encoder blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MobileViTBlock(hidden_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Emotion prediction head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes),\n",
    "            nn.Tanh()  # Output in [-1, 1] range\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights for better convergence\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x, return_attention=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (B, 3, 224, 224)\n",
    "            return_attention: If True, return attention weights\n",
    "        Returns:\n",
    "            emotions: Predicted valence and arousal (B, 2)\n",
    "            attentions: List of attention weights (if return_attention=True)\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # (B, hidden_dim, H/patch, W/patch)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, hidden_dim)\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (B, num_patches+1, hidden_dim)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        attentions = []\n",
    "        for block in self.blocks:\n",
    "            x, attn = block(x)\n",
    "            if return_attention:\n",
    "                attentions.append(attn)\n",
    "        \n",
    "        # Final norm\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Use CLS token for prediction\n",
    "        cls_output = x[:, 0]\n",
    "        emotions = self.head(cls_output)\n",
    "        \n",
    "        if return_attention:\n",
    "            return emotions, attentions\n",
    "        return emotions\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        \"\"\"Return number of parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Initialize student model\n",
    "print(\"=\" * 80)\n",
    "print(\"üéì INITIALIZING MOBILE-OPTIMIZED STUDENT MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "mobile_student = MobileViTStudent(\n",
    "    image_size=VIT_IMAGE_SIZE,\n",
    "    hidden_dim=192,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    mlp_ratio=2.0,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "student_params = mobile_student.get_num_params()\n",
    "teacher_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\nüìä Model Comparison:\")\n",
    "print(f\"  Teacher (Full ViT):\")\n",
    "print(f\"    - Parameters: {teacher_params:,}\")\n",
    "print(f\"    - Memory: ~{teacher_params * 4 / 1024**2:.1f} MB (fp32)\")\n",
    "print(f\"    - Layers: 12\")\n",
    "print(f\"    - Hidden dim: 768\")\n",
    "print(f\"\\n  Student (MobileViT):\")\n",
    "print(f\"    - Parameters: {student_params:,}\")\n",
    "print(f\"    - Memory: ~{student_params * 4 / 1024**2:.1f} MB (fp32)\")\n",
    "print(f\"    - Layers: 4\")\n",
    "print(f\"    - Hidden dim: 192\")\n",
    "print(f\"\\n  üìâ Compression:\")\n",
    "print(f\"    - Size reduction: {teacher_params / student_params:.1f}x smaller\")\n",
    "print(f\"    - Memory savings: {(teacher_params - student_params) * 4 / 1024**2:.1f} MB\")\n",
    "print(f\"    - Target platforms: Android phones with 4GB+ RAM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(2, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE).to(DEVICE)\n",
    "    test_output, test_attentions = mobile_student(test_input, return_attention=True)\n",
    "    print(f\"\\n‚úÖ Forward pass successful!\")\n",
    "    print(f\"   Input shape: {test_input.shape}\")\n",
    "    print(f\"   Output shape: {test_output.shape}\")\n",
    "    print(f\"   Attention maps: {len(test_attentions)} layers\")\n",
    "    print(f\"   Output range: [{test_output.min().item():.3f}, {test_output.max().item():.3f}]\")\n",
    "\n",
    "print(f\"\\nüí° Model ready for distillation training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2339f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeDistillationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Comprehensive distillation loss combining:\n",
    "    1. Response-based distillation (output matching)\n",
    "    2. Feature-based distillation (intermediate layer matching)\n",
    "    3. Attention transfer (where the model \"looks\")\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 alpha=0.5,           # Weight for hard target loss\n",
    "                 beta=0.3,            # Weight for feature distillation\n",
    "                 gamma=0.2,           # Weight for attention transfer\n",
    "                 temperature=4.0,     # Softmax temperature for soft targets\n",
    "                 response_loss='mse'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Loss functions\n",
    "        self.hard_loss = nn.MSELoss()\n",
    "        self.feature_loss = nn.MSELoss()\n",
    "        self.attention_loss = nn.MSELoss()\n",
    "        \n",
    "        print(f\"üéØ Distillation Loss Configuration:\")\n",
    "        print(f\"   Œ± (hard targets):    {alpha:.2f}\")\n",
    "        print(f\"   Œ≤ (features):        {beta:.2f}\")\n",
    "        print(f\"   Œ≥ (attention):       {gamma:.2f}\")\n",
    "        print(f\"   Temperature:         {temperature}\")\n",
    "    \n",
    "    def forward(self, \n",
    "                student_outputs, \n",
    "                teacher_outputs, \n",
    "                true_labels,\n",
    "                student_features=None,\n",
    "                teacher_features=None,\n",
    "                student_attentions=None,\n",
    "                teacher_attentions=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            student_outputs: Student predictions (B, 2)\n",
    "            teacher_outputs: Teacher predictions (B, 2)\n",
    "            true_labels: Ground truth labels (B, 2)\n",
    "            student_features: Student intermediate features (optional)\n",
    "            teacher_features: Teacher intermediate features (optional)\n",
    "            student_attentions: Student attention maps (optional)\n",
    "            teacher_attentions: Teacher attention maps (optional)\n",
    "        \n",
    "        Returns:\n",
    "            total_loss: Combined distillation loss\n",
    "            loss_dict: Dictionary with individual loss components\n",
    "        \"\"\"\n",
    "        # 1. Hard target loss (student vs ground truth)\n",
    "        loss_hard = self.hard_loss(student_outputs, true_labels)\n",
    "        \n",
    "        # 2. Soft target loss (student vs teacher with temperature)\n",
    "        # Apply temperature to make distributions softer\n",
    "        soft_student = student_outputs / self.temperature\n",
    "        soft_teacher = teacher_outputs / self.temperature\n",
    "        loss_soft = self.hard_loss(soft_student, soft_teacher.detach()) * (self.temperature ** 2)\n",
    "        \n",
    "        # Combined response loss\n",
    "        loss_response = self.alpha * loss_hard + (1 - self.alpha) * loss_soft\n",
    "        \n",
    "        # 3. Feature-based distillation (if features provided)\n",
    "        loss_feature = torch.tensor(0.0).to(student_outputs.device)\n",
    "        if student_features is not None and teacher_features is not None:\n",
    "            # Match intermediate representations\n",
    "            # Features might have different dimensions, so we project them\n",
    "            for s_feat, t_feat in zip(student_features, teacher_features):\n",
    "                # Normalize features\n",
    "                s_feat_norm = F.normalize(s_feat, dim=-1)\n",
    "                t_feat_norm = F.normalize(t_feat.detach(), dim=-1)\n",
    "                loss_feature += self.feature_loss(s_feat_norm, t_feat_norm)\n",
    "            loss_feature /= len(student_features)\n",
    "        \n",
    "        # 4. Attention transfer (if attention maps provided)\n",
    "        loss_attention = torch.tensor(0.0).to(student_outputs.device)\n",
    "        if student_attentions is not None and teacher_attentions is not None:\n",
    "            # Match attention distributions\n",
    "            for s_attn, t_attn in zip(student_attentions, teacher_attentions):\n",
    "                # Normalize attention maps\n",
    "                s_attn_norm = F.softmax(s_attn.mean(dim=1), dim=-1)  # Average over heads\n",
    "                t_attn_norm = F.softmax(t_attn.mean(dim=1).detach(), dim=-1)\n",
    "                loss_attention += self.attention_loss(s_attn_norm, t_attn_norm)\n",
    "            loss_attention /= len(student_attentions)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss_response + self.beta * loss_feature + self.gamma * loss_attention\n",
    "        \n",
    "        # Return loss dictionary for monitoring\n",
    "        loss_dict = {\n",
    "            'total': total_loss.item(),\n",
    "            'hard': loss_hard.item(),\n",
    "            'soft': loss_soft.item(),\n",
    "            'response': loss_response.item(),\n",
    "            'feature': loss_feature.item(),\n",
    "            'attention': loss_attention.item()\n",
    "        }\n",
    "        \n",
    "        return total_loss, loss_dict\n",
    "\n",
    "\n",
    "def extract_teacher_features(teacher_model, inputs):\n",
    "    \"\"\"\n",
    "    Extract intermediate features from teacher model for distillation.\n",
    "    This requires accessing internal layers of the ViT model.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Hook function to capture intermediate outputs\n",
    "    def hook_fn(module, input, output):\n",
    "        # For ViT, we want the output of each transformer block\n",
    "        features.append(output[0][:, 0, :])  # CLS token representation\n",
    "    \n",
    "    # Register hooks on transformer blocks\n",
    "    hooks = []\n",
    "    if hasattr(teacher_model, 'vit'):\n",
    "        # Access ViT encoder blocks\n",
    "        if hasattr(teacher_model.vit, 'encoder'):\n",
    "            for block in teacher_model.vit.encoder.layer:\n",
    "                hooks.append(block.register_forward_hook(hook_fn))\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model(inputs)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return outputs, features\n",
    "\n",
    "\n",
    "def extract_student_features(student_model, inputs):\n",
    "    \"\"\"Extract intermediate features from student model.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Hook function\n",
    "    def hook_fn(module, input, output):\n",
    "        features.append(output[:, 0, :])  # CLS token\n",
    "    \n",
    "    # Register hooks on student blocks\n",
    "    hooks = []\n",
    "    for block in student_model.blocks:\n",
    "        hooks.append(block.register_forward_hook(hook_fn))\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = student_model(inputs, return_attention=False)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return outputs, features\n",
    "\n",
    "\n",
    "# Initialize distillation loss\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîß DISTILLATION LOSS INITIALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "distillation_criterion = KnowledgeDistillationLoss(\n",
    "    alpha=0.5,       # 50% ground truth, 50% teacher\n",
    "    beta=0.3,        # 30% weight for feature matching\n",
    "    gamma=0.2,       # 20% weight for attention transfer\n",
    "    temperature=4.0\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Distillation loss initialized!\")\n",
    "print(f\"   Loss components: Response + Feature + Attention\")\n",
    "print(f\"   Total weight: Œ± + Œ≤ + Œ≥ = 1.0\")\n",
    "\n",
    "# Test distillation loss\n",
    "print(f\"\\nüß™ Testing distillation loss...\")\n",
    "with torch.no_grad():\n",
    "    # Create dummy data\n",
    "    dummy_input = torch.randn(4, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE).to(DEVICE)\n",
    "    dummy_labels = torch.randn(4, 2).to(DEVICE)\n",
    "    \n",
    "    # Get teacher outputs\n",
    "    teacher_out, teacher_feats = extract_teacher_features(model, dummy_input)\n",
    "    \n",
    "    # Get student outputs  \n",
    "    student_out, student_attns = mobile_student(dummy_input, return_attention=True)\n",
    "    student_out_feat, student_feats = extract_student_features(mobile_student, dummy_input)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss, loss_dict = distillation_criterion(\n",
    "        student_out, teacher_out, dummy_labels,\n",
    "        student_features=student_feats[:len(teacher_feats)],  # Match teacher depth\n",
    "        teacher_features=teacher_feats,\n",
    "        student_attentions=student_attns,\n",
    "        teacher_attentions=None  # Teacher attention extraction is complex\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Distillation loss test successful!\")\n",
    "    print(f\"   Total loss: {loss_dict['total']:.4f}\")\n",
    "    print(f\"   - Hard target:  {loss_dict['hard']:.4f}\")\n",
    "    print(f\"   - Soft target:  {loss_dict['soft']:.4f}\")\n",
    "    print(f\"   - Feature:      {loss_dict['feature']:.4f}\")\n",
    "    print(f\"   - Attention:    {loss_dict['attention']:.4f}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distillation_epoch(student, teacher, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train student model for one epoch using knowledge distillation\"\"\"\n",
    "    student.train()\n",
    "    teacher.eval()  # Teacher is always in eval mode\n",
    "    \n",
    "    running_losses = {\n",
    "        'total': 0.0, 'hard': 0.0, 'soft': 0.0, \n",
    "        'response': 0.0, 'feature': 0.0, 'attention': 0.0\n",
    "    }\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Distillation Epoch {epoch+1}')\n",
    "    \n",
    "    for batch_idx, (spectrograms, labels) in enumerate(progress_bar):\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Get teacher predictions (no gradients)\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs, teacher_features = extract_teacher_features(teacher, spectrograms)\n",
    "        \n",
    "        # Get student predictions (with gradients)\n",
    "        student_outputs, student_attentions = student(spectrograms, return_attention=True)\n",
    "        _, student_features = extract_student_features(student, spectrograms)\n",
    "        \n",
    "        # Calculate distillation loss\n",
    "        optimizer.zero_grad()\n",
    "        loss, loss_dict = criterion(\n",
    "            student_outputs, \n",
    "            teacher_outputs, \n",
    "            labels,\n",
    "            student_features=student_features[:len(teacher_features)],\n",
    "            teacher_features=teacher_features,\n",
    "            student_attentions=student_attentions,\n",
    "            teacher_attentions=None\n",
    "        )\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running losses\n",
    "        for key in running_losses:\n",
    "            running_losses[key] += loss_dict[key]\n",
    "        \n",
    "        # Update progress bar\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            avg_loss = running_losses['total'] / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'hard': f'{loss_dict[\"hard\"]:.4f}',\n",
    "                'soft': f'{loss_dict[\"soft\"]:.4f}'\n",
    "            })\n",
    "    \n",
    "    # Calculate epoch averages\n",
    "    epoch_losses = {key: val / len(train_loader) for key, val in running_losses.items()}\n",
    "    return epoch_losses\n",
    "\n",
    "\n",
    "def evaluate_distillation(student, teacher, test_loader, device):\n",
    "    \"\"\"Evaluate student and teacher models side by side\"\"\"\n",
    "    student.eval()\n",
    "    teacher.eval()\n",
    "    \n",
    "    student_preds = []\n",
    "    teacher_preds = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for spectrograms, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            student_out = student(spectrograms)\n",
    "            teacher_out = teacher(spectrograms)\n",
    "            \n",
    "            student_preds.append(student_out.cpu().numpy())\n",
    "            teacher_preds.append(teacher_out.cpu().numpy())\n",
    "            true_labels.append(labels.numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    student_preds = np.concatenate(student_preds, axis=0)\n",
    "    teacher_preds = np.concatenate(teacher_preds, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    def calc_metrics(predictions, targets):\n",
    "        mae = np.mean(np.abs(predictions - targets), axis=0)\n",
    "        \n",
    "        # CCC for valence and arousal\n",
    "        def ccc(y_true, y_pred):\n",
    "            mean_true = np.mean(y_true)\n",
    "            mean_pred = np.mean(y_pred)\n",
    "            var_true = np.var(y_true)\n",
    "            var_pred = np.var(y_pred)\n",
    "            covariance = np.mean((y_true - mean_true) * (y_pred - mean_pred))\n",
    "            ccc_val = (2 * covariance) / (var_true + var_pred + (mean_true - mean_pred)**2)\n",
    "            return ccc_val\n",
    "        \n",
    "        ccc_valence = ccc(targets[:, 0], predictions[:, 0])\n",
    "        ccc_arousal = ccc(targets[:, 1], predictions[:, 1])\n",
    "        \n",
    "        return {\n",
    "            'mae_valence': mae[0],\n",
    "            'mae_arousal': mae[1],\n",
    "            'mae_avg': np.mean(mae),\n",
    "            'ccc_valence': ccc_valence,\n",
    "            'ccc_arousal': ccc_arousal,\n",
    "            'ccc_avg': (ccc_valence + ccc_arousal) / 2\n",
    "        }\n",
    "    \n",
    "    teacher_metrics = calc_metrics(teacher_preds, true_labels)\n",
    "    student_metrics = calc_metrics(student_preds, true_labels)\n",
    "    \n",
    "    return {\n",
    "        'student': student_metrics,\n",
    "        'teacher': teacher_metrics,\n",
    "        'student_preds': student_preds,\n",
    "        'teacher_preds': teacher_preds,\n",
    "        'true_labels': true_labels\n",
    "    }\n",
    "\n",
    "\n",
    "# Distillation training configuration\n",
    "print(\"=\" * 80)\n",
    "print(\"üéì KNOWLEDGE DISTILLATION TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "DISTILL_EPOCHS = 20\n",
    "DISTILL_LR = 2e-4\n",
    "DISTILL_WEIGHT_DECAY = 0.01\n",
    "DISTILL_PATIENCE = 5\n",
    "\n",
    "# Initialize optimizer\n",
    "distill_optimizer = optim.AdamW(\n",
    "    mobile_student.parameters(),\n",
    "    lr=DISTILL_LR,\n",
    "    weight_decay=DISTILL_WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "distill_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    distill_optimizer,\n",
    "    T_0=5,  # Restart every 5 epochs\n",
    "    T_mult=2,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Training Configuration:\")\n",
    "print(f\"   Epochs: {DISTILL_EPOCHS}\")\n",
    "print(f\"   Learning rate: {DISTILL_LR}\")\n",
    "print(f\"   Weight decay: {DISTILL_WEIGHT_DECAY}\")\n",
    "print(f\"   Patience: {DISTILL_PATIENCE}\")\n",
    "print(f\"   Scheduler: Cosine Annealing with Warm Restarts\")\n",
    "\n",
    "# Training history\n",
    "distill_history = {\n",
    "    'train_loss': [], 'train_hard': [], 'train_soft': [],\n",
    "    'train_feature': [], 'train_attention': [],\n",
    "    'val_student_ccc': [], 'val_teacher_ccc': [],\n",
    "    'val_student_mae': [], 'val_teacher_mae': []\n",
    "}\n",
    "\n",
    "best_student_ccc = 0.0\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"üöÄ Starting distillation training...\")\n",
    "print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "# Freeze teacher\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for epoch in range(DISTILL_EPOCHS):\n",
    "    # Train student\n",
    "    epoch_losses = train_distillation_epoch(\n",
    "        mobile_student, model, train_loader, \n",
    "        distillation_criterion, distill_optimizer, DEVICE, epoch\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    print(f\"\\nüìä Epoch {epoch + 1}/{DISTILL_EPOCHS} - Evaluating...\")\n",
    "    eval_results = evaluate_distillation(mobile_student, model, val_loader, DEVICE)\n",
    "    \n",
    "    # Update scheduler\n",
    "    distill_scheduler.step()\n",
    "    \n",
    "    # Store history\n",
    "    distill_history['train_loss'].append(epoch_losses['total'])\n",
    "    distill_history['train_hard'].append(epoch_losses['hard'])\n",
    "    distill_history['train_soft'].append(epoch_losses['soft'])\n",
    "    distill_history['train_feature'].append(epoch_losses['feature'])\n",
    "    distill_history['train_attention'].append(epoch_losses['attention'])\n",
    "    distill_history['val_student_ccc'].append(eval_results['student']['ccc_avg'])\n",
    "    distill_history['val_teacher_ccc'].append(eval_results['teacher']['ccc_avg'])\n",
    "    distill_history['val_student_mae'].append(eval_results['student']['mae_avg'])\n",
    "    distill_history['val_teacher_mae'].append(eval_results['teacher']['mae_avg'])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Epoch {epoch + 1}/{DISTILL_EPOCHS} Summary:\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"Training Loss: {epoch_losses['total']:.4f}\")\n",
    "    print(f\"  ‚îú‚îÄ Hard:      {epoch_losses['hard']:.4f}\")\n",
    "    print(f\"  ‚îú‚îÄ Soft:      {epoch_losses['soft']:.4f}\")\n",
    "    print(f\"  ‚îú‚îÄ Feature:   {epoch_losses['feature']:.4f}\")\n",
    "    print(f\"  ‚îî‚îÄ Attention: {epoch_losses['attention']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTeacher Performance:\")\n",
    "    print(f\"  ‚îú‚îÄ CCC Avg:     {eval_results['teacher']['ccc_avg']:.4f}\")\n",
    "    print(f\"  ‚îú‚îÄ CCC Valence: {eval_results['teacher']['ccc_valence']:.4f}\")\n",
    "    print(f\"  ‚îú‚îÄ CCC Arousal: {eval_results['teacher']['ccc_arousal']:.4f}\")\n",
    "    print(f\"  ‚îî‚îÄ MAE Avg:     {eval_results['teacher']['mae_avg']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nStudent Performance:\")\n",
    "    print(f\"  ‚îú‚îÄ CCC Avg:     {eval_results['student']['ccc_avg']:.4f}\")\n",
    "    print(f\"  ‚îú‚îÄ CCC Valence: {eval_results['student']['ccc_valence']:.4f}\")\n",
    "    print(f\"  ‚îú‚îÄ CCC Arousal: {eval_results['student']['ccc_arousal']:.4f}\")\n",
    "    print(f\"  ‚îî‚îÄ MAE Avg:     {eval_results['student']['mae_avg']:.4f}\")\n",
    "    \n",
    "    # Calculate retention percentage\n",
    "    ccc_retention = (eval_results['student']['ccc_avg'] / eval_results['teacher']['ccc_avg']) * 100\n",
    "    print(f\"\\nüìà Knowledge Retention: {ccc_retention:.1f}% of teacher performance\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    # Early stopping and model saving\n",
    "    current_ccc = eval_results['student']['ccc_avg']\n",
    "    if current_ccc > best_student_ccc:\n",
    "        best_student_ccc = current_ccc\n",
    "        best_model_state = mobile_student.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': mobile_student.state_dict(),\n",
    "            'optimizer_state_dict': distill_optimizer.state_dict(),\n",
    "            'best_ccc': best_student_ccc,\n",
    "            'eval_results': eval_results,\n",
    "            'history': distill_history\n",
    "        }, os.path.join(OUTPUT_DIR, 'mobile_vit_student_best.pth'))\n",
    "        \n",
    "        print(f\"‚úÖ Saved best model (CCC: {best_student_ccc:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"‚è≥ No improvement. Patience: {patience_counter}/{DISTILL_PATIENCE}\")\n",
    "        \n",
    "        if patience_counter >= DISTILL_PATIENCE:\n",
    "            print(f\"\\nüõë Early stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    mobile_student.load_state_dict(best_model_state)\n",
    "    print(f\"\\n‚úÖ Loaded best student model (CCC: {best_student_ccc:.4f})\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"üéâ Distillation Training Complete!\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Best Student CCC: {best_student_ccc:.4f}\")\n",
    "print(f\"Training epochs: {len(distill_history['train_loss'])}\")\n",
    "print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060591cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "print(\"=\" * 80)\n",
    "print(\"üß™ FINAL TEST SET EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_results = evaluate_distillation(mobile_student, model, test_loader, DEVICE)\n",
    "\n",
    "print(f\"\\nüìä Teacher Model (Full ViT - {teacher_params:,} params):\")\n",
    "print(f\"   Valence - CCC: {test_results['teacher']['ccc_valence']:.4f}, MAE: {test_results['teacher']['mae_valence']:.4f}\")\n",
    "print(f\"   Arousal - CCC: {test_results['teacher']['ccc_arousal']:.4f}, MAE: {test_results['teacher']['mae_arousal']:.4f}\")\n",
    "print(f\"   Average - CCC: {test_results['teacher']['ccc_avg']:.4f}, MAE: {test_results['teacher']['mae_avg']:.4f}\")\n",
    "\n",
    "print(f\"\\nüì± Student Model (MobileViT - {student_params:,} params):\")\n",
    "print(f\"   Valence - CCC: {test_results['student']['ccc_valence']:.4f}, MAE: {test_results['student']['mae_valence']:.4f}\")\n",
    "print(f\"   Arousal - CCC: {test_results['student']['ccc_arousal']:.4f}, MAE: {test_results['student']['mae_arousal']:.4f}\")\n",
    "print(f\"   Average - CCC: {test_results['student']['ccc_avg']:.4f}, MAE: {test_results['student']['mae_avg']:.4f}\")\n",
    "\n",
    "# Calculate performance retention\n",
    "ccc_retention = (test_results['student']['ccc_avg'] / test_results['teacher']['ccc_avg']) * 100\n",
    "mae_increase = ((test_results['student']['mae_avg'] - test_results['teacher']['mae_avg']) \n",
    "                / test_results['teacher']['mae_avg']) * 100\n",
    "\n",
    "print(f\"\\nüìà Compression Results:\")\n",
    "print(f\"   Size Reduction:    {teacher_params / student_params:.1f}x smaller\")\n",
    "print(f\"   Memory Savings:    {(teacher_params - student_params) * 4 / 1024**2:.1f} MB\")\n",
    "print(f\"   CCC Retention:     {ccc_retention:.1f}%\")\n",
    "print(f\"   MAE Increase:      {mae_increase:+.1f}%\")\n",
    "\n",
    "if ccc_retention >= 90:\n",
    "    print(f\"\\n‚úÖ Excellent! Student retains >90% of teacher performance\")\n",
    "elif ccc_retention >= 85:\n",
    "    print(f\"\\n‚úÖ Good! Student retains >85% of teacher performance\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Student performance could be improved with more training\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Visualize test results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Valence predictions\n",
    "axes[0, 0].scatter(test_results['true_labels'][:, 0], \n",
    "                   test_results['teacher_preds'][:, 0], \n",
    "                   alpha=0.5, s=20, label='Teacher', color='blue')\n",
    "axes[0, 0].scatter(test_results['true_labels'][:, 0], \n",
    "                   test_results['student_preds'][:, 0], \n",
    "                   alpha=0.5, s=20, label='Student', color='red')\n",
    "axes[0, 0].plot([-1, 1], [-1, 1], 'k--', lw=2, label='Perfect')\n",
    "axes[0, 0].set_xlabel('True Valence')\n",
    "axes[0, 0].set_ylabel('Predicted Valence')\n",
    "axes[0, 0].set_title(f'Valence Predictions\\nTeacher CCC: {test_results[\"teacher\"][\"ccc_valence\"]:.3f} | Student CCC: {test_results[\"student\"][\"ccc_valence\"]:.3f}')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Arousal predictions\n",
    "axes[0, 1].scatter(test_results['true_labels'][:, 1], \n",
    "                   test_results['teacher_preds'][:, 1], \n",
    "                   alpha=0.5, s=20, label='Teacher', color='blue')\n",
    "axes[0, 1].scatter(test_results['true_labels'][:, 1], \n",
    "                   test_results['student_preds'][:, 1], \n",
    "                   alpha=0.5, s=20, label='Student', color='red')\n",
    "axes[0, 1].plot([-1, 1], [-1, 1], 'k--', lw=2, label='Perfect')\n",
    "axes[0, 1].set_xlabel('True Arousal')\n",
    "axes[0, 1].set_ylabel('Predicted Arousal')\n",
    "axes[0, 1].set_title(f'Arousal Predictions\\nTeacher CCC: {test_results[\"teacher\"][\"ccc_arousal\"]:.3f} | Student CCC: {test_results[\"student\"][\"ccc_arousal\"]:.3f}')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training curves\n",
    "axes[1, 0].plot(distill_history['train_loss'], label='Total Loss', linewidth=2)\n",
    "axes[1, 0].plot(distill_history['train_hard'], label='Hard Target', linewidth=2, alpha=0.7)\n",
    "axes[1, 0].plot(distill_history['train_soft'], label='Soft Target', linewidth=2, alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_title('Distillation Training Losses')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# CCC comparison\n",
    "epochs = range(1, len(distill_history['val_student_ccc']) + 1)\n",
    "axes[1, 1].plot(epochs, distill_history['val_teacher_ccc'], \n",
    "                label='Teacher CCC', linewidth=2, color='blue', marker='o')\n",
    "axes[1, 1].plot(epochs, distill_history['val_student_ccc'], \n",
    "                label='Student CCC', linewidth=2, color='red', marker='s')\n",
    "axes[1, 1].fill_between(epochs, distill_history['val_student_ccc'], \n",
    "                         distill_history['val_teacher_ccc'], alpha=0.2)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Concordance Correlation Coefficient')\n",
    "axes[1, 1].set_title('Student vs Teacher CCC During Training')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'distillation_results.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Visualization saved to {OUTPUT_DIR}/distillation_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce74069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model for Android deployment\n",
    "print(\"=\" * 80)\n",
    "print(\"üì¶ EXPORTING MODEL FOR ANDROID DEPLOYMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Save PyTorch model\n",
    "mobile_model_path = os.path.join(OUTPUT_DIR, 'mobile_vit_emotion_model.pth')\n",
    "torch.save({\n",
    "    'model_state_dict': mobile_student.state_dict(),\n",
    "    'model_config': {\n",
    "        'image_size': VIT_IMAGE_SIZE,\n",
    "        'hidden_dim': 192,\n",
    "        'num_layers': 4,\n",
    "        'num_heads': 4,\n",
    "        'num_classes': 2\n",
    "    },\n",
    "    'test_metrics': test_results['student'],\n",
    "    'compression_ratio': teacher_params / student_params,\n",
    "    'imagenet_mean': IMAGENET_MEAN,\n",
    "    'imagenet_std': IMAGENET_STD\n",
    "}, mobile_model_path)\n",
    "\n",
    "print(f\"‚úÖ Saved PyTorch model: {mobile_model_path}\")\n",
    "print(f\"   Size: {os.path.getsize(mobile_model_path) / 1024**2:.2f} MB\")\n",
    "\n",
    "# 2. Export to TorchScript for mobile\n",
    "print(f\"\\nüîß Converting to TorchScript...\")\n",
    "mobile_student.eval()\n",
    "\n",
    "# Create example input\n",
    "example_input = torch.randn(1, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE).to(DEVICE)\n",
    "\n",
    "# Trace the model\n",
    "try:\n",
    "    traced_model = torch.jit.trace(mobile_student, example_input)\n",
    "    \n",
    "    # Optimize for mobile\n",
    "    traced_model_optimized = torch.jit.optimize_for_inference(traced_model)\n",
    "    \n",
    "    # Save TorchScript model\n",
    "    torchscript_path = os.path.join(OUTPUT_DIR, 'mobile_vit_emotion_model.pt')\n",
    "    traced_model_optimized.save(torchscript_path)\n",
    "    \n",
    "    print(f\"‚úÖ Saved TorchScript model: {torchscript_path}\")\n",
    "    print(f\"   Size: {os.path.getsize(torchscript_path) / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Test TorchScript model\n",
    "    with torch.no_grad():\n",
    "        original_output = mobile_student(example_input)\n",
    "        scripted_output = traced_model_optimized(example_input)\n",
    "        max_diff = torch.max(torch.abs(original_output - scripted_output)).item()\n",
    "    \n",
    "    print(f\"‚úÖ TorchScript verification: max diff = {max_diff:.6f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è TorchScript export failed: {e}\")\n",
    "    print(f\"   Continuing with PyTorch model only...\")\n",
    "\n",
    "# 3. Dynamic Quantization for even smaller size\n",
    "print(f\"\\n‚ö° Applying dynamic quantization...\")\n",
    "try:\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        mobile_student.cpu(),\n",
    "        {nn.Linear, nn.MultiheadAttention},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "    \n",
    "    # Save quantized model\n",
    "    quantized_path = os.path.join(OUTPUT_DIR, 'mobile_vit_emotion_model_quantized.pth')\n",
    "    torch.save({\n",
    "        'model': quantized_model,\n",
    "        'model_config': {\n",
    "            'image_size': VIT_IMAGE_SIZE,\n",
    "            'hidden_dim': 192,\n",
    "            'num_layers': 4,\n",
    "            'num_heads': 4,\n",
    "            'num_classes': 2\n",
    "        },\n",
    "        'imagenet_mean': IMAGENET_MEAN,\n",
    "        'imagenet_std': IMAGENET_STD\n",
    "    }, quantized_path)\n",
    "    \n",
    "    print(f\"‚úÖ Saved quantized model: {quantized_path}\")\n",
    "    print(f\"   Size: {os.path.getsize(quantized_path) / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Test quantized model\n",
    "    quantized_model.eval()\n",
    "    with torch.no_grad():\n",
    "        quantized_output = quantized_model(example_input.cpu())\n",
    "        max_diff_quantized = torch.max(torch.abs(original_output.cpu() - quantized_output)).item()\n",
    "    \n",
    "    print(f\"‚úÖ Quantization verification: max diff = {max_diff_quantized:.6f}\")\n",
    "    \n",
    "    # Move back to GPU if needed\n",
    "    mobile_student.to(DEVICE)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Quantization failed: {e}\")\n",
    "    print(f\"   Continuing without quantization...\")\n",
    "\n",
    "# 4. Create deployment package\n",
    "print(f\"\\nüì¶ Creating deployment package...\")\n",
    "\n",
    "deployment_info = {\n",
    "    'model_name': 'MobileViT Music Emotion Recognition',\n",
    "    'version': '1.0.0',\n",
    "    'description': 'Lightweight ViT for predicting valence and arousal from music spectrograms',\n",
    "    'input_format': {\n",
    "        'shape': [1, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE],\n",
    "        'type': 'float32',\n",
    "        'range': 'ImageNet normalized',\n",
    "        'mean': IMAGENET_MEAN,\n",
    "        'std': IMAGENET_STD\n",
    "    },\n",
    "    'output_format': {\n",
    "        'shape': [1, 2],\n",
    "        'type': 'float32',\n",
    "        'range': '[-1, 1]',\n",
    "        'labels': ['valence', 'arousal']\n",
    "    },\n",
    "    'model_specs': {\n",
    "        'parameters': student_params,\n",
    "        'size_mb': os.path.getsize(mobile_model_path) / 1024**2,\n",
    "        'layers': 4,\n",
    "        'hidden_dim': 192,\n",
    "        'compression_ratio': f'{teacher_params / student_params:.1f}x'\n",
    "    },\n",
    "    'performance': {\n",
    "        'test_ccc_valence': test_results['student']['ccc_valence'],\n",
    "        'test_ccc_arousal': test_results['student']['ccc_arousal'],\n",
    "        'test_ccc_avg': test_results['student']['ccc_avg'],\n",
    "        'test_mae_valence': test_results['student']['mae_valence'],\n",
    "        'test_mae_arousal': test_results['student']['mae_arousal'],\n",
    "        'retention_vs_teacher': f\"{ccc_retention:.1f}%\"\n",
    "    },\n",
    "    'android_requirements': {\n",
    "        'min_ram': '2GB',\n",
    "        'recommended_ram': '4GB',\n",
    "        'min_android_version': '8.0 (API 26)',\n",
    "        'pytorch_mobile_version': '1.13+',\n",
    "        'estimated_inference_time': '50-100ms on Snapdragon 870'\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "deployment_info_path = os.path.join(OUTPUT_DIR, 'deployment_info.json')\n",
    "with open(deployment_info_path, 'w') as f:\n",
    "    json.dump(deployment_info, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved deployment info: {deployment_info_path}\")\n",
    "\n",
    "# 5. Create inference example\n",
    "inference_example = '''\n",
    "# Example: Using the model on Android with PyTorch Mobile\n",
    "\n",
    "import org.pytorch.Module;\n",
    "import org.pytorch.Tensor;\n",
    "import org.pytorch.IValue;\n",
    "\n",
    "// Load model\n",
    "Module model = Module.load(assetFilePath(\"mobile_vit_emotion_model.pt\"));\n",
    "\n",
    "// Prepare input (melspectrogram as 224x224 RGB image)\n",
    "float[][][][] input = preprocessMelspectrogram(melspec);  // Your preprocessing\n",
    "\n",
    "// Create tensor\n",
    "Tensor inputTensor = Tensor.fromBlob(\n",
    "    input,\n",
    "    new long[]{1, 3, 224, 224}\n",
    ");\n",
    "\n",
    "// Run inference\n",
    "Tensor outputTensor = model.forward(IValue.from(inputTensor)).toTensor();\n",
    "float[] emotions = outputTensor.getDataAsFloatArray();\n",
    "\n",
    "// Get results\n",
    "float valence = emotions[0];  // Range: [-1, 1]\n",
    "float arousal = emotions[1];  // Range: [-1, 1]\n",
    "\n",
    "// Convert to 1-9 scale if needed\n",
    "float valence_scaled = (valence + 1) * 4 + 1;  // Maps [-1,1] to [1,9]\n",
    "float arousal_scaled = (arousal + 1) * 4 + 1;\n",
    "'''\n",
    "\n",
    "example_path = os.path.join(OUTPUT_DIR, 'android_inference_example.java')\n",
    "with open(example_path, 'w') as f:\n",
    "    f.write(inference_example)\n",
    "\n",
    "print(f\"‚úÖ Saved inference example: {example_path}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"üì± ANDROID DEPLOYMENT SUMMARY\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Model Files:\")\n",
    "print(f\"  ‚îú‚îÄ PyTorch:      mobile_vit_emotion_model.pth ({os.path.getsize(mobile_model_path) / 1024**2:.2f} MB)\")\n",
    "if os.path.exists(torchscript_path):\n",
    "    print(f\"  ‚îú‚îÄ TorchScript:  mobile_vit_emotion_model.pt ({os.path.getsize(torchscript_path) / 1024**2:.2f} MB)\")\n",
    "if os.path.exists(quantized_path):\n",
    "    print(f\"  ‚îú‚îÄ Quantized:    mobile_vit_emotion_model_quantized.pth ({os.path.getsize(quantized_path) / 1024**2:.2f} MB)\")\n",
    "print(f\"  ‚îú‚îÄ Deployment:   deployment_info.json\")\n",
    "print(f\"  ‚îî‚îÄ Example:      android_inference_example.java\")\n",
    "\n",
    "print(f\"\\nModel Specifications:\")\n",
    "print(f\"  ‚îú‚îÄ Parameters:   {student_params:,}\")\n",
    "print(f\"  ‚îú‚îÄ Compression:  {teacher_params / student_params:.1f}x smaller than teacher\")\n",
    "print(f\"  ‚îú‚îÄ Performance:  {ccc_retention:.1f}% of teacher CCC\")\n",
    "print(f\"  ‚îî‚îÄ Memory:       ~{student_params * 4 / 1024**2:.0f} MB (fp32)\")\n",
    "\n",
    "print(f\"\\nAndroid Requirements:\")\n",
    "print(f\"  ‚îú‚îÄ Min RAM:      2GB\")\n",
    "print(f\"  ‚îú‚îÄ Recommended:  4GB+\")\n",
    "print(f\"  ‚îú‚îÄ Android:      8.0+ (API 26+)\")\n",
    "print(f\"  ‚îî‚îÄ PyTorch:      1.13+ Mobile\")\n",
    "\n",
    "print(f\"\\nüéØ Recommended Usage:\")\n",
    "print(f\"  1. Use TorchScript model (.pt) for production\")\n",
    "print(f\"  2. Use quantized model for even lower memory devices\")\n",
    "print(f\"  3. Implement mel-spectrogram preprocessing on device\")\n",
    "print(f\"  4. Cache model to avoid repeated loading\")\n",
    "print(f\"  5. Run inference on background thread\")\n",
    "\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"‚úÖ Model export complete! Ready for Android deployment.\")\n",
    "print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ba5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Valence loss\n",
    "axes[0, 1].plot(history['train_valence_loss'], label='Train Valence Loss', marker='o')\n",
    "axes[0, 1].plot(history['val_valence_loss'], label='Val Valence Loss', marker='s')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Valence Loss')\n",
    "axes[0, 1].set_title('Valence Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Arousal loss\n",
    "axes[1, 0].plot(history['train_arousal_loss'], label='Train Arousal Loss', marker='o')\n",
    "axes[1, 0].plot(history['val_arousal_loss'], label='Val Arousal Loss', marker='s')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Arousal Loss')\n",
    "axes[1, 0].set_title('Arousal Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# CCC scores\n",
    "axes[1, 1].plot(history['val_valence_ccc'], label='Valence CCC', marker='o')\n",
    "axes[1, 1].plot(history['val_arousal_ccc'], label='Arousal CCC', marker='s')\n",
    "axes[1, 1].axhline(y=0.7, color='r', linestyle='--', alpha=0.5, label='Good threshold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('CCC Score')\n",
    "axes[1, 1].set_title('Concordance Correlation Coefficient')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vit_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Training curves saved to 'vit_training_curves.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(model, dataloader, criterion, device, epoch, phase='Val'):\n",
    "    \"\"\"Evaluate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_valence_loss = 0.0\n",
    "    running_arousal_loss = 0.0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1} [{phase}]')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for spectrograms, labels in progress_bar:\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(spectrograms)\n",
    "            loss, valence_loss, arousal_loss = criterion(predictions, labels)\n",
    "            \n",
    "            # Update metrics\n",
    "            running_loss += loss.item()\n",
    "            running_valence_loss += valence_loss.item()\n",
    "            running_arousal_loss += arousal_loss.item()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_valence_loss = running_valence_loss / len(dataloader)\n",
    "    epoch_arousal_loss = running_arousal_loss / len(dataloader)\n",
    "    \n",
    "    # Calculate CCC for valence and arousal\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    valence_ccc = concordance_correlation_coefficient(all_labels[:, 0], all_predictions[:, 0])\n",
    "    arousal_ccc = concordance_correlation_coefficient(all_labels[:, 1], all_predictions[:, 1])\n",
    "    \n",
    "    return epoch_loss, epoch_valence_loss, epoch_arousal_loss, valence_ccc, arousal_ccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab1d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_valence_loss = 0.0\n",
    "    running_arousal_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1} [Train]')\n",
    "    \n",
    "    for batch_idx, (spectrograms, labels) in enumerate(progress_bar):\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(spectrograms)\n",
    "        loss, valence_loss, arousal_loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        running_loss += loss.item()\n",
    "        running_valence_loss += valence_loss.item()\n",
    "        running_arousal_loss += arousal_loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            avg_loss = running_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'val_loss': f'{running_valence_loss/(batch_idx+1):.4f}',\n",
    "                'aro_loss': f'{running_arousal_loss/(batch_idx+1):.4f}'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_valence_loss = running_valence_loss / len(dataloader)\n",
    "    epoch_arousal_loss = running_arousal_loss / len(dataloader)\n",
    "    \n",
    "    return epoch_loss, epoch_valence_loss, epoch_arousal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00194e",
   "metadata": {},
   "source": [
    "## üéì ViT Training Loop\n",
    "\n",
    "Now we'll train the Vision Transformer on our combined dataset (real + GAN-augmented)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ffc9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archive outputs\n",
    "!zip -r /kaggle/working/vit_output.zip /kaggle/working/vit_augmented\n",
    "print(\"‚úÖ Outputs archived to vit_output.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9228f17f",
   "metadata": {},
   "source": [
    "## üß™ Comprehensive Model Testing\n",
    "\n",
    "Perform thorough testing of the trained ViT model including edge cases and robustness evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae684aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_robustness(model, test_loader, device=DEVICE):\n",
    "    \"\"\"Test model robustness with various edge cases and perturbations.\"\"\"\n",
    "    print(\"üß™ Testing model robustness...\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Test results storage\n",
    "    test_results = {\n",
    "        'normal_predictions': [],\n",
    "        'noisy_predictions': [],\n",
    "        'augmented_predictions': [],\n",
    "        'targets': [],\n",
    "        'confidence_scores': []\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_loader, desc='Robustness Testing')):\n",
    "            if i >= 10:  # Limit to first 10 batches for testing\n",
    "                break\n",
    "                \n",
    "            inputs = batch['pixel_values'].to(device)\n",
    "            targets = batch['emotions'].to(device)\n",
    "            \n",
    "            # 1. Normal prediction\n",
    "            normal_output = model(inputs)\n",
    "            \n",
    "            # 2. Add noise and test\n",
    "            noise = torch.randn_like(inputs) * 0.1\n",
    "            noisy_inputs = torch.clamp(inputs + noise, 0, 1)\n",
    "            noisy_output = model(noisy_inputs)\n",
    "            \n",
    "            # 3. Test with augmentation (random rotation)\n",
    "            augmented_inputs = torch.roll(inputs, shifts=10, dims=-1)\n",
    "            augmented_output = model(augmented_inputs)\n",
    "            \n",
    "            # Calculate confidence (inverse of prediction variance)\n",
    "            confidence = 1.0 / (torch.var(normal_output, dim=1) + 1e-6)\n",
    "            \n",
    "            # Store results\n",
    "            test_results['normal_predictions'].append(normal_output.cpu())\n",
    "            test_results['noisy_predictions'].append(noisy_output.cpu())\n",
    "            test_results['augmented_predictions'].append(augmented_output.cpu())\n",
    "            test_results['targets'].append(targets.cpu())\n",
    "            test_results['confidence_scores'].append(confidence.cpu())\n",
    "    \n",
    "    # Concatenate all results\n",
    "    for key in test_results:\n",
    "        if test_results[key]:\n",
    "            test_results[key] = torch.cat(test_results[key], dim=0).numpy()\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "def analyze_prediction_patterns(test_results):\n",
    "    \"\"\"Analyze prediction patterns and model behavior.\"\"\"\n",
    "    print(\"\\\\nüìä Analyzing prediction patterns...\")\n",
    "    \n",
    "    normal_pred = test_results['normal_predictions']\n",
    "    noisy_pred = test_results['noisy_predictions']\n",
    "    aug_pred = test_results['augmented_predictions']\n",
    "    targets = test_results['targets']\n",
    "    \n",
    "    # Calculate robustness metrics\n",
    "    noise_robustness = np.mean(np.abs(normal_pred - noisy_pred))\n",
    "    aug_robustness = np.mean(np.abs(normal_pred - aug_pred))\n",
    "    \n",
    "    print(f\"üîä Noise Robustness (MAE): {noise_robustness:.4f}\")\n",
    "    print(f\"üîÑ Augmentation Robustness (MAE): {aug_robustness:.4f}\")\n",
    "    \n",
    "    # Plot robustness analysis\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Prediction consistency\n",
    "    axes[0, 0].scatter(normal_pred[:, 0], noisy_pred[:, 0], alpha=0.6, color='blue')\n",
    "    axes[0, 0].plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "    axes[0, 0].set_xlabel('Normal Prediction (Valence)')\n",
    "    axes[0, 0].set_ylabel('Noisy Prediction (Valence)')\n",
    "    axes[0, 0].set_title('Noise Robustness - Valence')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].scatter(normal_pred[:, 1], noisy_pred[:, 1], alpha=0.6, color='red')\n",
    "    axes[0, 1].plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "    axes[0, 1].set_xlabel('Normal Prediction (Arousal)')\n",
    "    axes[0, 1].set_ylabel('Noisy Prediction (Arousal)')\n",
    "    axes[0, 1].set_title('Noise Robustness - Arousal')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Prediction distribution\n",
    "    axes[0, 2].hist(normal_pred.flatten(), bins=30, alpha=0.7, label='Normal', color='blue')\n",
    "    axes[0, 2].hist(noisy_pred.flatten(), bins=30, alpha=0.7, label='Noisy', color='orange')\n",
    "    axes[0, 2].set_xlabel('Prediction Value')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].set_title('Prediction Distribution')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error analysis\n",
    "    normal_error = np.abs(normal_pred - targets)\n",
    "    noisy_error = np.abs(noisy_pred - targets)\n",
    "    \n",
    "    axes[1, 0].hist(normal_error[:, 0], bins=20, alpha=0.7, label='Normal', color='blue')\n",
    "    axes[1, 0].hist(noisy_error[:, 0], bins=20, alpha=0.7, label='Noisy', color='orange')\n",
    "    axes[1, 0].set_xlabel('Absolute Error')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Valence Error Distribution')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].hist(normal_error[:, 1], bins=20, alpha=0.7, label='Normal', color='blue')\n",
    "    axes[1, 1].hist(noisy_error[:, 1], bins=20, alpha=0.7, label='Noisy', color='orange')\n",
    "    axes[1, 1].set_xlabel('Absolute Error')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Arousal Error Distribution')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confidence analysis\n",
    "    confidence = test_results['confidence_scores']\n",
    "    axes[1, 2].scatter(confidence, normal_error.mean(axis=1), alpha=0.6, color='green')\n",
    "    axes[1, 2].set_xlabel('Confidence Score')\n",
    "    axes[1, 2].set_ylabel('Prediction Error')\n",
    "    axes[1, 2].set_title('Confidence vs Error')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'noise_robustness': noise_robustness,\n",
    "        'augmentation_robustness': aug_robustness,\n",
    "        'mean_confidence': np.mean(confidence)\n",
    "    }\n",
    "\n",
    "def test_edge_cases(model, device=DEVICE):\n",
    "    \"\"\"Test model behavior on edge cases.\"\"\"\n",
    "    print(\"\\\\nüö® Testing edge cases...\")\n",
    "    \n",
    "    model.eval()\n",
    "    edge_cases = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Test with all zeros (silence)\n",
    "        zeros_input = torch.zeros(1, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE).to(device)\n",
    "        zeros_pred = model(zeros_input)\n",
    "        edge_cases['silence'] = zeros_pred.cpu().numpy()\n",
    "        \n",
    "        # Test with all ones (maximum intensity)\n",
    "        ones_input = torch.ones(1, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE).to(device)\n",
    "        ones_pred = model(ones_input)\n",
    "        edge_cases['maximum'] = ones_pred.cpu().numpy()\n",
    "        \n",
    "        # Test with random noise\n",
    "        noise_input = torch.randn(1, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE).to(device)\n",
    "        noise_input = torch.clamp(noise_input, 0, 1)\n",
    "        noise_pred = model(noise_input)\n",
    "        edge_cases['noise'] = noise_pred.cpu().numpy()\n",
    "        \n",
    "        # Test with checkerboard pattern\n",
    "        checker_input = torch.zeros(1, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE)\n",
    "        checker_input[:, :, ::2, ::2] = 1\n",
    "        checker_input[:, :, 1::2, 1::2] = 1\n",
    "        checker_input = checker_input.to(device)\n",
    "        checker_pred = model(checker_input)\n",
    "        edge_cases['checkerboard'] = checker_pred.cpu().numpy()\n",
    "    \n",
    "    print(\"Edge case predictions:\")\n",
    "    for case, pred in edge_cases.items():\n",
    "        valence, arousal = pred[0]\n",
    "        print(f\"  {case:12}: Valence={valence:.3f}, Arousal={arousal:.3f}\")\n",
    "    \n",
    "    return edge_cases\n",
    "\n",
    "def performance_benchmark(model, test_loader, device=DEVICE):\n",
    "    \"\"\"Benchmark model performance and timing.\"\"\"\n",
    "    print(\"\\\\n‚ö° Performance benchmarking...\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    dummy_input = torch.randn(1, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE).to(device)\n",
    "    for _ in range(5):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Timing test\n",
    "    import time\n",
    "    times = []\n",
    "    batch_sizes = [1, 4, 8, 16]\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        test_input = torch.randn(batch_size, 3, VIT_IMAGE_SIZE, VIT_IMAGE_SIZE).to(device)\n",
    "        \n",
    "        # Measure inference time\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):  # Average over 10 runs\n",
    "                _ = model(test_input)\n",
    "        \n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_time = (end_time - start_time) / 10\n",
    "        times.append(avg_time)\n",
    "        \n",
    "        print(f\"  Batch size {batch_size:2d}: {avg_time:.4f}s ({batch_size/avg_time:.1f} samples/s)\")\n",
    "    \n",
    "    # Memory usage\n",
    "    if device.type == 'cuda':\n",
    "        memory_usage = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "        print(f\"  Max GPU memory: {memory_usage:.1f} MB\")\n",
    "    \n",
    "    return {'batch_sizes': batch_sizes, 'inference_times': times}\n",
    "\n",
    "# Run comprehensive testing\n",
    "if 'best_vit_model' in locals() and 'test_loader' in locals():\n",
    "    print(\"üöÄ Starting comprehensive ViT model testing...\")\n",
    "    \n",
    "    # Load best model\n",
    "    try:\n",
    "        best_model_path = '/kaggle/working/vit_augmented/best_vit_model.pth'\n",
    "        if os.path.exists(best_model_path):\n",
    "            best_vit_model.load_state_dict(torch.load(best_model_path))\n",
    "            print(\"‚úÖ Best model loaded for testing\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Using current model state for testing\")\n",
    "    \n",
    "    # 1. Robustness testing\n",
    "    test_results = test_model_robustness(best_vit_model, test_loader)\n",
    "    robustness_metrics = analyze_prediction_patterns(test_results)\n",
    "    \n",
    "    # 2. Edge case testing\n",
    "    edge_results = test_edge_cases(best_vit_model)\n",
    "    \n",
    "    # 3. Performance benchmarking\n",
    "    perf_results = performance_benchmark(best_vit_model, test_loader)\n",
    "    \n",
    "    # Summary report\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"üìã COMPREHENSIVE TESTING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"‚úÖ Robustness Testing:\")\n",
    "    print(f\"   - Noise Robustness: {robustness_metrics['noise_robustness']:.4f}\")\n",
    "    print(f\"   - Augmentation Robustness: {robustness_metrics['augmentation_robustness']:.4f}\")\n",
    "    print(f\"   - Mean Confidence: {robustness_metrics['mean_confidence']:.4f}\")\n",
    "    print(f\"\\\\n‚úÖ Edge Cases: All {len(edge_results)} test cases completed\")\n",
    "    print(f\"\\\\n‚úÖ Performance: Benchmarked across {len(perf_results['batch_sizes'])} batch sizes\")\n",
    "    print(\"\\\\nüéâ All tests completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping comprehensive testing - model or test data not available\")\n",
    "    print(\"Please ensure the model is trained and test data is prepared.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
