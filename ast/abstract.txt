# Audio Spectrogram Transformer for Emotion Recognition with GAN-Based Data Augmentation

## Title
**Large Acoustic Models for Emotion-Conditioned Music Understanding in Resource-Constrained Settings**

---

## Abstract

This work proposes the development of a **large acoustic model** for **emotion-aware music understanding and recommendation**, designed to move beyond genre- or artist-based systems by learning direct mappings between **musical features** and **emotional responses**.

### üéØ Core Methodology

#### 1. **Dataset Curation and Expansion**
- Large-scale emotion-annotated corpora (**DEAM dataset**)
- Locally curated music samples from Uganda
- Annotations along the **valence‚Äìarousal (VA) scale**
- **Data augmentation** using **Generative Adversarial Networks (GANs)** to create synthetic variations in:
  - Tempo
  - Timbre
  - Harmonic complexity
  - Instrumentation

#### 2. **Feature Extraction Pipeline**
Captures multi-dimensional acoustic properties:

**Low-level Acoustic Descriptors:**
- Tempo
- Loudness
- Spectral centroid
- **MFCCs** (Mel-Frequency Cepstral Coefficients)

**Tonal Features:**
- Chroma vectors
- Mode detection
- Harmonic change detection

**Temporal Dynamics:**
- Beat strength
- Rhythmic variability
- Onset timing

#### 3. **Model Architecture: Audio Spectrogram Transformer (AST)**
- **Patch-based embedding** of mel-spectrograms
- **Self-attention mechanisms** to capture temporal and spectral patterns
- **Multitask learning** objectives:
  - **Regression**: Continuous VA trajectories
  - **Classification**: Categorical affect states

#### 4. **Psychological Mechanisms of Music-Emotion Linkage**
The model explicitly encodes:
- **Tempo and loudness shifts** ‚Üí Brain-stem reflexes, arousal spikes
- **Mode and harmonic changes** ‚Üí Emotional contagion, valence modulation
- **Unexpected events** ‚Üí Musical expectancy violations
- **Complexity/novelty measures** ‚Üí Aesthetic judgment

### üéµ Current Implementation Focus

This project focuses on the **emotion recognition** component using:
- **Audio Spectrogram Transformer (AST)**
- **GAN-based data augmentation** to expand training data
- **Valence-Arousal prediction** from audio spectrograms

### üìä Evaluation Metrics

**Objective Metrics:**
- **Concordance Correlation Coefficient (CCC)** for VA prediction
- **Mean Squared Error (MSE)** for regression accuracy
- **Mean Absolute Error (MAE)** for average prediction error

**Subjective Assessment:**
- Alignment with intended emotional states
- Cultural appropriateness validation

### üåç Target Application

A **scientifically grounded**, **resource-efficient** acoustic system that:
- ‚úÖ Learns how music induces emotion
- ‚úÖ Predicts emotional responses from audio
- ‚úÖ Enables emotion-aware music recommendation
- ‚úÖ Applicable in **resource-constrained contexts** (e.g., Uganda, mobile devices)

---

## Keywords

`Acoustic Emotion Modeling` ‚Ä¢ `Valence‚ÄìArousal Framework` ‚Ä¢ `Audio Spectrogram Transformer` ‚Ä¢ `Generative Adversarial Networks` ‚Ä¢ `Data Augmentation` ‚Ä¢ `Music Information Retrieval` ‚Ä¢ `Emotion Recognition` ‚Ä¢ `Deep Learning` ‚Ä¢ `Mel-Spectrograms`

---

## Project Components

### ‚úÖ Implemented
1. **AST Model Architecture** - Vision Transformer adapted for audio
2. **DEAM Dataset Integration** - ~1800 songs with VA annotations
3. **Training Pipeline** - PyTorch-based with comprehensive metrics
4. **Evaluation Framework** - MSE, MAE, CCC metrics

### üöß In Progress
1. **GAN-Based Data Augmentation** - Synthetic spectrogram generation
2. **Augmented Training Pipeline** - Training AST with expanded dataset

### üîÆ Future Work
1. Knowledge Distillation for edge deployment
2. Diffusion-based music generation
3. Emotion-aware recommendation system
4. Cultural generalization with Ugandan music

---

*Last Updated: October 2025*