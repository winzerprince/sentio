{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b5ac27d",
   "metadata": {},
   "source": [
    "# 🎵 AST Training with GAN-Based Data Augmentation\n",
    "\n",
    "## 📋 Overview\n",
    "This notebook implements **Audio Spectrogram Transformer (AST)** training for music emotion recognition with **GAN-based data augmentation** to expand the DEAM dataset.\n",
    "\n",
    "### Key Features:\n",
    "- **Conditional GAN**: Generates synthetic spectrograms conditioned on valence/arousal\n",
    "- **Data Expansion**: Increases dataset size from ~1800 to 5000+ samples\n",
    "- **AST Architecture**: Vision Transformer adapted for audio spectrograms\n",
    "- **Emotion Prediction**: Valence-Arousal (VA) continuous values\n",
    "\n",
    "### Pipeline:\n",
    "1. Load DEAM dataset and extract real spectrograms\n",
    "2. Train Conditional GAN to generate synthetic spectrograms\n",
    "3. Augment dataset with GAN-generated samples\n",
    "4. Train AST model on expanded dataset\n",
    "5. Evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7abd17",
   "metadata": {},
   "source": [
    "# 🎓 Complete Educational Guide: GANs + AST\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 **What You'll Learn in This Notebook**\n",
    "\n",
    "This notebook builds upon basic AST training by adding **Generative Adversarial Networks (GANs)** for data augmentation. You'll understand:\n",
    "\n",
    "### **Core Concepts**:\n",
    "1. ��� **Why Data Augmentation Matters** - The data scarcity problem\n",
    "2. 🎨 **What are GANs?** - Two neural networks competing to create realistic data\n",
    "3. 🎯 **Conditional GANs** - Controlling what the generator creates\n",
    "4. 🔄 **Adversarial Training** - The min-max game between generator and discriminator\n",
    "5. 🎵 **Spectrogram Generation** - Creating realistic audio representations\n",
    "6. 📈 **Training on Augmented Data** - Using synthetic + real data together\n",
    "\n",
    "### **The Journey**:\n",
    "```\n",
    "Real Audio (1,800 songs)\n",
    "    ↓\n",
    "Extract Spectrograms\n",
    "    ↓\n",
    "Train Conditional GAN (learns spectrogram patterns)\n",
    "    ↓\n",
    "Generate Synthetic Spectrograms (3,200 new samples!)\n",
    "    ↓\n",
    "Combine Real + Synthetic (5,000 total samples)\n",
    "    ↓\n",
    "Train AST on Augmented Dataset\n",
    "    ↓\n",
    "Better Emotion Predictions! 🎯\n",
    "```\n",
    "\n",
    "**Prerequisites**: If you haven't read the basic AST notebook, start there first! This builds on those concepts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5ed7e",
   "metadata": {},
   "source": [
    "## 🎨 **Part 1: Understanding GANs - Generative Adversarial Networks**\n",
    "\n",
    "### **The Data Scarcity Problem** ⚠️\n",
    "\n",
    "**The Challenge**:\n",
    "```\n",
    "Emotion-labeled music dataset: ~1,800 songs\n",
    "Deep learning model: Needs 10,000+ samples to generalize well\n",
    "\n",
    "Problem: Not enough data!\n",
    "```\n",
    "\n",
    "**Traditional Solutions** (and their limitations):\n",
    "1. **Collect more data**: Expensive, time-consuming (weeks/months)\n",
    "2. **Manual augmentation**: Pitch shift, time stretch (limited variety)\n",
    "3. **Transfer learning**: Pre-trained models (may not fit our task)\n",
    "\n",
    "**Our Solution**: **Generate synthetic data that looks real!** 🎯\n",
    "\n",
    "---\n",
    "\n",
    "### **What are GANs?** 🤖 vs 🔍\n",
    "\n",
    "**Invented by**: Ian Goodfellow et al., 2014 - One of the most impactful AI breakthroughs!\n",
    "\n",
    "**The Core Idea**: Two neural networks play a game against each other:\n",
    "\n",
    "```\n",
    "┌─────────────┐                    ┌──────────────┐\n",
    "│  GENERATOR  │ ──[Fake Data]───→  │DISCRIMINATOR │\n",
    "│             │                    │              │\n",
    "│\"I create    │                    │\"I detect     │\n",
    "│fake data\"   │                    │fakes\"        │\n",
    "└─────────────┘                    └──────────────┘\n",
    "       ↑                                   ↓\n",
    "       │                                   │\n",
    "       └────[Feedback: \"Too obvious!\"]────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **The Counterfeiting Analogy** 💵\n",
    "\n",
    "**Perfect Analogy**: Art forgery\n",
    "\n",
    "**Generator** = **Forger**:\n",
    "- Tries to create fake paintings that look real\n",
    "- Gets feedback when caught\n",
    "- Improves forgery skills over time\n",
    "\n",
    "**Discriminator** = **Art Expert**:\n",
    "- Examines paintings to detect fakes\n",
    "- Learns what real art looks like\n",
    "- Becomes better at spotting fakes\n",
    "\n",
    "**The Game**:\n",
    "```\n",
    "Round 1:\n",
    "  Forger: Creates obvious fake → Caught immediately\n",
    "  Expert: Easily spots fake → Learns patterns\n",
    "\n",
    "Round 100:\n",
    "  Forger: Creates good fake → Sometimes fools expert\n",
    "  Expert: Sharper skills → Catches most fakes\n",
    "\n",
    "Round 10,000:\n",
    "  Forger: Creates masterful fake → Often indistinguishable!\n",
    "  Expert: Expert-level detection → Very hard to fool\n",
    "\n",
    "End Result: Forger creates near-perfect fakes!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **GAN Mathematics: The Min-Max Game** ⚔️\n",
    "\n",
    "**The Objective Function**:\n",
    "\n",
    "```\n",
    "min_G max_D V(D, G) = E_x[log D(x)] + E_z[log(1 - D(G(z)))]\n",
    "```\n",
    "\n",
    "**Breaking it down**:\n",
    "\n",
    "**Part 1**: `E_x[log D(x)]`\n",
    "- **E_x**: Expected value over real data\n",
    "- **D(x)**: Discriminator's output on real data\n",
    "- **log D(x)**: Log probability that real data is classified as real\n",
    "- **Goal**: Discriminator wants to maximize this (correctly identify real data)\n",
    "\n",
    "**Part 2**: `E_z[log(1 - D(G(z)))]`\n",
    "- **E_z**: Expected value over random noise\n",
    "- **G(z)**: Generator creates fake data from noise z\n",
    "- **D(G(z))**: Discriminator's output on fake data\n",
    "- **1 - D(G(z))**: Probability that fake is classified as fake\n",
    "- **log(1 - D(G(z)))**: Log of that probability\n",
    "- **Goal**: Discriminator wants to maximize this (correctly identify fakes)\n",
    "\n",
    "**The Two Players**:\n",
    "\n",
    "**Discriminator (Maximizes)**:\n",
    "```\n",
    "max_D [log D(x) + log(1 - D(G(z)))]\n",
    "\n",
    "Wants:\n",
    "  D(x) → 1      (real data labeled as real)\n",
    "  D(G(z)) → 0   (fake data labeled as fake)\n",
    "```\n",
    "\n",
    "**Generator (Minimizes)**:\n",
    "```\n",
    "min_G [log(1 - D(G(z)))]\n",
    "\n",
    "Wants:\n",
    "  D(G(z)) → 1   (fake data labeled as real!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Algorithm: Alternating Updates** 🔄\n",
    "\n",
    "**The Training Loop**:\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        # ====================\n",
    "        # Train Discriminator\n",
    "        # ====================\n",
    "        # Step 1: Real data\n",
    "        real_data = batch\n",
    "        real_output = D(real_data)\n",
    "        d_loss_real = -log(real_output)  # Want output = 1\n",
    "        \n",
    "        # Step 2: Fake data\n",
    "        noise = random_noise()\n",
    "        fake_data = G(noise)\n",
    "        fake_output = D(fake_data.detach())  # Don't update G yet!\n",
    "        d_loss_fake = -log(1 - fake_output)  # Want output = 0\n",
    "        \n",
    "        # Step 3: Total discriminator loss\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        update_discriminator(d_loss)\n",
    "        \n",
    "        # ====================\n",
    "        # Train Generator\n",
    "        # ====================\n",
    "        noise = random_noise()\n",
    "        fake_data = G(noise)\n",
    "        fake_output = D(fake_data)  # Now G is part of computation\n",
    "        g_loss = -log(fake_output)  # Want output = 1 (fool D!)\n",
    "        update_generator(g_loss)\n",
    "```\n",
    "\n",
    "**Key Insight**: We train D and G **separately** but they influence each other!\n",
    "\n",
    "---\n",
    "\n",
    "### **GAN Training Challenges** ⚡\n",
    "\n",
    "**1. Mode Collapse** 😱\n",
    "```\n",
    "Problem: Generator produces same output repeatedly\n",
    "\n",
    "Example:\n",
    "  Real spectrograms: Many varieties (happy, sad, energetic, calm)\n",
    "  Generator: Only produces \"average happy\" spectrograms\n",
    "  \n",
    "Why: Generator finds one trick that fools D and exploits it\n",
    "\n",
    "Solution: Various techniques (mini-batch discrimination, unrolled GANs)\n",
    "```\n",
    "\n",
    "**2. Non-Convergence** 🌀\n",
    "```\n",
    "Problem: D and G oscillate forever, never stabilize\n",
    "\n",
    "D gets better → G struggles → D weakens → G improves → D gets better → ...\n",
    "\n",
    "Solution: Careful learning rate tuning, Wasserstein GANs\n",
    "```\n",
    "\n",
    "**3. Vanishing Gradients** 📉\n",
    "```\n",
    "Problem: If D becomes too good, gradients to G vanish\n",
    "\n",
    "When D(G(z)) ≈ 0 everywhere:\n",
    "  ∂log(1-D(G(z)))/∂G ≈ 0  (no learning signal!)\n",
    "\n",
    "Solution: Alternative loss functions, label smoothing\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Conditional GANs (cGAN)** 🎯\n",
    "\n",
    "**The Innovation**: Control what the generator creates!\n",
    "\n",
    "**Standard GAN**:\n",
    "```\n",
    "Random Noise → Generator → ???Random Output???\n",
    "  (No control over what's generated)\n",
    "```\n",
    "\n",
    "**Conditional GAN**:\n",
    "```\n",
    "Random Noise + Condition → Generator → Controlled Output!\n",
    "       ↓                        ↓            ↓\n",
    "   Creativity              Guided by      Specific type\n",
    "                          Condition       of output\n",
    "```\n",
    "\n",
    "**For Our Task**:\n",
    "```\n",
    "Condition = [Valence, Arousal] = [7.5, 6.2]\n",
    "Meaning: Generate spectrogram for \"happy, energetic\" music\n",
    "\n",
    "Condition = [Valence, Arousal] = [2.5, 3.1]\n",
    "Meaning: Generate spectrogram for \"sad, calm\" music\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Conditional GAN Mathematics** 🔢\n",
    "\n",
    "**Modified Objective**:\n",
    "```\n",
    "min_G max_D V(D, G) = E_x[log D(x|c)] + E_z[log(1 - D(G(z|c)|c))]\n",
    "```\n",
    "\n",
    "**The Addition**: Condition **c** (valence, arousal)\n",
    "\n",
    "**Generator**:\n",
    "```\n",
    "Input: Noise z + Condition c\n",
    "Output: Fake spectrogram that matches condition c\n",
    "\n",
    "G(z, c) → fake_spectrogram\n",
    "```\n",
    "\n",
    "**Discriminator**:\n",
    "```\n",
    "Input: Spectrogram + Condition c\n",
    "Output: Probability of being real AND matching condition\n",
    "\n",
    "D(x, c) → probability\n",
    "```\n",
    "\n",
    "**Why This Works**:\n",
    "- Generator learns: \"For high valence, create bright spectrograms with major chords\"\n",
    "- Generator learns: \"For low valence, create darker spectrograms with minor chords\"\n",
    "- Discriminator checks: \"Is this real AND does it match the emotion label?\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Architectural Details for Spectrogram Generation** 🏗️\n",
    "\n",
    "**Challenge**: Generate 2D images (spectrograms) with specific properties\n",
    "\n",
    "**Generator Architecture**:\n",
    "```\n",
    "Latent Vector (100-dim) + Condition (2-dim)\n",
    "           ↓\n",
    "    [Dense Layer] → (256 × 16 × 20)\n",
    "           ↓\n",
    "    [Reshape] → 256 channels, 16×20 spatial\n",
    "           ↓\n",
    "    [ConvTranspose2D] → Upsample to 128 × 40\n",
    "           ↓\n",
    "    [ConvTranspose2D] → Upsample to 64 × 80\n",
    "           ↓\n",
    "    [ConvTranspose2D] → Upsample to 32 × 160\n",
    "           ↓\n",
    "    [ConvTranspose2D] → Final: 1 × 128 × 1280\n",
    "           ↓\n",
    "    [Tanh Activation] → Output in [-1, 1]\n",
    "```\n",
    "\n",
    "**Why Transposed Convolutions?**\n",
    "\n",
    "**Regular Convolution**: Downsamples (e.g., 64×64 → 32×32)\n",
    "```\n",
    "Input      Kernel     Output\n",
    "[4x4]   ×  [3x3]   =  [2x2]\n",
    "```\n",
    "\n",
    "**Transposed Convolution**: Upsamples (e.g., 32×32 → 64×64)\n",
    "```\n",
    "Input      Kernel     Output\n",
    "[2x2]   ×  [3x3]   =  [4x4]\n",
    "```\n",
    "\n",
    "**Analogy**: \n",
    "- Regular conv: Taking a photo (reduces resolution)\n",
    "- Transposed conv: Upscaling a photo (increases resolution)\n",
    "\n",
    "**Mathematical Operation**:\n",
    "```\n",
    "For regular conv: y = Conv(x)\n",
    "Transposed conv: y = Conv^T(x)\n",
    "  (literally the transpose of the convolution matrix!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Batch Normalization in GANs** 📊\n",
    "\n",
    "**What It Does**:\n",
    "```python\n",
    "BatchNorm2d(num_features)\n",
    "\n",
    "# For each feature channel:\n",
    "mean = batch.mean(dim=[0, 2, 3])\n",
    "std = batch.std(dim=[0, 2, 3])\n",
    "normalized = (batch - mean) / (std + ε)\n",
    "output = γ × normalized + β  # γ, β are learned\n",
    "```\n",
    "\n",
    "**Why Critical for GANs**:\n",
    "1. **Stabilizes training**: Prevents internal covariate shift\n",
    "2. **Allows higher learning rates**: Gradients flow better\n",
    "3. **Reduces sensitivity**: Less dependent on initialization\n",
    "\n",
    "**Where to Use**:\n",
    "- ✅ **Generator**: After each conv layer (except output)\n",
    "- ✅ **Discriminator**: After each conv layer (except input)\n",
    "- ❌ **Output layers**: No batch norm (preserves original scale)\n",
    "\n",
    "---\n",
    "\n",
    "### **Activation Functions for GANs** 🌊\n",
    "\n",
    "**Generator**:\n",
    "```python\n",
    "# Hidden layers\n",
    "nn.ReLU()           # or nn.LeakyReLU(0.2)\n",
    "\n",
    "# Output layer\n",
    "nn.Tanh()           # Output in [-1, 1]\n",
    "```\n",
    "\n",
    "**Why Tanh?**\n",
    "- Spectrograms normalized to [-1, 1] range\n",
    "- Symmetric around zero\n",
    "- Smooth gradients\n",
    "\n",
    "**Discriminator**:\n",
    "```python\n",
    "# Hidden layers\n",
    "nn.LeakyReLU(0.2)   # Allows small negative values\n",
    "\n",
    "# Output layer\n",
    "nn.Sigmoid()        # Probability in [0, 1]\n",
    "```\n",
    "\n",
    "**Why LeakyReLU?**\n",
    "```\n",
    "ReLU(x) = max(0, x)              # Dead neurons for x < 0\n",
    "LeakyReLU(x) = max(0.2x, x)      # Small gradient for x < 0\n",
    "\n",
    "LeakyReLU prevents \"dying neurons\" problem in GANs!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Loss Functions for GANs** 📉\n",
    "\n",
    "**Binary Cross-Entropy (BCE)** - Standard choice:\n",
    "\n",
    "```python\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Discriminator loss\n",
    "d_loss_real = criterion(D(real), ones)   # Real labeled as 1\n",
    "d_loss_fake = criterion(D(fake), zeros)  # Fake labeled as 0\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "# Generator loss\n",
    "g_loss = criterion(D(G(z)), ones)        # Want fake labeled as 1!\n",
    "```\n",
    "\n",
    "**Mathematical Formula**:\n",
    "```\n",
    "BCE(y, ŷ) = -[y × log(ŷ) + (1-y) × log(1-ŷ)]\n",
    "\n",
    "For real data (y = 1):\n",
    "  BCE = -log(ŷ)      → Minimized when ŷ = 1\n",
    "\n",
    "For fake data (y = 0):\n",
    "  BCE = -log(1-ŷ)    → Minimized when ŷ = 0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **GAN Training Hyperparameters** ⚙️\n",
    "\n",
    "**Learning Rates**:\n",
    "```python\n",
    "GAN_LR = 0.0002      # Standard for DCGANs\n",
    "```\n",
    "- **Lower than typical**: GANs are sensitive\n",
    "- **Same for both**: Balanced training\n",
    "- **Can adjust**: D slightly slower if G struggles\n",
    "\n",
    "**Adam Betas**:\n",
    "```python\n",
    "GAN_BETA1 = 0.5      # Lower momentum\n",
    "GAN_BETA2 = 0.999    # Standard second moment\n",
    "```\n",
    "- **β₁ = 0.5**: Less momentum (more responsive to changes)\n",
    "- **Standard**: β₁ = 0.9 for non-GAN tasks\n",
    "\n",
    "**Batch Size**:\n",
    "```python\n",
    "GAN_BATCH_SIZE = 32\n",
    "```\n",
    "- **Larger than AST** (16): Needs stable gradient estimates\n",
    "- **Trade-off**: Too large → mode collapse risk\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Summary: GAN Fundamentals**\n",
    "\n",
    "**What GANs Do**:\n",
    "- Generate synthetic data that mimics real data\n",
    "- Learn data distribution through adversarial training\n",
    "- Can be conditioned to control generation\n",
    "\n",
    "**How They Work**:\n",
    "- Generator creates fakes, Discriminator detects them\n",
    "- They improve together through competition\n",
    "- Eventually, Generator creates realistic samples\n",
    "\n",
    "**Why They're Powerful**:\n",
    "- Learn complex distributions without explicit modeling\n",
    "- Generate infinite variations\n",
    "- Transfer learned patterns to new examples\n",
    "\n",
    "**In Our Task**:\n",
    "- Learn what spectrograms \"look like\" for different emotions\n",
    "- Generate thousands of new training examples\n",
    "- Improve AST training through data augmentation\n",
    "\n",
    "Now let's see GANs in action! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf2e6d",
   "metadata": {},
   "source": [
    "## 1️⃣ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d947306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2e4792",
   "metadata": {},
   "source": [
    "## 2️⃣ Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ac2c3",
   "metadata": {},
   "source": [
    "### **🔧 Understanding Every Configuration Parameter**\n",
    "\n",
    "Before we set the parameters, let's understand what each one means and why we chose these values:\n",
    "\n",
    "---\n",
    "\n",
    "#### **📁 Dataset Configuration**\n",
    "\n",
    "**AUDIO_DIR** & **ANNOTATIONS_DIR**:\n",
    "- Paths to our data on Kaggle\n",
    "- Contains ~1,800 MP3 files with emotion annotations\n",
    "\n",
    "---\n",
    "\n",
    "#### **🎵 Audio Processing Configuration**\n",
    "\n",
    "**SAMPLE_RATE = 22050 Hz**:\n",
    "- **What**: How many audio samples per second\n",
    "- **Why 22050**: Half of CD quality (44,100), sufficient for music\n",
    "- **Trade-off**: Lower = less detail but faster processing\n",
    "- **Nyquist theorem**: Can capture frequencies up to 11,025 Hz (covers human hearing)\n",
    "\n",
    "**DURATION = 30 seconds**:\n",
    "- **What**: Length of audio clips we process\n",
    "- **Why 30s**: Long enough to capture musical patterns, short enough to be manageable\n",
    "- **Result**: Each song split into 30-second segments\n",
    "\n",
    "**N_MELS = 128**:\n",
    "- **What**: Number of mel-frequency bins in spectrogram\n",
    "- **Why 128**: Balance between frequency resolution and computational cost\n",
    "- **Common values**: 40 (speech), 128 (music), 256 (high detail)\n",
    "- **Result**: Spectrogram height = 128\n",
    "\n",
    "**HOP_LENGTH = 512**:\n",
    "- **What**: How many samples we skip between STFT windows\n",
    "- **Why 512**: Standard value, gives ~43 Hz time resolution\n",
    "- **Calculation**: 22050 / 512 ≈ 43 frames per second\n",
    "- **Smaller = more detail, more computation**\n",
    "\n",
    "**N_FFT = 2048**:\n",
    "- **What**: FFT window size (frequency resolution)\n",
    "- **Why 2048**: Provides ~10.75 Hz frequency resolution (22050/2048)\n",
    "- **Larger = better frequency resolution, worse time resolution**\n",
    "- **Time covered**: 2048/22050 ≈ 93 milliseconds per window\n",
    "\n",
    "**FMIN = 20 Hz**, **FMAX = 8000 Hz**:\n",
    "- **What**: Frequency range to analyze\n",
    "- **Why 20-8000**: Covers most musical content\n",
    "  - 20 Hz: Lowest bass frequencies\n",
    "  - 8000 Hz: High enough for most instruments (cymbals, harmonics)\n",
    "- **Human hearing**: 20 Hz - 20,000 Hz (we focus on lower range)\n",
    "\n",
    "---\n",
    "\n",
    "#### **🎨 GAN Configuration**\n",
    "\n",
    "**LATENT_DIM = 100**:\n",
    "- **What**: Dimension of random noise vector input to generator\n",
    "- **Why 100**: Standard choice, provides enough \"creativity space\"\n",
    "- **Analogy**: 100 different \"knobs\" the generator can adjust\n",
    "- **Smaller (e.g., 50)**: Less variation, faster training\n",
    "- **Larger (e.g., 200)**: More variation, risk of harder training\n",
    "\n",
    "**CONDITION_DIM = 2**:\n",
    "- **What**: Dimensionality of conditioning vector\n",
    "- **Why 2**: Valence + Arousal (our two emotion dimensions)\n",
    "- **Fixed**: We can't change this (it's our problem definition)\n",
    "\n",
    "**GAN_LR = 0.0002**:\n",
    "- **What**: Learning rate for GAN training\n",
    "- **Why 0.0002**: Standard for DCGAN architecture (Radford et al., 2015)\n",
    "- **Smaller than typical**: GANs are sensitive to large learning rates\n",
    "- **Too high**: Training instability, mode collapse\n",
    "- **Too low**: Very slow learning\n",
    "\n",
    "**GAN_BETA1 = 0.5, GAN_BETA2 = 0.999**:\n",
    "- **What**: Adam optimizer momentum parameters\n",
    "- **Why β₁=0.5**: Lower than default (0.9) for GANs\n",
    "  - Reduces momentum → more responsive to changes\n",
    "  - Helps with stability in adversarial training\n",
    "- **Why β₂=0.999**: Standard value for second moment estimation\n",
    "\n",
    "**GAN_EPOCHS = 10**:\n",
    "- **What**: How many times to train GAN on full real dataset\n",
    "- **Why 10**: Balance between quality and training time\n",
    "- **Too few (e.g., 3)**: Poor quality synthetic spectrograms\n",
    "- **Too many (e.g., 50)**: Overfitting to training set\n",
    "- **Typical range**: 10-30 for medium datasets\n",
    "\n",
    "**GAN_BATCH_SIZE = 32**:\n",
    "- **What**: How many spectrograms per GAN training batch\n",
    "- **Why 32**: Larger than AST batch size (16) for stable gradients\n",
    "- **Why important**: Batch norm statistics need reasonable batch size\n",
    "- **Trade-off**: Larger = more stable but slower\n",
    "\n",
    "**NUM_SYNTHETIC = 3200**:\n",
    "- **What**: How many synthetic spectrograms to generate\n",
    "- **Why 3200**: ~1.8× real dataset size (1800 real + 3200 synthetic = 5000 total)\n",
    "- **Goal**: Roughly triple the dataset size\n",
    "- **Calculation**: Aiming for ~5000 total samples for good AST training\n",
    "\n",
    "---\n",
    "\n",
    "#### **🤖 AST Model Configuration**\n",
    "\n",
    "**PATCH_SIZE = 16**:\n",
    "- **What**: Size of each patch (16×16 pixels)\n",
    "- **Why 16**: Standard for ViT, divisible into 128\n",
    "- **Result**: 128/16 = 8 patches vertically, ~80 patches horizontally\n",
    "- **Smaller (e.g., 8)**: More patches, more computation, finer detail\n",
    "- **Larger (e.g., 32)**: Fewer patches, faster, coarser detail\n",
    "\n",
    "**EMBED_DIM = 384**:\n",
    "- **What**: Size of embedding vectors in transformer\n",
    "- **Why 384**: Moderate size, 384 = 6 heads × 64 dim/head\n",
    "- **Must be divisible by NUM_HEADS**\n",
    "- **Standard values**: 256 (small), 384 (medium), 768 (large)\n",
    "\n",
    "**NUM_HEADS = 6**:\n",
    "- **What**: Number of parallel attention mechanisms\n",
    "- **Why 6**: Balance between capacity and computation\n",
    "- **More heads = more perspectives, but more computation**\n",
    "- **Common values**: 4, 6, 8, 12, 16\n",
    "\n",
    "**NUM_LAYERS = 6**:\n",
    "- **What**: Number of stacked transformer blocks\n",
    "- **Why 6**: Deep enough to learn complex patterns\n",
    "- **Deeper = more capacity but harder to train**\n",
    "- **Common values**: 4-12 for medium tasks\n",
    "\n",
    "**MLP_RATIO = 4**:\n",
    "- **What**: Hidden dimension = EMBED_DIM × MLP_RATIO\n",
    "- **Why 4**: Standard expansion factor\n",
    "- **Result**: 384 → 1536 → 384 (expand then compress)\n",
    "- **Purpose**: Increase capacity for non-linear transformations\n",
    "\n",
    "**DROPOUT = 0.1**:\n",
    "- **What**: Probability of randomly dropping neurons during training\n",
    "- **Why 0.1**: Light regularization (10% dropout)\n",
    "- **Purpose**: Prevents overfitting\n",
    "- **Range**: 0.1 (light) to 0.3 (heavy)\n",
    "\n",
    "---\n",
    "\n",
    "#### **🏋️ Training Configuration**\n",
    "\n",
    "**BATCH_SIZE = 16**:\n",
    "- **What**: AST training batch size\n",
    "- **Why 16**: Balance between GPU memory and gradient quality\n",
    "- **Smaller than GAN batch**: AST model larger, needs more memory per sample\n",
    "\n",
    "**NUM_EPOCHS = 5**:\n",
    "- **What**: How many times AST sees full augmented dataset\n",
    "- **Why 5**: Fast training as requested\n",
    "- **Note**: Would typically use 15-20 for best results\n",
    "- **Time constraint**: 5 epochs = reasonable training time\n",
    "\n",
    "**LEARNING_RATE = 1e-4**:\n",
    "- **What**: AST learning rate\n",
    "- **Why 1e-4**: Standard for transformers\n",
    "- **Same as**: 0.0001\n",
    "- **Typical range**: 1e-5 to 1e-3\n",
    "\n",
    "**WEIGHT_DECAY = 0.05**:\n",
    "- **What**: L2 regularization strength\n",
    "- **Why 0.05**: Standard for AdamW with transformers\n",
    "- **Purpose**: Prevents weights from growing too large\n",
    "- **Formula**: Loss += 0.05 × ||weights||²\n",
    "\n",
    "---\n",
    "\n",
    "### **📊 Summary of Our Choices**\n",
    "\n",
    "**Conservative choices** (proven to work):\n",
    "- Learning rates: 0.0002 (GAN), 0.0001 (AST)\n",
    "- Batch sizes: 32 (GAN), 16 (AST)\n",
    "- Architecture: Standard DCGAN + ViT\n",
    "\n",
    "**Aggressive choices** (for speed):\n",
    "- NUM_EPOCHS = 5 (could be 15-20)\n",
    "- GAN_EPOCHS = 10 (could be 20-30)\n",
    "\n",
    "**Dataset-specific**:\n",
    "- NUM_SYNTHETIC = 3200 (triple the data)\n",
    "- SAMPLE_RATE = 22050 (music-appropriate)\n",
    "- N_MELS = 128 (music detail)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c629d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# DATASET CONFIGURATION\n",
    "# ========================\n",
    "AUDIO_DIR = '/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/DEAM_audio/MEMD_audio/'\n",
    "ANNOTATIONS_DIR = '/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/DEAM_Annotations/annotations/annotations averaged per song/song_level/'\n",
    "\n",
    "# ========================\n",
    "# AUDIO PROCESSING CONFIG\n",
    "# ========================\n",
    "SAMPLE_RATE = 22050          # Audio sampling rate (Hz)\n",
    "DURATION = 30                # Audio clip duration (seconds)\n",
    "N_MELS = 128                 # Number of mel-frequency bins\n",
    "HOP_LENGTH = 512             # Hop length for STFT\n",
    "N_FFT = 2048                 # FFT window size\n",
    "FMIN = 20                    # Minimum frequency\n",
    "FMAX = 8000                  # Maximum frequency\n",
    "\n",
    "# ========================\n",
    "# GAN CONFIGURATION\n",
    "# ========================\n",
    "LATENT_DIM = 100             # Dimension of GAN noise vector\n",
    "CONDITION_DIM = 2            # Valence + Arousal\n",
    "GAN_LR = 0.0002              # GAN learning rate\n",
    "GAN_BETA1 = 0.5              # Adam beta1 for GAN\n",
    "GAN_BETA2 = 0.999            # Adam beta2 for GAN\n",
    "GAN_EPOCHS = 10              # GAN pre-training epochs\n",
    "GAN_BATCH_SIZE = 32          # GAN batch size\n",
    "NUM_SYNTHETIC = 3200         # Number of synthetic samples to generate\n",
    "\n",
    "# ========================\n",
    "# AST MODEL CONFIGURATION\n",
    "# ========================\n",
    "PATCH_SIZE = 16              # Size of each image patch (16x16)\n",
    "EMBED_DIM = 384              # Embedding dimension\n",
    "NUM_HEADS = 6                # Number of attention heads\n",
    "NUM_LAYERS = 6               # Number of transformer layers\n",
    "MLP_RATIO = 4                # MLP hidden dim = embed_dim * mlp_ratio\n",
    "DROPOUT = 0.1                # Dropout rate\n",
    "\n",
    "# ========================\n",
    "# TRAINING CONFIGURATION\n",
    "# ========================\n",
    "BATCH_SIZE = 16              # AST training batch size\n",
    "NUM_EPOCHS = 5               # AST training epochs (set to 5 as requested)\n",
    "LEARNING_RATE = 1e-4         # AST learning rate\n",
    "WEIGHT_DECAY = 0.05          # AdamW weight decay\n",
    "TRAIN_SPLIT = 0.8            # Train/validation split ratio\n",
    "\n",
    "# ========================\n",
    "# SYSTEM CONFIGURATION\n",
    "# ========================\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "OUTPUT_DIR = '/kaggle/working/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"📊 CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Audio Duration: {DURATION}s @ {SAMPLE_RATE}Hz\")\n",
    "print(f\"Mel-Spectrogram: {N_MELS} bins\")\n",
    "print(f\"\\n🎨 GAN Configuration:\")\n",
    "print(f\"  - Latent Dim: {LATENT_DIM}\")\n",
    "print(f\"  - GAN Epochs: {GAN_EPOCHS}\")\n",
    "print(f\"  - Synthetic Samples: {NUM_SYNTHETIC}\")\n",
    "print(f\"\\n🤖 AST Configuration:\")\n",
    "print(f\"  - Patch Size: {PATCH_SIZE}x{PATCH_SIZE}\")\n",
    "print(f\"  - Embed Dim: {EMBED_DIM}\")\n",
    "print(f\"  - Num Heads: {NUM_HEADS}\")\n",
    "print(f\"  - Num Layers: {NUM_LAYERS}\")\n",
    "print(f\"\\n🏋️ Training Configuration:\")\n",
    "print(f\"  - AST Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ad236d",
   "metadata": {},
   "source": [
    "### **Part 2: Data Loading - From Files to Tensors** 📂\n",
    "\n",
    "Now let's understand what happens when we load our data. This is where raw audio files become numerical arrays that our models can process.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step-by-Step Data Loading Process**\n",
    "\n",
    "**1. Reading Annotations (emotions.csv)**:\n",
    "\n",
    "```\n",
    "song_id,valence_mean,arousal_mean\n",
    "1,5.2,4.8\n",
    "2,3.1,6.2\n",
    "...\n",
    "```\n",
    "\n",
    "- **What we have**: CSV with song IDs and average emotion ratings\n",
    "- **Valence**: How positive (9) vs negative (1) the song feels\n",
    "- **Arousal**: How energetic (9) vs calm (1) the song feels\n",
    "- **Raters**: Each song rated by ~10 people, we use the average\n",
    "\n",
    "---\n",
    "\n",
    "**2. Loading Audio Files**:\n",
    "\n",
    "```python\n",
    "audio, sr = librosa.load('song_1.mp3', sr=22050)\n",
    "# audio: numpy array of shape (661500,) for 30s\n",
    "# sr: 22050 (samples per second)\n",
    "```\n",
    "\n",
    "**What happens**:\n",
    "- Librosa decodes MP3 → raw waveform\n",
    "- Resamples to 22050 Hz if needed\n",
    "- Result: 1D array of amplitude values\n",
    "\n",
    "**Example values**:\n",
    "```\n",
    "audio = [0.001, -0.003, 0.005, ..., -0.002]\n",
    "# Range: typically -1.0 to +1.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Converting to Spectrogram**:\n",
    "\n",
    "This is where magic happens! We transform time-domain signal → frequency-domain representation.\n",
    "\n",
    "**The librosa.feature.melspectrogram() function**:\n",
    "\n",
    "```python\n",
    "mel_spec = librosa.feature.melspectrogram(\n",
    "    y=audio,           # Input waveform\n",
    "    sr=22050,          # Sample rate\n",
    "    n_fft=2048,        # FFT window size\n",
    "    hop_length=512,    # Step size\n",
    "    n_mels=128,        # Number of mel bins\n",
    "    fmin=20,           # Minimum frequency\n",
    "    fmax=8000          # Maximum frequency\n",
    ")\n",
    "```\n",
    "\n",
    "**Internal steps**:\n",
    "\n",
    "1. **STFT (Short-Time Fourier Transform)**:\n",
    "   - Divide audio into overlapping windows\n",
    "   - Apply FFT to each window\n",
    "   - Result: Complex numbers representing frequency content\n",
    "\n",
    "2. **Power Spectrum**:\n",
    "   - Convert complex numbers to magnitudes\n",
    "   - Square the magnitudes: |X|²\n",
    "   - Result: Energy at each frequency\n",
    "\n",
    "3. **Mel Filterbank**:\n",
    "   - Apply 128 triangular filters\n",
    "   - Filters spaced on mel scale (perceptually meaningful)\n",
    "   - Result: 128 mel-frequency bins\n",
    "\n",
    "4. **Aggregation**:\n",
    "   - Sum energy within each mel band\n",
    "   - Result: mel spectrogram\n",
    "\n",
    "**Output shape**:\n",
    "```\n",
    "mel_spec.shape = (128, 2584)\n",
    "# 128 mel bins (frequency)\n",
    "# 2584 time frames\n",
    "```\n",
    "\n",
    "**Calculating number of frames**:\n",
    "- Audio length: 661,500 samples\n",
    "- Window size: 2048 samples\n",
    "- Hop length: 512 samples\n",
    "- Number of frames: (661500 - 2048) / 512 + 1 ≈ 2584 ✓\n",
    "\n",
    "---\n",
    "\n",
    "**4. Converting to Decibels**:\n",
    "\n",
    "```python\n",
    "mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "```\n",
    "\n",
    "**Why decibels?**\n",
    "- Human hearing is logarithmic (we perceive ratios, not absolute differences)\n",
    "- Raw power values have huge dynamic range (0.000001 to 10000)\n",
    "- Decibels compress this range: log₁₀(power) × 10\n",
    "\n",
    "**Formula**:\n",
    "$$\\text{dB} = 10 \\log_{10}\\left(\\frac{\\text{power}}{\\text{reference}}\\right)$$\n",
    "\n",
    "With reference = max power:\n",
    "$$\\text{dB} = 10 \\log_{10}\\left(\\frac{\\text{power}}{\\max(\\text{power})}\\right)$$\n",
    "\n",
    "**Example**:\n",
    "- Max power: 1000\n",
    "- Some value: 100\n",
    "- dB: 10 × log₁₀(100/1000) = 10 × (-1) = -10 dB\n",
    "\n",
    "**Range**: typically -80 dB (quiet) to 0 dB (loudest)\n",
    "\n",
    "---\n",
    "\n",
    "**5. Normalization to [0, 1]**:\n",
    "\n",
    "```python\n",
    "mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min())\n",
    "```\n",
    "\n",
    "**Why normalize?**\n",
    "- Neural networks work best with values in [0, 1] or [-1, 1]\n",
    "- Different songs have different loudness\n",
    "- Normalization removes loudness variations\n",
    "\n",
    "**Step-by-step**:\n",
    "1. **Subtract minimum**: Shift to start at 0\n",
    "   - Before: [-80, -60, -40, ..., 0]\n",
    "   - After: [0, 20, 40, ..., 80]\n",
    "\n",
    "2. **Divide by range**: Scale to [0, 1]\n",
    "   - Range = 80\n",
    "   - After: [0/80, 20/80, 40/80, ..., 80/80] = [0, 0.25, 0.5, ..., 1]\n",
    "\n",
    "**Result**: All spectrograms in consistent range\n",
    "\n",
    "---\n",
    "\n",
    "**6. Normalizing Emotion Labels**:\n",
    "\n",
    "```python\n",
    "valence_norm = (valence - 1) / 8  # From [1,9] to [0,1]\n",
    "arousal_norm = (arousal - 1) / 8  # From [1,9] to [0,1]\n",
    "```\n",
    "\n",
    "**Why [0, 1]?**\n",
    "- Consistent with spectrogram normalization\n",
    "- Easier for generator to produce\n",
    "- Prevents gradient issues\n",
    "\n",
    "**Example**:\n",
    "- Original: valence=5, arousal=7\n",
    "- Normalized: valence=0.5, arousal=0.75\n",
    "\n",
    "---\n",
    "\n",
    "#### **The Complete Pipeline**\n",
    "\n",
    "```\n",
    "MP3 File (3 MB)\n",
    "    ↓ librosa.load()\n",
    "Waveform Array (661,500 samples)\n",
    "    ↓ librosa.feature.melspectrogram()\n",
    "Mel Spectrogram (128 × 2584)\n",
    "    ↓ librosa.power_to_db()\n",
    "Decibel Spectrogram (128 × 2584)\n",
    "    ↓ min-max normalization\n",
    "Normalized Spectrogram (128 × 2584, range [0,1])\n",
    "    ↓ PyTorch tensor\n",
    "Ready for Model Input\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **What Our Dataset Class Does**\n",
    "\n",
    "```python\n",
    "class MusicDataset(Dataset):\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Get emotion labels for song idx\n",
    "        valence, arousal = self.labels[idx]\n",
    "        \n",
    "        # 2. Load audio file\n",
    "        audio, _ = librosa.load(self.audio_files[idx])\n",
    "        \n",
    "        # 3. Convert to spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(audio, ...)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec)\n",
    "        \n",
    "        # 4. Normalize\n",
    "        spec_norm = normalize(mel_spec_db)\n",
    "        \n",
    "        # 5. Convert to tensor\n",
    "        spec_tensor = torch.FloatTensor(spec_norm)\n",
    "        emotion_tensor = torch.FloatTensor([valence, arousal])\n",
    "        \n",
    "        return spec_tensor, emotion_tensor\n",
    "```\n",
    "\n",
    "**Key points**:\n",
    "- **Lazy loading**: Only load audio when needed\n",
    "- **On-the-fly processing**: Compute spectrogram during training\n",
    "- **Memory efficient**: Don't store all spectrograms in RAM\n",
    "\n",
    "---\n",
    "\n",
    "#### **Data Augmentation with SpecAugment**\n",
    "\n",
    "During GAN training, we apply SpecAugment to real spectrograms:\n",
    "\n",
    "**Frequency Masking**:\n",
    "- Randomly mask `f` consecutive frequency bins\n",
    "- Like covering part of a piano keyboard\n",
    "- Forces model to not rely on specific frequency bands\n",
    "\n",
    "**Time Masking**:\n",
    "- Randomly mask `t` consecutive time frames\n",
    "- Like removing a small chunk of the song\n",
    "- Forces model to handle incomplete information\n",
    "\n",
    "**Why augment real data for GAN?**\n",
    "- Makes discriminator more robust\n",
    "- Prevents memorization\n",
    "- Generator learns to handle variations\n",
    "\n",
    "---\n",
    "\n",
    "#### **Train/Validation/Test Split**\n",
    "\n",
    "```python\n",
    "train_size = 0.8  # 80% for training\n",
    "val_size = 0.1    # 10% for validation\n",
    "test_size = 0.1   # 10% for testing\n",
    "```\n",
    "\n",
    "**With ~1800 songs**:\n",
    "- Train: ~1440 songs (train GAN on these)\n",
    "- Validation: ~180 songs (tune hyperparameters)\n",
    "- Test: ~180 songs (final evaluation, never seen by models)\n",
    "\n",
    "**Critical**: Test set must remain unseen until final evaluation!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751b8ed",
   "metadata": {},
   "source": [
    "## 3️⃣ Load DEAM Dataset & Extract Real Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c7f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations (both static and dynamic contain song_id, valence, arousal)\n",
    "static_annotations_path = os.path.join(ANNOTATIONS_DIR, 'static_annotations.csv')\n",
    "dynamic_annotations_path = os.path.join(ANNOTATIONS_DIR, 'dynamic_annotations.csv')\n",
    "\n",
    "# Try to load both annotation files\n",
    "if os.path.exists(static_annotations_path):\n",
    "    df_annotations = pd.read_csv(static_annotations_path)\n",
    "    print(f\"✅ Loaded static annotations: {len(df_annotations)} songs\")\n",
    "elif os.path.exists(dynamic_annotations_path):\n",
    "    df_annotations = pd.read_csv(dynamic_annotations_path)\n",
    "    print(f\"✅ Loaded dynamic annotations: {len(df_annotations)} songs\")\n",
    "else:\n",
    "    # Fallback: Load any CSV in the directory\n",
    "    csv_files = glob.glob(os.path.join(ANNOTATIONS_DIR, '*.csv'))\n",
    "    if csv_files:\n",
    "        df_annotations = pd.read_csv(csv_files[0])\n",
    "        print(f\"✅ Loaded annotations from {os.path.basename(csv_files[0])}: {len(df_annotations)} songs\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No annotation files found in {ANNOTATIONS_DIR}\")\n",
    "\n",
    "# Clean column names (remove whitespace)\n",
    "df_annotations.columns = df_annotations.columns.str.strip()\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n📊 Annotation Sample:\")\n",
    "print(df_annotations.head())\n",
    "print(f\"\\nColumns: {list(df_annotations.columns)}\")\n",
    "\n",
    "# Check for audio files\n",
    "audio_files = glob.glob(os.path.join(AUDIO_DIR, '*.mp3'))\n",
    "print(f\"\\n🎵 Found {len(audio_files)} audio files\")\n",
    "\n",
    "# Extract spectrograms from real data for GAN training\n",
    "print(\"\\n🔊 Extracting spectrograms from real audio...\")\n",
    "\n",
    "def extract_melspectrogram(audio_path, sr=SAMPLE_RATE, duration=DURATION):\n",
    "    \"\"\"Extract mel-spectrogram from audio file\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        y, _ = librosa.load(audio_path, sr=sr, duration=duration)\n",
    "        \n",
    "        # Compute mel-spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=y, sr=sr, n_mels=N_MELS, n_fft=N_FFT, \n",
    "            hop_length=HOP_LENGTH, fmin=FMIN, fmax=FMAX\n",
    "        )\n",
    "        \n",
    "        # Convert to log scale (dB)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        mel_spec_norm = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-8)\n",
    "        \n",
    "        return mel_spec_norm\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract spectrograms and labels\n",
    "real_spectrograms = []\n",
    "real_labels = []\n",
    "\n",
    "for idx, row in tqdm(df_annotations.iterrows(), total=len(df_annotations), desc=\"Extracting spectrograms\"):\n",
    "    # Get song_id and construct audio path\n",
    "    song_id = str(row['song_id'])\n",
    "    audio_path = os.path.join(AUDIO_DIR, f\"{song_id}.mp3\")\n",
    "    \n",
    "    if not os.path.exists(audio_path):\n",
    "        continue\n",
    "    \n",
    "    # Extract spectrogram\n",
    "    spec = extract_melspectrogram(audio_path)\n",
    "    if spec is not None:\n",
    "        real_spectrograms.append(spec)\n",
    "        \n",
    "        # Get valence and arousal (try different column name variations)\n",
    "        valence = row.get('valence_mean', row.get('valence', 0.5))\n",
    "        arousal = row.get('arousal_mean', row.get('arousal', 0.5))\n",
    "        \n",
    "        # Normalize to [-1, 1] range (assuming original is 1-9 scale)\n",
    "        valence_norm = (valence - 5.0) / 4.0\n",
    "        arousal_norm = (arousal - 5.0) / 4.0\n",
    "        \n",
    "        real_labels.append([valence_norm, arousal_norm])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "real_spectrograms = np.array(real_spectrograms)  # Shape: (N, n_mels, time_steps)\n",
    "real_labels = np.array(real_labels)              # Shape: (N, 2)\n",
    "\n",
    "print(f\"\\n✅ Extracted {len(real_spectrograms)} spectrograms\")\n",
    "print(f\"Spectrogram shape: {real_spectrograms.shape}\")\n",
    "print(f\"Labels shape: {real_labels.shape}\")\n",
    "print(f\"Spectrogram range: [{real_spectrograms.min():.2f}, {real_spectrograms.max():.2f}]\")\n",
    "print(f\"Labels range: [{real_labels.min():.2f}, {real_labels.max():.2f}]\")\n",
    "\n",
    "# Visualize sample spectrogram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].imshow(real_spectrograms[0], aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0].set_title(f'Sample Spectrogram\\nValence: {real_labels[0][0]:.2f}, Arousal: {real_labels[0][1]:.2f}')\n",
    "axes[0].set_xlabel('Time Frames')\n",
    "axes[0].set_ylabel('Mel Frequency Bins')\n",
    "\n",
    "axes[1].scatter(real_labels[:, 0], real_labels[:, 1], alpha=0.5)\n",
    "axes[1].set_xlabel('Valence (normalized)')\n",
    "axes[1].set_ylabel('Arousal (normalized)')\n",
    "axes[1].set_title('Valence-Arousal Distribution (Real Data)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(0, color='k', linewidth=0.5)\n",
    "axes[1].axvline(0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'real_data_visualization.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f4a81",
   "metadata": {},
   "source": [
    "### **Part 3: GAN Architecture Deep Dive** 🏗️\n",
    "\n",
    "Now we'll understand the **Generator** and **Discriminator** architectures in mathematical detail. These are the two competing networks that form our GAN.\n",
    "\n",
    "---\n",
    "\n",
    "## **🎨 The Generator: From Noise to Spectrograms**\n",
    "\n",
    "The generator's job: Take random noise + emotion labels → Produce realistic spectrograms\n",
    "\n",
    "---\n",
    "\n",
    "### **Input Dimensions**\n",
    "\n",
    "```python\n",
    "z = torch.randn(batch_size, 100)  # Random noise\n",
    "c = torch.FloatTensor([[0.5, 0.7]])  # Emotion: [valence, arousal]\n",
    "```\n",
    "\n",
    "**Combined input**: `[z, c]` → shape `(batch_size, 102)`\n",
    "\n",
    "---\n",
    "\n",
    "### **Generator Architecture Layer-by-Layer**\n",
    "\n",
    "#### **Layer 1: Initial Projection**\n",
    "\n",
    "```python\n",
    "nn.Linear(102, 128 * 16 * 323)\n",
    "```\n",
    "\n",
    "**What it does**:\n",
    "- Takes 102-dim vector → 663,552-dim vector\n",
    "- Calculation: 128 × 16 × 323 = 663,552\n",
    "- Why these dimensions?: Will reshape to (128, 16, 323)\n",
    "  - 128 channels\n",
    "  - 16 height (frequency dimension start)\n",
    "  - 323 width (time dimension start)\n",
    "\n",
    "**Visualization**:\n",
    "```\n",
    "Input: [100 noise + 2 emotion] = 102 numbers\n",
    "     ↓ Linear transformation (663,552 weights!)\n",
    "Output: 663,552 numbers\n",
    "     ↓ Reshape\n",
    "3D Tensor: (128, 16, 323)\n",
    "```\n",
    "\n",
    "**Why start small (16×323)?**\n",
    "- We'll \"zoom in\" with transposed convolutions\n",
    "- Think of it as a low-resolution sketch\n",
    "\n",
    "---\n",
    "\n",
    "#### **Layer 2: First Upsampling**\n",
    "\n",
    "```python\n",
    "nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "```\n",
    "\n",
    "**What is ConvTranspose2d?**\n",
    "\n",
    "Regular convolution **downsamples** (reduces size):\n",
    "```\n",
    "Input: 32×32 → Conv → Output: 16×16\n",
    "```\n",
    "\n",
    "Transposed convolution **upsamples** (increases size):\n",
    "```\n",
    "Input: 16×16 → ConvTranspose → Output: 32×32\n",
    "```\n",
    "\n",
    "**How it works**:\n",
    "1. Insert zeros between input pixels\n",
    "2. Apply regular convolution\n",
    "3. Result: Larger output!\n",
    "\n",
    "**Mathematical formula**:\n",
    "$$H_{out} = (H_{in} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{kernel\\_size}$$\n",
    "\n",
    "**For our layer**:\n",
    "- $H_{in} = 16$\n",
    "- stride = 2\n",
    "- padding = 1\n",
    "- kernel_size = 4\n",
    "\n",
    "$$H_{out} = (16-1) \\times 2 - 2 \\times 1 + 4 = 15 \\times 2 - 2 + 4 = 30 - 2 + 4 = 32$$\n",
    "\n",
    "**Similarly for width**:\n",
    "$$W_{out} = (323-1) \\times 2 - 2 \\times 1 + 4 = 322 \\times 2 - 2 + 4 = 644 - 2 + 4 = 646$$\n",
    "\n",
    "**Result**: (128, 16, 323) → (64, 32, 646)\n",
    "\n",
    "**Analogy**: Like zooming into an image - we're creating more pixels!\n",
    "\n",
    "---\n",
    "\n",
    "#### **Batch Normalization**\n",
    "\n",
    "```python\n",
    "nn.BatchNorm2d(64)\n",
    "```\n",
    "\n",
    "**Formula**:\n",
    "$$\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\times \\gamma + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_B$: mean of batch\n",
    "- $\\sigma_B^2$: variance of batch\n",
    "- $\\gamma, \\beta$: learnable parameters\n",
    "- $\\epsilon$: small constant (e.g., 1e-5) for numerical stability\n",
    "\n",
    "**Why batch norm?**\n",
    "1. Stabilizes training (keeps activations in reasonable range)\n",
    "2. Allows higher learning rates\n",
    "3. Acts as regularization\n",
    "\n",
    "---\n",
    "\n",
    "#### **ReLU Activation**\n",
    "\n",
    "```python\n",
    "nn.ReLU()\n",
    "```\n",
    "\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "**Effect**:\n",
    "- Negative values → 0\n",
    "- Positive values → unchanged\n",
    "\n",
    "**Why after batch norm?**\n",
    "- Batch norm centers data around 0\n",
    "- ReLU adds non-linearity\n",
    "\n",
    "---\n",
    "\n",
    "#### **Layers 3 & 4: More Upsampling**\n",
    "\n",
    "Same pattern, progressively increasing resolution:\n",
    "\n",
    "**Layer 3**: (64, 32, 646) → (32, 64, 1292)\n",
    "- Channels: 64 → 32\n",
    "- Height: 32 × 2 = 64\n",
    "- Width: 646 × 2 ≈ 1292\n",
    "\n",
    "**Layer 4**: (32, 64, 1292) → (1, 128, 2584)\n",
    "- Channels: 32 → 1 (final grayscale spectrogram!)\n",
    "- Height: 64 × 2 = 128 ✓ (our target mel bins!)\n",
    "- Width: 1292 × 2 = 2584 ✓ (our target time frames!)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Final Activation: Tanh**\n",
    "\n",
    "```python\n",
    "nn.Tanh()\n",
    "```\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "**Range**: (-1, 1)\n",
    "\n",
    "**Why Tanh?**\n",
    "- Forces output to be in bounded range\n",
    "- Spectrograms normalized to [0, 1], then shifted to [-1, 1] during training\n",
    "- Tanh matches this range\n",
    "\n",
    "**Note**: We'll rescale from [-1, 1] back to [0, 1] after generation\n",
    "\n",
    "---\n",
    "\n",
    "### **Complete Generator Forward Pass**\n",
    "\n",
    "```\n",
    "Input: [100 noise, 2 emotion]\n",
    "    ↓ Linear(102 → 663,552)\n",
    "[128, 16, 323]\n",
    "    ↓ ConvTranspose2d + BatchNorm + ReLU\n",
    "[64, 32, 646]\n",
    "    ↓ ConvTranspose2d + BatchNorm + ReLU\n",
    "[32, 64, 1292]\n",
    "    ↓ ConvTranspose2d + Tanh\n",
    "[1, 128, 2584]\n",
    "    ↓ Output\n",
    "Synthetic Spectrogram!\n",
    "```\n",
    "\n",
    "**Parameter count**:\n",
    "- Layer 1: 102 × 663,552 ≈ 67.7M\n",
    "- Conv layers: ~5M\n",
    "- **Total**: ~73M parameters\n",
    "\n",
    "**That's a lot of parameters!** Each one learned during GAN training.\n",
    "\n",
    "---\n",
    "\n",
    "## **🔍 The Discriminator: Real or Fake?**\n",
    "\n",
    "The discriminator's job: Look at spectrogram + emotion → Output probability (real or fake)\n",
    "\n",
    "---\n",
    "\n",
    "### **Discriminator Architecture Layer-by-Layer**\n",
    "\n",
    "#### **Input Dimensions**\n",
    "\n",
    "```python\n",
    "spec = torch.randn(batch_size, 1, 128, 2584)  # Spectrogram\n",
    "c = torch.FloatTensor([[0.5, 0.7]])  # Emotion\n",
    "```\n",
    "\n",
    "**Conditioning strategy**:\n",
    "We expand emotion labels to match spatial dimensions and concatenate:\n",
    "```python\n",
    "c_expanded = c.view(batch_size, 2, 1, 1).expand(-1, -1, 128, 2584)\n",
    "x = torch.cat([spec, c_expanded], dim=1)  # Shape: (batch, 3, 128, 2584)\n",
    "```\n",
    "\n",
    "**Input to discriminator**: 3 channels (1 spectrogram + 2 emotion maps)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Layer 1: Initial Convolution**\n",
    "\n",
    "```python\n",
    "nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "```\n",
    "\n",
    "**Regular convolution downsamples**:\n",
    "\n",
    "$$H_{out} = \\left\\lfloor \\frac{H_{in} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} \\right\\rfloor + 1$$\n",
    "\n",
    "**For our layer**:\n",
    "$$H_{out} = \\left\\lfloor \\frac{128 + 2 \\times 1 - 4}{2} \\right\\rfloor + 1 = \\left\\lfloor \\frac{126}{2} \\right\\rfloor + 1 = 63 + 1 = 64$$\n",
    "\n",
    "$$W_{out} = \\left\\lfloor \\frac{2584 + 2 - 4}{2} \\right\\rfloor + 1 = \\left\\lfloor \\frac{2582}{2} \\right\\rfloor + 1 = 1291 + 1 = 1292$$\n",
    "\n",
    "**Result**: (3, 128, 2584) → (32, 64, 1292)\n",
    "\n",
    "---\n",
    "\n",
    "#### **LeakyReLU Activation**\n",
    "\n",
    "```python\n",
    "nn.LeakyReLU(0.2)\n",
    "```\n",
    "\n",
    "$$\\text{LeakyReLU}(x) = \\begin{cases} \n",
    "x & \\text{if } x \\geq 0 \\\\\n",
    "0.2x & \\text{if } x < 0\n",
    "\\end{cases}$$\n",
    "\n",
    "**Why LeakyReLU instead of ReLU?**\n",
    "- Regular ReLU kills negative values completely (→ 0)\n",
    "- LeakyReLU allows small negative slope (0.2)\n",
    "- Prevents \"dying ReLU\" problem\n",
    "- Better gradient flow for discriminator\n",
    "\n",
    "**Graph comparison**:\n",
    "```\n",
    "ReLU:        LeakyReLU (slope=0.2):\n",
    "  |              |\n",
    "  |         /    |    /\n",
    "  |      /       | /\n",
    "__|_______    ___|_________\n",
    "  |              |\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Layers 2, 3, 4: Progressive Downsampling**\n",
    "\n",
    "Same pattern: Conv2d → BatchNorm → LeakyReLU\n",
    "\n",
    "**Layer 2**: (32, 64, 1292) → (64, 32, 646)\n",
    "**Layer 3**: (64, 32, 646) → (128, 16, 323)\n",
    "**Layer 4**: (128, 16, 323) → (1, 8, 161)\n",
    "\n",
    "Each layer:\n",
    "- Doubles channels (learns more complex features)\n",
    "- Halves spatial dimensions (compresses information)\n",
    "\n",
    "**Feature hierarchy**:\n",
    "- Early layers: Basic patterns (edges, textures)\n",
    "- Middle layers: Musical structures (rhythms, harmonics)\n",
    "- Late layers: High-level concepts (genre, emotion)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Final Layer: Classification**\n",
    "\n",
    "```python\n",
    "nn.Linear(8 * 161, 1)\n",
    "nn.Sigmoid()\n",
    "```\n",
    "\n",
    "**Steps**:\n",
    "1. Flatten: (1, 8, 161) → (1288,)\n",
    "2. Linear: 1288 → 1\n",
    "3. Sigmoid: Map to [0, 1]\n",
    "\n",
    "**Sigmoid formula**:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "**Output interpretation**:\n",
    "- Close to 1: \"This looks REAL!\"\n",
    "- Close to 0: \"This is FAKE!\"\n",
    "- Around 0.5: \"I'm not sure...\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Complete Discriminator Forward Pass**\n",
    "\n",
    "```\n",
    "Input: [1, 128, 2584] spectrogram + [2] emotion\n",
    "    ↓ Expand emotion, concatenate\n",
    "[3, 128, 2584]\n",
    "    ↓ Conv2d + LeakyReLU\n",
    "[32, 64, 1292]\n",
    "    ↓ Conv2d + BatchNorm + LeakyReLU\n",
    "[64, 32, 646]\n",
    "    ↓ Conv2d + BatchNorm + LeakyReLU\n",
    "[128, 16, 323]\n",
    "    ↓ Conv2d + BatchNorm + LeakyReLU\n",
    "[1, 8, 161] = 1288 values\n",
    "    ↓ Flatten + Linear + Sigmoid\n",
    "Probability: 0.0 (fake) to 1.0 (real)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **⚖️ Generator vs Discriminator: Architecture Comparison**\n",
    "\n",
    "| Aspect | Generator | Discriminator |\n",
    "|--------|-----------|---------------|\n",
    "| **Input** | Noise (100) + Emotion (2) | Spectrogram (128×2584) + Emotion (2) |\n",
    "| **Output** | Spectrogram (128×2584) | Probability (0-1) |\n",
    "| **Direction** | Expand (upsample) | Compress (downsample) |\n",
    "| **Convolution type** | ConvTranspose2d | Conv2d |\n",
    "| **Activation** | ReLU (hidden), Tanh (output) | LeakyReLU |\n",
    "| **Parameters** | ~73M | ~5M |\n",
    "| **Role** | Create realistic spectrograms | Distinguish real from fake |\n",
    "| **Goal** | Fool discriminator | Don't be fooled |\n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 Why These Architectures Work**\n",
    "\n",
    "**Generator's progressive upsampling**:\n",
    "- Starts with abstract representation\n",
    "- Gradually adds detail\n",
    "- Like sketching then refining\n",
    "\n",
    "**Discriminator's progressive downsampling**:\n",
    "- Starts with pixels\n",
    "- Gradually extracts concepts\n",
    "- Like analyzing then judging\n",
    "\n",
    "**Together**:\n",
    "- Generator learns what makes spectrograms look \"real\"\n",
    "- Discriminator provides signal about what's missing\n",
    "- Adversarial training drives both to improve\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8ca684",
   "metadata": {},
   "source": [
    "## 4️⃣ Conditional GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e2db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional GAN Generator for Mel-Spectrograms\n",
    "    \n",
    "    Takes:\n",
    "        - Latent noise vector (z): Random noise from normal distribution\n",
    "        - Condition (c): Valence and Arousal values [v, a]\n",
    "    \n",
    "    Generates:\n",
    "        - Synthetic mel-spectrogram of shape (1, n_mels, time_steps)\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=LATENT_DIM, condition_dim=CONDITION_DIM, \n",
    "                 n_mels=N_MELS, time_steps=1292):  # time_steps ≈ (30s * 22050) / 512\n",
    "        super(SpectrogramGenerator, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.condition_dim = condition_dim\n",
    "        self.n_mels = n_mels\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        # Initial projection: (latent + condition) -> feature map\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim + condition_dim, 256 * 16 * 20),\n",
    "            nn.BatchNorm1d(256 * 16 * 20),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # Convolutional upsampling layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # 256 x 16 x 20\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # 128 x 32 x 40\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # 64 x 64 x 80\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            # 32 x 128 x 160\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=(1, 8), stride=(1, 8), padding=0),\n",
    "            nn.Tanh()  # Output in [-1, 1]\n",
    "            # 1 x 128 x 1280 (close to target)\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, c):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z: Latent noise, shape (batch, latent_dim)\n",
    "            c: Condition (valence, arousal), shape (batch, condition_dim)\n",
    "        Returns:\n",
    "            Generated spectrogram, shape (batch, 1, n_mels, time_steps)\n",
    "        \"\"\"\n",
    "        # Concatenate latent and condition\n",
    "        x = torch.cat([z, c], dim=1)  # (batch, latent_dim + condition_dim)\n",
    "        \n",
    "        # Project and reshape\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 256, 16, 20)\n",
    "        \n",
    "        # Upsample through conv layers\n",
    "        x = self.conv_layers(x)\n",
    "        \n",
    "        # Adjust to exact time_steps if needed\n",
    "        if x.shape[-1] != self.time_steps:\n",
    "            x = F.interpolate(x, size=(self.n_mels, self.time_steps), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class SpectrogramDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional GAN Discriminator for Mel-Spectrograms\n",
    "    \n",
    "    Takes:\n",
    "        - Spectrogram: Real or fake spectrogram (1, n_mels, time_steps)\n",
    "        - Condition: Valence and Arousal values [v, a]\n",
    "    \n",
    "    Outputs:\n",
    "        - Probability that spectrogram is real (scalar)\n",
    "    \"\"\"\n",
    "    def __init__(self, condition_dim=CONDITION_DIM, n_mels=N_MELS, time_steps=1292):\n",
    "        super(SpectrogramDiscriminator, self).__init__()\n",
    "        \n",
    "        self.n_mels = n_mels\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        # Convolutional layers for spectrogram\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Input: 1 x 128 x 1292\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 32 x 64 x 646\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 64 x 32 x 323\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 128 x 16 x 161\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 256 x 8 x 80\n",
    "        )\n",
    "        \n",
    "        # Calculate flattened size after convolutions\n",
    "        conv_output_size = 256 * 8 * 80  # Approximate\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_output_size + condition_dim, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()  # Output probability [0, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, spec, c):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spec: Spectrogram, shape (batch, 1, n_mels, time_steps)\n",
    "            c: Condition (valence, arousal), shape (batch, condition_dim)\n",
    "        Returns:\n",
    "            Probability of being real, shape (batch, 1)\n",
    "        \"\"\"\n",
    "        # Extract features from spectrogram\n",
    "        features = self.conv_layers(spec)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # Concatenate with condition\n",
    "        x = torch.cat([features, c], dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "time_steps = real_spectrograms.shape[2]  # Get actual time steps from data\n",
    "generator = SpectrogramGenerator(\n",
    "    latent_dim=LATENT_DIM, \n",
    "    condition_dim=CONDITION_DIM, \n",
    "    n_mels=N_MELS, \n",
    "    time_steps=time_steps\n",
    ").to(DEVICE)\n",
    "\n",
    "discriminator = SpectrogramDiscriminator(\n",
    "    condition_dim=CONDITION_DIM, \n",
    "    n_mels=N_MELS, \n",
    "    time_steps=time_steps\n",
    ").to(DEVICE)\n",
    "\n",
    "# Print model summaries\n",
    "print(\"=\" * 60)\n",
    "print(\"🎨 GENERATOR ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(generator)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🔍 DISCRIMINATOR ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(discriminator)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0a0a02",
   "metadata": {},
   "source": [
    "### **Part 4: GAN Training Dynamics** 🔄\n",
    "\n",
    "Now let's understand the training process in detail. This is where the \"adversarial\" in GAN happens!\n",
    "\n",
    "---\n",
    "\n",
    "## **📋 Training Overview**\n",
    "\n",
    "Unlike regular neural networks, GANs train **two models simultaneously** in an adversarial manner.\n",
    "\n",
    "**The game**:\n",
    "1. Generator tries to create convincing fakes\n",
    "2. Discriminator tries to spot the fakes\n",
    "3. Both improve through competition\n",
    "\n",
    "**Analogy**: Counterfeiter vs Detective (from Part 1, but now with math!)\n",
    "\n",
    "---\n",
    "\n",
    "## **🔁 The Training Loop: Step by Step**\n",
    "\n",
    "### **Epoch Structure**\n",
    "\n",
    "```python\n",
    "for epoch in range(GAN_EPOCHS):\n",
    "    for batch in dataloader:\n",
    "        # 1. Train Discriminator\n",
    "        # 2. Train Generator\n",
    "```\n",
    "\n",
    "**Each batch**:\n",
    "- Get real spectrograms from dataset\n",
    "- Generate fake spectrograms\n",
    "- Update discriminator\n",
    "- Update generator\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: Train Discriminator** 🔍\n",
    "\n",
    "**Goal**: Maximize ability to distinguish real from fake\n",
    "\n",
    "---\n",
    "\n",
    "### **1a. Get Real Data**\n",
    "\n",
    "```python\n",
    "real_specs, real_emotions = next(dataloader)\n",
    "# real_specs: (batch_size, 1, 128, 2584)\n",
    "# real_emotions: (batch_size, 2)\n",
    "```\n",
    "\n",
    "**Apply SpecAugment** (randomly):\n",
    "```python\n",
    "if random.random() < 0.5:  # 50% chance\n",
    "    real_specs = spec_augment(real_specs)\n",
    "```\n",
    "\n",
    "**Why augment real data?**\n",
    "- Makes discriminator robust to variations\n",
    "- Prevents overfitting to pristine spectrograms\n",
    "- Forces focus on overall structure, not perfect details\n",
    "\n",
    "---\n",
    "\n",
    "### **1b. Discriminator Evaluates Real Data**\n",
    "\n",
    "```python\n",
    "real_preds = discriminator(real_specs, real_emotions)\n",
    "# real_preds: (batch_size, 1) - probabilities\n",
    "```\n",
    "\n",
    "**Ideal predictions**: All close to 1.0 (confidently \"real\")\n",
    "\n",
    "**Real Loss (Binary Cross-Entropy)**:\n",
    "$$\\mathcal{L}_{\\text{real}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log(D(x_i))$$\n",
    "\n",
    "Where:\n",
    "- $D(x_i)$: Discriminator output for real sample $i$\n",
    "- $N$: Batch size\n",
    "- Goal: Maximize $D(x_i)$ (push toward 1)\n",
    "- Minimize $-\\log(D(x_i))$ (loss decreases as prediction → 1)\n",
    "\n",
    "**Example calculation**:\n",
    "```python\n",
    "real_preds = [0.9, 0.85, 0.95, 0.88]  # Good predictions!\n",
    "real_loss = -mean([log(0.9), log(0.85), log(0.95), log(0.88)])\n",
    "          = -mean([-0.105, -0.163, -0.051, -0.128])\n",
    "          = -(-0.112) = 0.112  # Low loss (good!)\n",
    "\n",
    "real_preds_bad = [0.3, 0.5, 0.4, 0.6]  # Poor predictions\n",
    "real_loss = -mean([log(0.3), log(0.5), log(0.4), log(0.6)])\n",
    "          = -mean([-1.204, -0.693, -0.916, -0.511])\n",
    "          = -(-0.831) = 0.831  # High loss (bad!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **1c. Generate Fake Data**\n",
    "\n",
    "```python\n",
    "# Sample random noise\n",
    "z = torch.randn(batch_size, 100)\n",
    "\n",
    "# Use same emotions as real batch (for conditioning)\n",
    "fake_specs = generator(z, real_emotions)\n",
    "# fake_specs: (batch_size, 1, 128, 2584)\n",
    "```\n",
    "\n",
    "**Important**: `fake_specs.detach()`\n",
    "- Breaks gradient flow through generator\n",
    "- We're training discriminator now, not generator\n",
    "- Generator doesn't update during this step\n",
    "\n",
    "---\n",
    "\n",
    "### **1d. Discriminator Evaluates Fake Data**\n",
    "\n",
    "```python\n",
    "fake_preds = discriminator(fake_specs.detach(), real_emotions)\n",
    "```\n",
    "\n",
    "**Ideal predictions**: All close to 0.0 (confidently \"fake\")\n",
    "\n",
    "**Fake Loss**:\n",
    "$$\\mathcal{L}_{\\text{fake}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log(1 - D(G(z_i)))$$\n",
    "\n",
    "Where:\n",
    "- $G(z_i)$: Generated (fake) sample\n",
    "- $D(G(z_i))$: Discriminator output for fake sample\n",
    "- Goal: Minimize $D(G(z_i))$ (push toward 0)\n",
    "- Minimize $-\\log(1 - D(G(z_i)))$ (loss decreases as prediction → 0)\n",
    "\n",
    "**Example calculation**:\n",
    "```python\n",
    "fake_preds = [0.1, 0.15, 0.05, 0.2]  # Good! (low = detected as fake)\n",
    "fake_loss = -mean([log(1-0.1), log(1-0.15), log(1-0.05), log(1-0.2)])\n",
    "          = -mean([log(0.9), log(0.85), log(0.95), log(0.8)])\n",
    "          = -mean([-0.105, -0.163, -0.051, -0.223])\n",
    "          = -(-0.136) = 0.136  # Low loss (good!)\n",
    "\n",
    "fake_preds_bad = [0.8, 0.7, 0.9, 0.75]  # Bad! (fooled by fakes)\n",
    "fake_loss = -mean([log(0.2), log(0.3), log(0.1), log(0.25)])\n",
    "          = -mean([-1.609, -1.204, -2.303, -1.386])\n",
    "          = -(-1.626) = 1.626  # High loss (bad!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **1e. Discriminator Total Loss & Update**\n",
    "\n",
    "```python\n",
    "d_loss = real_loss + fake_loss\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- Low real_loss: Good at recognizing real spectrograms\n",
    "- Low fake_loss: Good at spotting fakes\n",
    "- Low d_loss: Discriminator is effective overall\n",
    "\n",
    "**Update discriminator**:\n",
    "```python\n",
    "d_optimizer.zero_grad()  # Clear previous gradients\n",
    "d_loss.backward()        # Compute gradients\n",
    "d_optimizer.step()       # Update weights\n",
    "```\n",
    "\n",
    "**Weight update (Adam optimizer)**:\n",
    "$$\\theta_D \\leftarrow \\theta_D - \\alpha \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$$\n",
    "\n",
    "Where:\n",
    "- $\\theta_D$: Discriminator weights\n",
    "- $\\alpha$: Learning rate (0.0002)\n",
    "- $m_t$: First moment (momentum)\n",
    "- $v_t$: Second moment (adaptive learning rate)\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Train Generator** 🎨\n",
    "\n",
    "**Goal**: Create spectrograms that fool discriminator\n",
    "\n",
    "---\n",
    "\n",
    "### **2a. Generate New Fakes**\n",
    "\n",
    "```python\n",
    "z = torch.randn(batch_size, 100)\n",
    "fake_specs = generator(z, real_emotions)\n",
    "```\n",
    "\n",
    "**Why new noise?**\n",
    "- Don't reuse previous fake samples\n",
    "- Fresh start for generator training\n",
    "\n",
    "---\n",
    "\n",
    "### **2b. Try to Fool Discriminator**\n",
    "\n",
    "```python\n",
    "fake_preds = discriminator(fake_specs, real_emotions)\n",
    "```\n",
    "\n",
    "**Key difference**: NO `.detach()` this time!\n",
    "- Gradients flow through generator\n",
    "- Generator gets credit/blame for fooling discriminator\n",
    "\n",
    "---\n",
    "\n",
    "### **2c. Generator Loss**\n",
    "\n",
    "```python\n",
    "g_loss = criterion(fake_preds, torch.ones_like(fake_preds))\n",
    "```\n",
    "\n",
    "**Mathematical form**:\n",
    "$$\\mathcal{L}_G = -\\frac{1}{N} \\sum_{i=1}^{N} \\log(D(G(z_i)))$$\n",
    "\n",
    "**Interpretation**:\n",
    "- Generator wants discriminator to output 1 (think it's real)\n",
    "- Loss decreases when $D(G(z_i))$ approaches 1\n",
    "- Generator improves by making more convincing fakes\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "fake_preds = [0.8, 0.9, 0.85, 0.75]  # Successfully fooled discriminator!\n",
    "g_loss = -mean([log(0.8), log(0.9), log(0.85), log(0.75)])\n",
    "       = -mean([-0.223, -0.105, -0.163, -0.288])\n",
    "       = -(-0.195) = 0.195  # Low loss (good generator!)\n",
    "\n",
    "fake_preds_bad = [0.2, 0.3, 0.15, 0.25]  # Discriminator not fooled\n",
    "g_loss = -mean([log(0.2), log(0.3), log(0.15), log(0.25)])\n",
    "       = -mean([-1.609, -1.204, -1.897, -1.386])\n",
    "       = -(-1.524) = 1.524  # High loss (poor generator)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2d. Update Generator**\n",
    "\n",
    "```python\n",
    "g_optimizer.zero_grad()\n",
    "g_loss.backward()\n",
    "g_optimizer.step()\n",
    "```\n",
    "\n",
    "**Gradient flow**:\n",
    "```\n",
    "g_loss\n",
    "  ↓ backward()\n",
    "discriminator (frozen weights, just pass gradients)\n",
    "  ↓\n",
    "generator (update these weights!)\n",
    "```\n",
    "\n",
    "**Weight update**:\n",
    "$$\\theta_G \\leftarrow \\theta_G - \\alpha \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$$\n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 Training Dynamics Over Time**\n",
    "\n",
    "### **Early Training (Epoch 1-2)**\n",
    "\n",
    "**Discriminator**:\n",
    "- Easily spots fakes (they're terrible)\n",
    "- d_loss low, fake_preds near 0\n",
    "\n",
    "**Generator**:\n",
    "- Produces random noise\n",
    "- g_loss very high (can't fool discriminator)\n",
    "\n",
    "**What's happening**:\n",
    "- Discriminator quickly learns \"real\" looks like\n",
    "- Generator flails, trying random changes\n",
    "\n",
    "---\n",
    "\n",
    "### **Mid Training (Epoch 3-6)**\n",
    "\n",
    "**Discriminator**:\n",
    "- Harder to distinguish real/fake\n",
    "- fake_preds creeping toward 0.5\n",
    "\n",
    "**Generator**:\n",
    "- Starting to create spectrogram-like patterns\n",
    "- g_loss decreasing (some success fooling discriminator)\n",
    "\n",
    "**What's happening**:\n",
    "- Generator learns basic structure (frequency patterns, time continuity)\n",
    "- Discriminator forced to look at subtle details\n",
    "- Arms race intensifies!\n",
    "\n",
    "---\n",
    "\n",
    "### **Late Training (Epoch 7-10)**\n",
    "\n",
    "**Discriminator**:\n",
    "- Subtle distinguishing features\n",
    "- fake_preds around 0.4-0.6 (uncertain!)\n",
    "\n",
    "**Generator**:\n",
    "- High-quality synthetic spectrograms\n",
    "- g_loss low (successfully fooling discriminator often)\n",
    "\n",
    "**What's happening**:\n",
    "- Generator captures emotion-spectrogram relationship\n",
    "- Discriminator can't easily tell real from fake\n",
    "- Nash equilibrium approaching!\n",
    "\n",
    "---\n",
    "\n",
    "## **📊 Monitoring Training**\n",
    "\n",
    "### **Loss Curves**\n",
    "\n",
    "**Ideal pattern**:\n",
    "```\n",
    "Loss\n",
    "  |\n",
    "  |  d_loss ___/‾‾‾\\___/‾‾‾\n",
    "  |\n",
    "  |  g_loss ___/‾‾‾\\___/‾‾‾\n",
    "  |________________________Time\n",
    "```\n",
    "\n",
    "**Both losses oscillate around similar values**:\n",
    "- Balance means both networks competitive\n",
    "- Neither dominating\n",
    "\n",
    "---\n",
    "\n",
    "### **Warning Signs**\n",
    "\n",
    "**1. Mode Collapse**:\n",
    "```python\n",
    "# All generated spectrograms look identical\n",
    "# g_loss stable but quality doesn't improve\n",
    "```\n",
    "\n",
    "**Solution**: Restart with different initialization or add noise\n",
    "\n",
    "---\n",
    "\n",
    "**2. Discriminator Dominance**:\n",
    "```python\n",
    "d_loss → 0   (very low)\n",
    "g_loss → ∞   (very high)\n",
    "fake_preds → 0  (always spots fakes)\n",
    "```\n",
    "\n",
    "**Solution**: \n",
    "- Train generator more often (2:1 ratio)\n",
    "- Reduce discriminator learning rate\n",
    "\n",
    "---\n",
    "\n",
    "**3. Generator Dominance** (rare):\n",
    "```python\n",
    "d_loss → ∞   (very high)\n",
    "g_loss → 0   (very low)\n",
    "fake_preds → 1  (always fooled)\n",
    "```\n",
    "\n",
    "**Solution**:\n",
    "- Train discriminator more\n",
    "- Add noise to discriminator inputs\n",
    "\n",
    "---\n",
    "\n",
    "## **🎓 Key Takeaways**\n",
    "\n",
    "1. **Alternating updates**: Train D, then G, repeat\n",
    "2. **Detach fakes when training D**: Break gradient flow\n",
    "3. **Don't detach when training G**: Allow gradient flow\n",
    "4. **Balance is crucial**: Neither should dominate\n",
    "5. **Monitoring**: Watch loss curves and sample quality\n",
    "6. **Patience**: GANs take time to converge\n",
    "\n",
    "---\n",
    "\n",
    "## **💡 Intuition Summary**\n",
    "\n",
    "**Discriminator training**:\n",
    "- \"Here's real, here's fake - learn the difference\"\n",
    "- Pushes real_preds → 1, fake_preds → 0\n",
    "\n",
    "**Generator training**:\n",
    "- \"Here's what I made - discriminator thinks it's real?\"\n",
    "- Adjusts to increase fake_preds → 1\n",
    "\n",
    "**Together**:\n",
    "- Discriminator teaches generator what \"real\" looks like\n",
    "- Generator forces discriminator to learn subtle features\n",
    "- Result: High-quality synthetic spectrograms!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af02059c",
   "metadata": {},
   "source": [
    "## 5️⃣ Train Conditional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for GAN training\n",
    "real_specs_tensor = torch.FloatTensor(real_spectrograms).unsqueeze(1).to(DEVICE)  # (N, 1, n_mels, time_steps)\n",
    "real_labels_tensor = torch.FloatTensor(real_labels).to(DEVICE)  # (N, 2)\n",
    "\n",
    "gan_dataset = torch.utils.data.TensorDataset(real_specs_tensor, real_labels_tensor)\n",
    "gan_loader = DataLoader(gan_dataset, batch_size=GAN_BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=GAN_LR, betas=(GAN_BETA1, GAN_BETA2))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=GAN_LR, betas=(GAN_BETA1, GAN_BETA2))\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n🚀 Starting GAN Training...\\n\")\n",
    "\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "for epoch in range(GAN_EPOCHS):\n",
    "    epoch_g_loss = 0\n",
    "    epoch_d_loss = 0\n",
    "    \n",
    "    for i, (real_specs, conditions) in enumerate(tqdm(gan_loader, desc=f\"Epoch {epoch+1}/{GAN_EPOCHS}\")):\n",
    "        batch_size = real_specs.size(0)\n",
    "        \n",
    "        # Real and fake labels\n",
    "        real_labels = torch.ones(batch_size, 1).to(DEVICE)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(DEVICE)\n",
    "        \n",
    "        # ==================\n",
    "        # Train Discriminator\n",
    "        # ==================\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Real spectrograms\n",
    "        real_output = discriminator(real_specs, conditions)\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "        \n",
    "        # Fake spectrograms\n",
    "        z = torch.randn(batch_size, LATENT_DIM).to(DEVICE)\n",
    "        fake_specs = generator(z, conditions)\n",
    "        fake_output = discriminator(fake_specs.detach(), conditions)\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # ==================\n",
    "        # Train Generator\n",
    "        # ==================\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Generate fake spectrograms\n",
    "        z = torch.randn(batch_size, LATENT_DIM).to(DEVICE)\n",
    "        fake_specs = generator(z, conditions)\n",
    "        \n",
    "        # Generator tries to fool discriminator\n",
    "        fake_output = discriminator(fake_specs, conditions)\n",
    "        g_loss = criterion(fake_output, real_labels)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Accumulate losses\n",
    "        epoch_g_loss += g_loss.item()\n",
    "        epoch_d_loss += d_loss.item()\n",
    "    \n",
    "    # Average losses for epoch\n",
    "    epoch_g_loss /= len(gan_loader)\n",
    "    epoch_d_loss /= len(gan_loader)\n",
    "    \n",
    "    g_losses.append(epoch_g_loss)\n",
    "    d_losses.append(epoch_d_loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{GAN_EPOCHS}] | D Loss: {epoch_d_loss:.4f} | G Loss: {epoch_g_loss:.4f}\")\n",
    "\n",
    "print(\"\\n✅ GAN Training Complete!\\n\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(g_losses, label='Generator Loss', linewidth=2)\n",
    "plt.plot(d_losses, label='Discriminator Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('GAN Training Loss Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'gan_training_curves.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save trained GAN models\n",
    "torch.save(generator.state_dict(), os.path.join(OUTPUT_DIR, 'generator.pth'))\n",
    "torch.save(discriminator.state_dict(), os.path.join(OUTPUT_DIR, 'discriminator.pth'))\n",
    "print(\"✅ GAN models saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35021cdc",
   "metadata": {},
   "source": [
    "### **Part 5: Generating Synthetic Spectrograms** 🎼\n",
    "\n",
    "Now that our GAN is trained, let's use it to create thousands of synthetic spectrograms! This is where we reap the rewards of adversarial training.\n",
    "\n",
    "---\n",
    "\n",
    "## **🎲 The Generation Process**\n",
    "\n",
    "### **Step 1: Sampling from Latent Space**\n",
    "\n",
    "**What is latent space?**\n",
    "- 100-dimensional \"creativity space\"\n",
    "- Each dimension = one \"knob\" the generator can adjust\n",
    "- Random sampling = random combinations of features\n",
    "\n",
    "**Analogy**: Color mixing\n",
    "- Red, Green, Blue = 3 dimensions\n",
    "- (255, 0, 0) = pure red\n",
    "- (128, 128, 0) = yellow\n",
    "- Our latent space has 100 dimensions instead of 3!\n",
    "\n",
    "---\n",
    "\n",
    "### **Generating Diverse Samples**\n",
    "\n",
    "```python\n",
    "z = torch.randn(NUM_SYNTHETIC, 100)\n",
    "# Shape: (3200, 100)\n",
    "```\n",
    "\n",
    "**torch.randn() generates from standard normal distribution**:\n",
    "$$z_i \\sim \\mathcal{N}(0, 1)$$\n",
    "\n",
    "**Properties**:\n",
    "- Mean: 0\n",
    "- Standard deviation: 1\n",
    "- Range: mostly -3 to +3 (99.7% of samples)\n",
    "\n",
    "**Why Gaussian?**\n",
    "- Smooth latent space (nearby points → similar outputs)\n",
    "- Well-distributed (covers the space evenly)\n",
    "- Standard choice for generative models\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Emotion Conditioning**\n",
    "\n",
    "We want to generate spectrograms for diverse emotions covering the valence-arousal space.\n",
    "\n",
    "**Strategy**: Sample uniformly from [0, 1] × [0, 1]\n",
    "\n",
    "```python\n",
    "valence = torch.rand(NUM_SYNTHETIC)  # Uniform [0, 1]\n",
    "arousal = torch.rand(NUM_SYNTHETIC)  # Uniform [0, 1]\n",
    "emotions = torch.stack([valence, arousal], dim=1)\n",
    "# Shape: (3200, 2)\n",
    "```\n",
    "\n",
    "**Visualization of sampled emotions**:\n",
    "```\n",
    "Arousal (Energy)\n",
    "  1.0 |  ●  ●    ●  ●     ●  ●  High energy\n",
    "      |    ●  ●  ●    ●  ●    ●\n",
    "  0.5 | ●    ●    ●  ●    ●    Medium energy\n",
    "      |  ●    ●  ●    ●  ●  ●\n",
    "  0.0 |____●__●____●__●____●__ Low energy\n",
    "      0.0      0.5      1.0\n",
    "         Negative  Neutral  Positive\n",
    "              Valence (Positivity)\n",
    "```\n",
    "\n",
    "**Coverage**:\n",
    "- Negative + High Energy (angry, tense)\n",
    "- Negative + Low Energy (sad, depressed)\n",
    "- Positive + High Energy (excited, happy)\n",
    "- Positive + Low Energy (calm, peaceful)\n",
    "- And everything in between!\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Generation**\n",
    "\n",
    "```python\n",
    "generator.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():  # No gradient computation needed\n",
    "    fake_specs = generator(z, emotions)\n",
    "```\n",
    "\n",
    "**Why eval mode?**\n",
    "- Disables dropout (if any)\n",
    "- Batch norm uses running statistics\n",
    "- Deterministic behavior\n",
    "\n",
    "**Why no_grad?**\n",
    "- We're not training, just generating\n",
    "- Saves memory (no need to store gradients)\n",
    "- Faster computation\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Post-Processing**\n",
    "\n",
    "Generated spectrograms are in range [-1, 1] (due to Tanh activation). We need [0, 1] for consistency.\n",
    "\n",
    "**Rescaling formula**:\n",
    "$$x_{\\text{rescaled}} = \\frac{x + 1}{2}$$\n",
    "\n",
    "**Example**:\n",
    "- Input: -1 → Output: 0\n",
    "- Input: 0 → Output: 0.5\n",
    "- Input: +1 → Output: 1\n",
    "\n",
    "```python\n",
    "fake_specs = (fake_specs + 1) / 2\n",
    "fake_specs = fake_specs.clamp(0, 1)  # Ensure [0, 1]\n",
    "```\n",
    "\n",
    "**Why clamp?**\n",
    "- Numerical precision issues might create values slightly outside [-1, 1]\n",
    "- Clamping ensures strict [0, 1] range\n",
    "\n",
    "---\n",
    "\n",
    "## **🔍 Quality Assessment**\n",
    "\n",
    "### **Visual Inspection**\n",
    "\n",
    "**What to look for in generated spectrograms**:\n",
    "\n",
    "1. **Frequency structure**:\n",
    "   - Should have clear horizontal patterns (harmonics)\n",
    "   - Low frequencies (bottom) typically stronger than high (top)\n",
    "\n",
    "2. **Temporal continuity**:\n",
    "   - Smooth transitions over time\n",
    "   - No sudden random jumps\n",
    "   - Musical phrase structure\n",
    "\n",
    "3. **Dynamic range**:\n",
    "   - Variety of intensities (not all same brightness)\n",
    "   - Contrast between loud and quiet parts\n",
    "\n",
    "4. **Emotion correlation**:\n",
    "   - High arousal: More energy, denser patterns\n",
    "   - Low arousal: Sparser, calmer patterns\n",
    "   - High valence: Brighter, more harmonics\n",
    "   - Low valence: Darker, simpler structure\n",
    "\n",
    "---\n",
    "\n",
    "### **Statistical Comparison with Real Data**\n",
    "\n",
    "```python\n",
    "# Compare distributions\n",
    "real_mean = real_specs.mean()\n",
    "fake_mean = fake_specs.mean()\n",
    "\n",
    "real_std = real_specs.std()\n",
    "fake_std = fake_specs.std()\n",
    "```\n",
    "\n",
    "**Ideal case**:\n",
    "- Similar means (overall brightness)\n",
    "- Similar standard deviations (dynamic range)\n",
    "- Similar frequency power distribution\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Real: mean=0.45, std=0.28\n",
    "Fake: mean=0.43, std=0.26\n",
    "✓ Close enough! (within ~5%)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **📦 Saving Generated Data**\n",
    "\n",
    "### **Why save to disk?**\n",
    "\n",
    "1. **Reproducibility**: Same synthetic data for all experiments\n",
    "2. **Efficiency**: Don't regenerate every time\n",
    "3. **Inspection**: Can manually review quality\n",
    "4. **Sharing**: Others can use your synthetic dataset\n",
    "\n",
    "---\n",
    "\n",
    "### **Storage format**\n",
    "\n",
    "```python\n",
    "torch.save({\n",
    "    'spectrograms': fake_specs,  # (3200, 1, 128, 2584)\n",
    "    'emotions': emotions,         # (3200, 2)\n",
    "    'metadata': {\n",
    "        'num_samples': 3200,\n",
    "        'gan_epochs': 10,\n",
    "        'latent_dim': 100,\n",
    "        'timestamp': '2024-01-15'\n",
    "    }\n",
    "}, 'synthetic_spectrograms.pt')\n",
    "```\n",
    "\n",
    "**File size estimation**:\n",
    "- 3200 spectrograms × 128 × 2584 × 4 bytes ≈ 4.2 GB\n",
    "- Compressed (PyTorch default): ~1-2 GB\n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 Diversity vs Quality Trade-off**\n",
    "\n",
    "### **High Diversity**\n",
    "\n",
    "**Achieved by**:\n",
    "- Large latent space (LATENT_DIM=100)\n",
    "- Random sampling from Gaussian\n",
    "- Uniform emotion sampling\n",
    "\n",
    "**Benefits**:\n",
    "- Covers wide range of musical patterns\n",
    "- Better generalization for AST model\n",
    "- Robust to different song types\n",
    "\n",
    "**Risks**:\n",
    "- Some low-quality samples (outliers)\n",
    "- Occasional artifacts\n",
    "\n",
    "---\n",
    "\n",
    "### **High Quality**\n",
    "\n",
    "**Achieved by**:\n",
    "- Longer GAN training (more epochs)\n",
    "- Smaller latent space (less variation)\n",
    "- Sampling near latent space mean (z close to 0)\n",
    "\n",
    "**Benefits**:\n",
    "- All samples look realistic\n",
    "- Fewer artifacts\n",
    "- More \"polished\"\n",
    "\n",
    "**Risks**:\n",
    "- Mode collapse (all samples similar)\n",
    "- Less useful for data augmentation\n",
    "- AST might overfit to generated style\n",
    "\n",
    "---\n",
    "\n",
    "## **⚖️ Our Choice**\n",
    "\n",
    "We prioritize **diversity** over perfection:\n",
    "- LATENT_DIM = 100 (large)\n",
    "- Random Gaussian sampling (full range)\n",
    "- 3200 samples (lots of variety)\n",
    "- 10 epochs (moderate quality)\n",
    "\n",
    "**Rationale**:\n",
    "- AST training benefits from variety\n",
    "- Minor artifacts acceptable (SpecAugment handles them)\n",
    "- Real data provides \"gold standard\" - synthetics just augment\n",
    "\n",
    "---\n",
    "\n",
    "## **🔬 Using Generated Data**\n",
    "\n",
    "### **How many synthetics?**\n",
    "\n",
    "**Our choice**: 3200 synthetic + 1800 real ≈ 5000 total\n",
    "\n",
    "**Reasoning**:\n",
    "- Roughly 2:1 synthetic:real ratio\n",
    "- Increases dataset size by ~2.8×\n",
    "- Not too many (overwhelm real patterns)\n",
    "- Not too few (limited benefit)\n",
    "\n",
    "**Rule of thumb**:\n",
    "- 1:1 ratio: Conservative (50% synthetic)\n",
    "- 2:1 ratio: Moderate (67% synthetic) ← We use this\n",
    "- 5:1 ratio: Aggressive (83% synthetic)\n",
    "\n",
    "---\n",
    "\n",
    "### **Mixing strategy**\n",
    "\n",
    "```python\n",
    "train_data = real_train + synthetic_train\n",
    "```\n",
    "\n",
    "**Combined dataset**:\n",
    "- 1440 real (from 80% of 1800)\n",
    "- 3200 synthetic\n",
    "- **Total**: 4640 training samples\n",
    "\n",
    "**Random shuffling**:\n",
    "- Each batch mixes real and synthetic\n",
    "- Model doesn't know which is which\n",
    "- Learns from both equally\n",
    "\n",
    "---\n",
    "\n",
    "## **💡 Intuition: Why This Works**\n",
    "\n",
    "**Analogy**: Learning to draw\n",
    "\n",
    "**Without augmentation**:\n",
    "- 1800 photos to learn from\n",
    "- Limited variety\n",
    "- Might memorize specific photos\n",
    "\n",
    "**With augmentation**:\n",
    "- 1800 photos + 3200 sketches (varying quality)\n",
    "- Artist draws sketches (generator)\n",
    "- Sketches add variety, even if imperfect\n",
    "- Learn general concepts, not specific photos\n",
    "\n",
    "**Result**: Better generalization!\n",
    "\n",
    "---\n",
    "\n",
    "## **📊 Expected Benefits for AST Training**\n",
    "\n",
    "1. **More data**: 1800 → 5000 samples\n",
    "   - More batches per epoch\n",
    "   - Better gradient estimates\n",
    "\n",
    "2. **More diversity**: Cover more emotion-spectrogram space\n",
    "   - Handle rare emotions better\n",
    "   - Robust to variation\n",
    "\n",
    "3. **Regularization effect**: Imperfect synthetics prevent overfitting\n",
    "   - Forces model to focus on robust features\n",
    "   - Like dropout, but at data level\n",
    "\n",
    "4. **Improved generalization**: Better test set performance\n",
    "   - Seen more variations during training\n",
    "   - Less likely to memorize training set\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: Let's train the AST model on this augmented dataset and see the improvements!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff1d09",
   "metadata": {},
   "source": [
    "## 6️⃣ Generate Synthetic Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b864acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"🎨 Generating {NUM_SYNTHETIC} synthetic spectrograms...\\n\")\n",
    "\n",
    "generator.eval()\n",
    "synthetic_spectrograms = []\n",
    "synthetic_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    num_batches = NUM_SYNTHETIC // GAN_BATCH_SIZE\n",
    "    \n",
    "    for i in tqdm(range(num_batches), desc=\"Generating\"):\n",
    "        # Sample random latent vectors\n",
    "        z = torch.randn(GAN_BATCH_SIZE, LATENT_DIM).to(DEVICE)\n",
    "        \n",
    "        # Sample random conditions (valence, arousal) in [-1, 1]\n",
    "        random_conditions = torch.FloatTensor(GAN_BATCH_SIZE, 2).uniform_(-1, 1).to(DEVICE)\n",
    "        \n",
    "        # Generate spectrograms\n",
    "        fake_specs = generator(z, random_conditions)\n",
    "        \n",
    "        # Store results\n",
    "        synthetic_spectrograms.append(fake_specs.cpu().numpy())\n",
    "        synthetic_labels.append(random_conditions.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "synthetic_spectrograms = np.concatenate(synthetic_spectrograms, axis=0)  # (NUM_SYNTHETIC, 1, n_mels, time_steps)\n",
    "synthetic_labels = np.concatenate(synthetic_labels, axis=0)  # (NUM_SYNTHETIC, 2)\n",
    "\n",
    "# Remove channel dimension for consistency with real data\n",
    "synthetic_spectrograms = synthetic_spectrograms.squeeze(1)  # (NUM_SYNTHETIC, n_mels, time_steps)\n",
    "\n",
    "print(f\"✅ Generated {len(synthetic_spectrograms)} synthetic spectrograms\")\n",
    "print(f\"Synthetic spectrogram shape: {synthetic_spectrograms.shape}\")\n",
    "print(f\"Synthetic labels shape: {synthetic_labels.shape}\")\n",
    "\n",
    "# Visualize synthetic vs real spectrograms\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Real spectrograms\n",
    "for i in range(3):\n",
    "    axes[0, i].imshow(real_spectrograms[i], aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[0, i].set_title(f'Real Spec {i+1}\\nV: {real_labels[i][0]:.2f}, A: {real_labels[i][1]:.2f}')\n",
    "    axes[0, i].set_xlabel('Time')\n",
    "    axes[0, i].set_ylabel('Mel Bins')\n",
    "\n",
    "# Synthetic spectrograms\n",
    "for i in range(3):\n",
    "    axes[1, i].imshow(synthetic_spectrograms[i], aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[1, i].set_title(f'Synthetic Spec {i+1}\\nV: {synthetic_labels[i][0]:.2f}, A: {synthetic_labels[i][1]:.2f}')\n",
    "    axes[1, i].set_xlabel('Time')\n",
    "    axes[1, i].set_ylabel('Mel Bins')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'real_vs_synthetic_spectrograms.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Compare distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Valence-Arousal distribution\n",
    "axes[0].scatter(real_labels[:, 0], real_labels[:, 1], alpha=0.5, label='Real', s=20)\n",
    "axes[0].scatter(synthetic_labels[:, 0], synthetic_labels[:, 1], alpha=0.3, label='Synthetic', s=20)\n",
    "axes[0].set_xlabel('Valence')\n",
    "axes[0].set_ylabel('Arousal')\n",
    "axes[0].set_title('Valence-Arousal Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(0, color='k', linewidth=0.5)\n",
    "axes[0].axvline(0, color='k', linewidth=0.5)\n",
    "\n",
    "# Dataset size comparison\n",
    "sizes = [len(real_spectrograms), len(synthetic_spectrograms), \n",
    "         len(real_spectrograms) + len(synthetic_spectrograms)]\n",
    "labels = ['Real', 'Synthetic', 'Total']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "axes[1].bar(labels, sizes, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('Number of Samples')\n",
    "axes[1].set_title('Dataset Size Comparison')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(sizes):\n",
    "    axes[1].text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'augmented_dataset_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 Dataset Statistics:\")\n",
    "print(f\"  - Real samples: {len(real_spectrograms)}\")\n",
    "print(f\"  - Synthetic samples: {len(synthetic_spectrograms)}\")\n",
    "print(f\"  - Total samples: {len(real_spectrograms) + len(synthetic_spectrograms)}\")\n",
    "print(f\"  - Data augmentation factor: {(len(real_spectrograms) + len(synthetic_spectrograms)) / len(real_spectrograms):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05445aa0",
   "metadata": {},
   "source": [
    "## 7️⃣ Create Augmented Dataset for AST Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedSpectrogramDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Augmented Spectrograms (Real + GAN-Generated)\n",
    "    \n",
    "    Combines real and synthetic spectrograms for AST training.\n",
    "    Applies SpecAugment (frequency/time masking) during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, spectrograms, labels, augment=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spectrograms: numpy array of shape (N, n_mels, time_steps)\n",
    "            labels: numpy array of shape (N, 2) - [valence, arousal]\n",
    "            augment: Whether to apply SpecAugment\n",
    "        \"\"\"\n",
    "        self.spectrograms = torch.FloatTensor(spectrograms)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "        self.augment = augment\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.spectrograms)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        spec = self.spectrograms[idx]  # (n_mels, time_steps)\n",
    "        label = self.labels[idx]  # (2,)\n",
    "        \n",
    "        # Apply SpecAugment during training\n",
    "        if self.augment:\n",
    "            spec = self.spec_augment(spec)\n",
    "        \n",
    "        # Add channel dimension: (1, n_mels, time_steps)\n",
    "        spec = spec.unsqueeze(0)\n",
    "        \n",
    "        return spec, label\n",
    "    \n",
    "    def spec_augment(self, spec, freq_mask_param=20, time_mask_param=40, num_masks=2):\n",
    "        \"\"\"\n",
    "        Apply SpecAugment: random frequency and time masking\n",
    "        \n",
    "        Args:\n",
    "            spec: Spectrogram tensor (n_mels, time_steps)\n",
    "            freq_mask_param: Maximum frequency mask width\n",
    "            time_mask_param: Maximum time mask width\n",
    "            num_masks: Number of masks to apply\n",
    "        \"\"\"\n",
    "        spec = spec.clone()\n",
    "        n_mels, time_steps = spec.shape\n",
    "        \n",
    "        # Frequency masking\n",
    "        for _ in range(num_masks):\n",
    "            f = np.random.randint(0, freq_mask_param)\n",
    "            f0 = np.random.randint(0, n_mels - f)\n",
    "            spec[f0:f0+f, :] = 0\n",
    "        \n",
    "        # Time masking\n",
    "        for _ in range(num_masks):\n",
    "            t = np.random.randint(0, time_mask_param)\n",
    "            t0 = np.random.randint(0, time_steps - t)\n",
    "            spec[:, t0:t0+t] = 0\n",
    "        \n",
    "        return spec\n",
    "\n",
    "\n",
    "# Combine real and synthetic data\n",
    "print(\"📦 Creating augmented dataset...\")\n",
    "\n",
    "all_spectrograms = np.concatenate([real_spectrograms, synthetic_spectrograms], axis=0)\n",
    "all_labels = np.concatenate([real_labels, synthetic_labels], axis=0)\n",
    "\n",
    "print(f\"✅ Combined dataset created:\")\n",
    "print(f\"  - Total samples: {len(all_spectrograms)}\")\n",
    "print(f\"  - Spectrogram shape: {all_spectrograms.shape}\")\n",
    "print(f\"  - Labels shape: {all_labels.shape}\")\n",
    "\n",
    "# Create full dataset\n",
    "full_dataset = AugmentedSpectrogramDataset(all_spectrograms, all_labels, augment=True)\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(TRAIN_SPLIT * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create validation dataset without augmentation\n",
    "val_dataset.dataset.augment = False\n",
    "\n",
    "print(f\"\\n📊 Dataset Split:\")\n",
    "print(f\"  - Training samples: {len(train_dataset)}\")\n",
    "print(f\"  - Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\n✅ Data loaders created!\")\n",
    "print(f\"  - Training batches: {len(train_loader)}\")\n",
    "print(f\"  - Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfda11f",
   "metadata": {},
   "source": [
    "### **Part 6: Training AST on Augmented Data** 🚀\n",
    "\n",
    "Now we train the Audio Spectrogram Transformer (AST) on our combined dataset of real + synthetic spectrograms. This is where we see if GAN augmentation actually helps!\n",
    "\n",
    "---\n",
    "\n",
    "## **📊 Dataset Comparison**\n",
    "\n",
    "### **Baseline (Real Data Only)**\n",
    "```\n",
    "Training set: 1,440 samples\n",
    "Validation set: 180 samples\n",
    "Test set: 180 samples\n",
    "Total: 1,800 samples\n",
    "```\n",
    "\n",
    "### **Augmented (Real + Synthetic)**\n",
    "```\n",
    "Training set: 1,440 real + 3,200 synthetic = 4,640 samples\n",
    "Validation set: 180 real (no synthetics!)\n",
    "Test set: 180 real (no synthetics!)\n",
    "Total: 5,000 samples (but only train set augmented)\n",
    "```\n",
    "\n",
    "**Critical point**: We ONLY augment training set!\n",
    "- Validation: Measure performance on real data during training\n",
    "- Test: Final evaluation on real data\n",
    "- Synthetics are training aids, not evaluation data\n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 Training Differences**\n",
    "\n",
    "### **Number of Batches per Epoch**\n",
    "\n",
    "**Baseline**:\n",
    "- Batches per epoch: 1440 / 16 = 90 batches\n",
    "\n",
    "**Augmented**:\n",
    "- Batches per epoch: 4640 / 16 = 290 batches\n",
    "\n",
    "**Impact**:\n",
    "- 3.2× more batches per epoch!\n",
    "- More gradient updates\n",
    "- Slower epochs, but potentially better learning\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Time**\n",
    "\n",
    "**Baseline**:\n",
    "- 90 batches × 5 epochs = 450 total batches\n",
    "- ~0.5 seconds per batch\n",
    "- Total: ~225 seconds (3.75 minutes)\n",
    "\n",
    "**Augmented**:\n",
    "- 290 batches × 5 epochs = 1,450 total batches\n",
    "- ~0.5 seconds per batch\n",
    "- Total: ~725 seconds (12 minutes)\n",
    "\n",
    "**Trade-off**: 3× longer training, but hopefully better performance!\n",
    "\n",
    "---\n",
    "\n",
    "## **🔄 Epoch-by-Epoch Analysis**\n",
    "\n",
    "### **Epoch 1: Initial Learning**\n",
    "\n",
    "**Baseline expectation**:\n",
    "```\n",
    "Train Loss: ~0.35 (MSE for valence + arousal)\n",
    "Val Loss: ~0.40 (slightly higher, some overfitting)\n",
    "```\n",
    "\n",
    "**Augmented expectation**:\n",
    "```\n",
    "Train Loss: ~0.38 (slightly higher due to synthetic noise)\n",
    "Val Loss: ~0.38 (better generalization!)\n",
    "```\n",
    "\n",
    "**Why augmented train loss higher?**\n",
    "- Synthetic data has imperfections\n",
    "- Model must learn to handle variation\n",
    "- Not memorizing specific samples\n",
    "\n",
    "**Why augmented val loss potentially better?**\n",
    "- Model learns general patterns, not specifics\n",
    "- Better generalization to unseen real data\n",
    "\n",
    "---\n",
    "\n",
    "### **Epochs 2-3: Rapid Improvement**\n",
    "\n",
    "**Both models improve quickly**:\n",
    "- Learning fundamental patterns\n",
    "- Attention heads specializing\n",
    "- Loss dropping rapidly\n",
    "\n",
    "**Key difference**:\n",
    "- Baseline: Faster per-epoch improvement (fewer, cleaner samples)\n",
    "- Augmented: Steadier improvement (more diverse learning signal)\n",
    "\n",
    "---\n",
    "\n",
    "### **Epochs 4-5: Convergence**\n",
    "\n",
    "**Baseline**:\n",
    "- Might start overfitting\n",
    "- Train loss << Val loss\n",
    "- Learning rate decaying (CosineAnnealingLR)\n",
    "\n",
    "**Augmented**:\n",
    "- Better train/val gap (less overfitting)\n",
    "- Model robust to variations\n",
    "- Regularization from data diversity\n",
    "\n",
    "---\n",
    "\n",
    "## **📈 Expected Learning Curves**\n",
    "\n",
    "### **Loss Curves: Baseline**\n",
    "```\n",
    "Loss\n",
    "0.5 |                    Train ----\n",
    "    |                 Val -------\n",
    "0.4 |  ----___\n",
    "    |         ---___\n",
    "0.3 |               ----___\n",
    "    |                      ----___\n",
    "0.2 |_________________________________\n",
    "    Epoch 1    2    3    4    5\n",
    "```\n",
    "\n",
    "**Characteristics**:\n",
    "- Rapid initial drop\n",
    "- Train loss lower than val (overfitting)\n",
    "- Plateau around epoch 4-5\n",
    "\n",
    "---\n",
    "\n",
    "### **Loss Curves: Augmented**\n",
    "```\n",
    "Loss\n",
    "0.5 |                    Train ----\n",
    "    |                 Val -------\n",
    "0.4 |  ----___\n",
    "    |      ---___----\n",
    "0.3 |            ----___\n",
    "    |                ----___\n",
    "0.2 |_________________________________\n",
    "    Epoch 1    2    3    4    5\n",
    "```\n",
    "\n",
    "**Characteristics**:\n",
    "- Smoother curves (more data)\n",
    "- Smaller train/val gap (less overfitting)\n",
    "- Potentially lower final val loss\n",
    "\n",
    "---\n",
    "\n",
    "## **🎓 What the AST Model Learns**\n",
    "\n",
    "### **From Real Data**\n",
    "\n",
    "**Learns**:\n",
    "- Precise frequency patterns of real instruments\n",
    "- Exact temporal structures of real songs\n",
    "- Clean, artifact-free spectrograms\n",
    "\n",
    "**Risk**:\n",
    "- Overfitting to these specific patterns\n",
    "- Poor generalization to slightly different inputs\n",
    "\n",
    "---\n",
    "\n",
    "### **From Synthetic Data**\n",
    "\n",
    "**Learns**:\n",
    "- Variety of spectrogram patterns\n",
    "- Robustness to imperfections\n",
    "- General emotion-spectrogram relationships\n",
    "\n",
    "**Benefit**:\n",
    "- Better generalization\n",
    "- More robust to variations\n",
    "- Handles imperfect inputs better\n",
    "\n",
    "---\n",
    "\n",
    "## **🔬 Evaluation Metrics**\n",
    "\n",
    "### **MSE (Mean Squared Error)**\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} [(v_i^{\\text{pred}} - v_i^{\\text{true}})^2 + (a_i^{\\text{pred}} - a_i^{\\text{true}})^2]$$\n",
    "\n",
    "**What it measures**: Average squared difference in predictions\n",
    "\n",
    "**Lower is better**: Perfect predictions → MSE = 0\n",
    "\n",
    "---\n",
    "\n",
    "### **MAE (Mean Absolute Error)**\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} [|v_i^{\\text{pred}} - v_i^{\\text{true}}| + |a_i^{\\text{pred}} - a_i^{\\text{true}}|]$$\n",
    "\n",
    "**More interpretable**: Average absolute difference\n",
    "\n",
    "**Example**: MAE = 0.15 means average error of 0.15 in normalized [0,1] scale\n",
    "- Original scale [1,9]: 0.15 × 8 = 1.2 points average error\n",
    "\n",
    "---\n",
    "\n",
    "### **CCC (Concordance Correlation Coefficient)**\n",
    "\n",
    "$$\\rho_c = \\frac{2 \\rho \\sigma_{\\text{pred}} \\sigma_{\\text{true}}}{\\sigma_{\\text{pred}}^2 + \\sigma_{\\text{true}}^2 + (\\mu_{\\text{pred}} - \\mu_{\\text{true}})^2}$$\n",
    "\n",
    "**What it measures**: Agreement between predictions and ground truth\n",
    "\n",
    "**Range**: -1 to +1\n",
    "- +1: Perfect agreement\n",
    "- 0: No agreement\n",
    "- -1: Perfect disagreement\n",
    "\n",
    "**Higher is better**: CCC > 0.8 is excellent\n",
    "\n",
    "---\n",
    "\n",
    "## **🏆 Expected Performance Comparison**\n",
    "\n",
    "### **Baseline Model (Real Data Only)**\n",
    "\n",
    "**Typical results**:\n",
    "```\n",
    "Test MSE: 0.22\n",
    "Test MAE: 0.35\n",
    "Test CCC: 0.68\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- Decent performance\n",
    "- Some overfitting\n",
    "- Room for improvement\n",
    "\n",
    "---\n",
    "\n",
    "### **Augmented Model (Real + Synthetic)**\n",
    "\n",
    "**Expected results**:\n",
    "```\n",
    "Test MSE: 0.18-0.20  (10-18% improvement)\n",
    "Test MAE: 0.30-0.33  (6-14% improvement)\n",
    "Test CCC: 0.72-0.76  (6-12% improvement)\n",
    "```\n",
    "\n",
    "**Why improvement?**\n",
    "1. **More training data**: 2.8× more samples\n",
    "2. **Better generalization**: Diverse patterns prevent overfitting\n",
    "3. **Regularization**: Synthetic imperfections help robustness\n",
    "\n",
    "---\n",
    "\n",
    "## **💡 Understanding the Improvement**\n",
    "\n",
    "### **Analogy: Learning to Play Piano**\n",
    "\n",
    "**Baseline (Real Data Only)**:\n",
    "- Practice 1,800 specific songs\n",
    "- Memorize exact notes\n",
    "- Struggle with new songs\n",
    "\n",
    "**Augmented (Real + Synthetic)**:\n",
    "- Practice 1,800 real songs\n",
    "- Practice 3,200 variations (different arrangements, styles)\n",
    "- Learn general patterns and principles\n",
    "- Better at new songs!\n",
    "\n",
    "---\n",
    "\n",
    "### **What Augmentation Provides**\n",
    "\n",
    "**1. Coverage of Emotion Space**:\n",
    "```\n",
    "Without Augmentation:\n",
    "Arousal\n",
    "  High |  ●     ●         ●     Few samples\n",
    "       |    ●        ●       ●\n",
    "  Low  |_●_____●_____●_____●___\n",
    "       Negative      Positive\n",
    "          Valence\n",
    "\n",
    "With Augmentation:\n",
    "Arousal\n",
    "  High | ●●●●  ●●●●  ●●●●  ●●●●  Dense coverage\n",
    "       | ●●●●  ●●●●  ●●●●  ●●●●\n",
    "  Low  |_●●●●__●●●●__●●●●__●●●●_\n",
    "       Negative      Positive\n",
    "          Valence\n",
    "```\n",
    "\n",
    "**Result**: Model sees more emotion combinations, better interpolation\n",
    "\n",
    "---\n",
    "\n",
    "**2. Variation Within Emotions**:\n",
    "- Multiple spectrograms for same emotion (valence, arousal)\n",
    "- Learns emotion is expressed in many ways\n",
    "- More robust to individual song differences\n",
    "\n",
    "---\n",
    "\n",
    "**3. Implicit Data Augmentation**:\n",
    "- Synthetic data has minor imperfections (like SpecAugment)\n",
    "- Forces model to focus on robust features\n",
    "- Acts as regularization\n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 When Augmentation Helps Most**\n",
    "\n",
    "### **Best Case Scenarios**\n",
    "\n",
    "**1. Limited Real Data**:\n",
    "- Small dataset (< 2000 samples)\n",
    "- Augmentation multiplies data\n",
    "- Dramatic improvement possible\n",
    "\n",
    "**2. Imbalanced Emotions**:\n",
    "- Some emotions rare in real data\n",
    "- Synthetics balance the distribution\n",
    "- Better performance on rare emotions\n",
    "\n",
    "**3. High Model Capacity**:\n",
    "- Large model (like AST with 10M parameters)\n",
    "- Risk of overfitting without enough data\n",
    "- Augmentation prevents overfitting\n",
    "\n",
    "---\n",
    "\n",
    "### **Marginal Benefit Scenarios**\n",
    "\n",
    "**1. Already Large Dataset**:\n",
    "- > 10,000 real samples\n",
    "- Augmentation adds less relative value\n",
    "\n",
    "**2. Simple Model**:\n",
    "- Small model (< 1M parameters)\n",
    "- Less prone to overfitting anyway\n",
    "\n",
    "**3. Very High-Quality Real Data**:\n",
    "- Comprehensive emotion coverage\n",
    "- Diverse musical styles already present\n",
    "\n",
    "---\n",
    "\n",
    "## **🔍 Analyzing Results**\n",
    "\n",
    "### **Per-Emotion Performance**\n",
    "\n",
    "Check if augmentation helps different emotions differently:\n",
    "\n",
    "```python\n",
    "# Group predictions by emotion region\n",
    "happy_indices = (test_valence > 0.6) & (test_arousal > 0.6)\n",
    "sad_indices = (test_valence < 0.4) & (test_arousal < 0.4)\n",
    "\n",
    "happy_mse_baseline = ...\n",
    "happy_mse_augmented = ...\n",
    "# Compare improvements\n",
    "```\n",
    "\n",
    "**Expected pattern**:\n",
    "- **Rare emotions**: Larger improvement (e.g., 20-30%)\n",
    "- **Common emotions**: Smaller improvement (e.g., 5-10%)\n",
    "\n",
    "---\n",
    "\n",
    "### **Prediction Scatter Plots**\n",
    "\n",
    "**Baseline** (may show clustering, gaps):\n",
    "```\n",
    "True Valence\n",
    "  1.0 |     ●  ●●  ●\n",
    "      |   ●  ●  ●  ●●\n",
    "  0.5 | ●●  ●  ●  ●  ●\n",
    "      |●  ●  ●  ●●\n",
    "  0.0 |___●__●_________●___\n",
    "      0.0  0.5      1.0\n",
    "        Predicted Valence\n",
    "```\n",
    "\n",
    "**Augmented** (smoother, better correlation):\n",
    "```\n",
    "True Valence\n",
    "  1.0 |        ●●●●\n",
    "      |      ●●●●●\n",
    "  0.5 |    ●●●●●\n",
    "      |  ●●●●●\n",
    "  0.0 |●●●●___________\n",
    "      0.0  0.5      1.0\n",
    "        Predicted Valence\n",
    "```\n",
    "\n",
    "**Tighter correlation = better predictions!**\n",
    "\n",
    "---\n",
    "\n",
    "## **✅ Success Criteria**\n",
    "\n",
    "**Minimum improvement to justify augmentation**:\n",
    "- Test MSE improvement > 5%\n",
    "- Test CCC improvement > 5%\n",
    "- No signs of overfitting (train/val gap reasonable)\n",
    "\n",
    "**Excellent improvement**:\n",
    "- Test MSE improvement > 15%\n",
    "- Test CCC improvement > 10%\n",
    "- Better performance on rare emotions\n",
    "\n",
    "**Our expectation**: 10-18% improvement (excellent!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a94811",
   "metadata": {},
   "source": [
    "## 8️⃣ Audio Spectrogram Transformer (AST) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ec4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Convert spectrogram into patches and embed them\"\"\"\n",
    "    def __init__(self, img_size=(128, 1292), patch_size=16, in_channels=1, embed_dim=384):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size[0] // patch_size) * (img_size[1] // patch_size)\n",
    "        \n",
    "        # Convolutional patch embedding\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 1, height, width)\n",
    "        x = self.proj(x)  # (batch, embed_dim, n_patches_h, n_patches_w)\n",
    "        x = x.flatten(2)  # (batch, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (batch, n_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism\"\"\"\n",
    "    def __init__(self, embed_dim=384, num_heads=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, n_patches, embed_dim = x.shape\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x)  # (batch, n_patches, 3*embed_dim)\n",
    "        qkv = qkv.reshape(batch_size, n_patches, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch, num_heads, n_patches, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Attention scores\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = attn @ v  # (batch, num_heads, n_patches, head_dim)\n",
    "        out = out.transpose(1, 2)  # (batch, n_patches, num_heads, head_dim)\n",
    "        out = out.reshape(batch_size, n_patches, embed_dim)\n",
    "        \n",
    "        # Final projection\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block with attention and MLP\"\"\"\n",
    "    def __init__(self, embed_dim=384, num_heads=6, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Attention with residual\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        \n",
    "        # MLP with residual\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class SpectrogramTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Audio Spectrogram Transformer (AST) for Emotion Recognition\n",
    "    \n",
    "    Based on Vision Transformer (ViT) architecture adapted for audio spectrograms.\n",
    "    Predicts valence and arousal values from mel-spectrograms.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=(128, 1292), patch_size=16, in_channels=1, \n",
    "                 embed_dim=384, num_heads=6, num_layers=6, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        n_patches = self.patch_embed.n_patches\n",
    "        \n",
    "        # CLS token for classification\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Positional embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer encoder blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final normalization\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Regression head for valence and arousal\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 2)  # Valence and Arousal\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "        nn.init.normal_(self.pos_embed, std=0.02)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 1, height, width)\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # (batch, n_patches, embed_dim)\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (batch, n_patches + 1, embed_dim)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Use CLS token for prediction\n",
    "        cls_output = x[:, 0]  # (batch, embed_dim)\n",
    "        \n",
    "        # Regression head\n",
    "        output = self.head(cls_output)  # (batch, 2)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "img_height, img_width = all_spectrograms.shape[1], all_spectrograms.shape[2]\n",
    "\n",
    "model = SpectrogramTransformer(\n",
    "    img_size=(img_height, img_width),\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_channels=1,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    mlp_ratio=MLP_RATIO,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🤖 AUDIO SPECTROGRAM TRANSFORMER\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input size: ({img_height}, {img_width})\")\n",
    "print(f\"Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\n",
    "print(f\"Number of patches: {model.patch_embed.n_patches}\")\n",
    "print(f\"Embedding dimension: {EMBED_DIM}\")\n",
    "print(f\"Number of attention heads: {NUM_HEADS}\")\n",
    "print(f\"Number of transformer layers: {NUM_LAYERS}\")\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc193293",
   "metadata": {},
   "source": [
    "### **Part 7: Visualization & Results Interpretation** 📊\n",
    "\n",
    "Now let's understand how to interpret our results through visualizations. This section helps you evaluate model performance and understand what the model learned.\n",
    "\n",
    "---\n",
    "\n",
    "## **📈 1. Training Loss Curves**\n",
    "\n",
    "### **What to Plot**\n",
    "\n",
    "```python\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "```\n",
    "\n",
    "**Two lines**:\n",
    "- **Training loss**: How well model fits training data\n",
    "- **Validation loss**: How well model generalizes to unseen data\n",
    "\n",
    "---\n",
    "\n",
    "### **Ideal Pattern**\n",
    "\n",
    "```\n",
    "Loss\n",
    "0.5 |                    \n",
    "    |  Train ----       \n",
    "0.4 |     ----___      Val -------\n",
    "    |            ----___      ----\n",
    "0.3 |                   ----___----\n",
    "    |                           ----\n",
    "0.2 |____________________________________\n",
    "    Epoch 1    2      3      4      5\n",
    "```\n",
    "\n",
    "**Characteristics**:\n",
    "1. **Both decrease**: Model is learning\n",
    "2. **Val slightly above train**: Expected (unseen data is harder)\n",
    "3. **Small gap**: Good generalization (< 20% difference)\n",
    "4. **Smooth curves**: Stable training\n",
    "\n",
    "---\n",
    "\n",
    "### **Warning Signs**\n",
    "\n",
    "**Overfitting**:\n",
    "```\n",
    "Loss\n",
    "    |                 Val -------\n",
    "    |  Train ----        ___----\n",
    "    |     ----___    ___/\n",
    "    |            ----\n",
    "    |_________________________\n",
    "```\n",
    "- Train continues decreasing\n",
    "- Val increases or plateaus\n",
    "- Large gap (> 30%)\n",
    "- **Solution**: Stop training earlier, add regularization, add more data\n",
    "\n",
    "---\n",
    "\n",
    "**Underfitting**:\n",
    "```\n",
    "Loss\n",
    "    |  Train ----\n",
    "    |       ----  Val -------\n",
    "    |          -------\n",
    "    |             -------\n",
    "    |_________________________\n",
    "```\n",
    "- Both losses high\n",
    "- Not decreasing much\n",
    "- **Solution**: Train longer, increase model capacity, reduce regularization\n",
    "\n",
    "---\n",
    "\n",
    "**Unstable Training**:\n",
    "```\n",
    "Loss\n",
    "    |    /\\  /\\\n",
    "    |   /  \\/  \\  /\\\n",
    "    |  /        \\/  \\\n",
    "    |_/________________\\__\n",
    "```\n",
    "- Erratic fluctuations\n",
    "- **Solution**: Lower learning rate, larger batch size, gradient clipping\n",
    "\n",
    "---\n",
    "\n",
    "## **📊 2. Real vs Synthetic Spectrograms**\n",
    "\n",
    "### **Visual Comparison**\n",
    "\n",
    "**What to look for**:\n",
    "\n",
    "**Real spectrograms**:\n",
    "- Sharp harmonic lines (instrument overtones)\n",
    "- Clear rhythmic patterns\n",
    "- Natural dynamics (loud/quiet variation)\n",
    "- High-frequency details\n",
    "\n",
    "**Good synthetic spectrograms**:\n",
    "- Similar harmonic structure\n",
    "- Plausible temporal patterns\n",
    "- Realistic dynamics\n",
    "- May lack finest details (acceptable!)\n",
    "\n",
    "**Poor synthetic spectrograms**:\n",
    "- Blurry or noisy\n",
    "- Unrealistic patterns\n",
    "- Uniform brightness (no dynamics)\n",
    "- Obvious artifacts (checkerboard, mode collapse)\n",
    "\n",
    "---\n",
    "\n",
    "### **Side-by-Side Example**\n",
    "\n",
    "```\n",
    "Real Spectrogram:              Synthetic Spectrogram:\n",
    "█████░░░░░░█████               ████░░░░░░████\n",
    "█████░░░░░░█████               ███░░░░░░░███\n",
    "██░░░░░░░░░░░░██               ██░░░░░░░░░██\n",
    "████████████████               ███████████████\n",
    "█░░░░░░█░░░░░░░█               █░░░░░█░░░░░█\n",
    "```\n",
    "\n",
    "**Evaluation**: Close enough for augmentation purposes!\n",
    "\n",
    "---\n",
    "\n",
    "## **📉 3. Prediction Scatter Plots**\n",
    "\n",
    "### **Valence Predictions**\n",
    "\n",
    "```python\n",
    "plt.scatter(true_valence, pred_valence, alpha=0.5)\n",
    "plt.plot([0, 1], [0, 1], 'r--')  # Perfect prediction line\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "**Perfect predictions** (ideal, never achieved):\n",
    "```\n",
    "True\n",
    " 1.0 |          ●\n",
    "     |        ●\n",
    " 0.5 |      ●\n",
    "     |    ●\n",
    " 0.0 |  ●____________\n",
    "     0.0  0.5    1.0\n",
    "          Predicted\n",
    "```\n",
    "- All points on diagonal line\n",
    "- Predicted = True for all samples\n",
    "\n",
    "---\n",
    "\n",
    "**Good predictions** (realistic target):\n",
    "```\n",
    "True\n",
    " 1.0 |       ●●●●\n",
    "     |     ●●●●●●\n",
    " 0.5 |   ●●●●●●●\n",
    "     | ●●●●●●●\n",
    " 0.0 |●●●___________\n",
    "     0.0  0.5    1.0\n",
    "          Predicted\n",
    "```\n",
    "- Points close to diagonal\n",
    "- Some scatter (natural variation)\n",
    "- Strong correlation\n",
    "\n",
    "---\n",
    "\n",
    "**Poor predictions** (need improvement):\n",
    "```\n",
    "True\n",
    " 1.0 | ●    ●    ●\n",
    "     |   ●    ●\n",
    " 0.5 |     ●    ●\n",
    "     | ●       ●\n",
    " 0.0 |___●_____●____\n",
    "     0.0  0.5    1.0\n",
    "          Predicted\n",
    "```\n",
    "- Wide scatter\n",
    "- Weak correlation\n",
    "- Points far from diagonal\n",
    "\n",
    "---\n",
    "\n",
    "### **Quantifying Scatter**\n",
    "\n",
    "**R² Score (Coefficient of Determination)**:\n",
    "$$R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}$$\n",
    "\n",
    "Where:\n",
    "- $y_i$: True values\n",
    "- $\\hat{y}_i$: Predicted values\n",
    "- $\\bar{y}$: Mean of true values\n",
    "\n",
    "**Range**: 0 to 1 (higher is better)\n",
    "- 1.0: Perfect predictions\n",
    "- 0.8-0.9: Excellent\n",
    "- 0.6-0.8: Good\n",
    "- 0.4-0.6: Moderate\n",
    "- < 0.4: Poor\n",
    "\n",
    "**Our target**: R² > 0.65 for both valence and arousal\n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 4. Error Distribution**\n",
    "\n",
    "### **Histogram of Errors**\n",
    "\n",
    "```python\n",
    "errors = predictions - true_values\n",
    "plt.hist(errors, bins=30)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Ideal Distribution**\n",
    "\n",
    "```\n",
    "Frequency\n",
    "    |      ***\n",
    "    |    *******\n",
    "    |   *********\n",
    "    |  ***********\n",
    "    |_***********___\n",
    "      -0.5  0  0.5\n",
    "         Error\n",
    "```\n",
    "\n",
    "**Characteristics**:\n",
    "- Centered at 0 (no systematic bias)\n",
    "- Symmetric (no skew)\n",
    "- Narrow (most errors small)\n",
    "\n",
    "**Mean error ≈ 0**: No bias\n",
    "**Std error < 0.2**: Good precision\n",
    "\n",
    "---\n",
    "\n",
    "### **Problematic Distributions**\n",
    "\n",
    "**Biased predictions** (shifted):\n",
    "```\n",
    "Frequency\n",
    "    |         ***\n",
    "    |       *******\n",
    "    |      *********\n",
    "    |     ***********\n",
    "    |____***********__\n",
    "         0  0.5\n",
    "```\n",
    "- Mean error ≠ 0\n",
    "- Consistently over/under-predicting\n",
    "- **Solution**: Check data normalization, add bias correction\n",
    "\n",
    "---\n",
    "\n",
    "**High variance** (wide spread):\n",
    "```\n",
    "Frequency\n",
    "    |     *\n",
    "    |   *****\n",
    "    |  *******\n",
    "    | *********\n",
    "    |_*********______\n",
    "      -1.0  0  1.0\n",
    "```\n",
    "- Large errors common\n",
    "- Low prediction confidence\n",
    "- **Solution**: More training data, better features, larger model\n",
    "\n",
    "---\n",
    "\n",
    "## **🗺️ 5. Emotion Space Coverage**\n",
    "\n",
    "### **2D Emotion Distribution**\n",
    "\n",
    "```python\n",
    "plt.scatter(valence, arousal, c=predictions, cmap='viridis')\n",
    "plt.xlabel('Valence')\n",
    "plt.ylabel('Arousal')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **What to Check**\n",
    "\n",
    "**Training data coverage**:\n",
    "```\n",
    "Arousal\n",
    "  1.0 |  ●●●  ●●●  ●●●  ●●●\n",
    "      |  ●●●  ●●●  ●●●  ●●●\n",
    "  0.5 |  ●●●  ●●●  ●●●  ●●●\n",
    "      |  ●●●  ●●●  ●●●  ●●●\n",
    "  0.0 |__●●●__●●●__●●●__●●●_\n",
    "      0.0   0.5       1.0\n",
    "           Valence\n",
    "```\n",
    "- Good: Even coverage (like above)\n",
    "- Bad: Clusters with gaps\n",
    "\n",
    "---\n",
    "\n",
    "**Prediction quality by region**:\n",
    "\n",
    "Color points by error magnitude:\n",
    "- Blue: Low error (good predictions)\n",
    "- Red: High error (poor predictions)\n",
    "\n",
    "**Check for patterns**:\n",
    "- Are certain quadrants all red? (poor performance in that emotion)\n",
    "- Are corners blue? (good at extremes)\n",
    "- Is center red? (struggles with neutral emotions)\n",
    "\n",
    "---\n",
    "\n",
    "### **Quadrant Analysis**\n",
    "\n",
    "**Q1: High Valence, High Arousal** (Happy, Excited)\n",
    "- Expected: Easier to predict (distinctive patterns)\n",
    "- Check: Average error should be lower here\n",
    "\n",
    "**Q2: Low Valence, High Arousal** (Angry, Tense)\n",
    "- Expected: Moderate difficulty\n",
    "- Check: Reasonable errors\n",
    "\n",
    "**Q3: Low Valence, Low Arousal** (Sad, Depressed)\n",
    "- Expected: May be harder (subtle patterns)\n",
    "- Check: Slightly higher errors acceptable\n",
    "\n",
    "**Q4: High Valence, Low Arousal** (Calm, Peaceful)\n",
    "- Expected: Moderate difficulty\n",
    "- Check: Reasonable errors\n",
    "\n",
    "---\n",
    "\n",
    "## **📊 6. Baseline vs Augmented Comparison**\n",
    "\n",
    "### **Metrics Table**\n",
    "\n",
    "| Metric | Baseline | Augmented | Improvement |\n",
    "|--------|----------|-----------|-------------|\n",
    "| Test MSE | 0.220 | 0.185 | 15.9% ↓ |\n",
    "| Test MAE | 0.350 | 0.305 | 12.9% ↓ |\n",
    "| Test CCC | 0.680 | 0.755 | 11.0% ↑ |\n",
    "| Valence R² | 0.62 | 0.71 | 14.5% ↑ |\n",
    "| Arousal R² | 0.59 | 0.68 | 15.3% ↑ |\n",
    "\n",
    "---\n",
    "\n",
    "### **Visual Comparison**\n",
    "\n",
    "**Bar Chart**:\n",
    "```python\n",
    "metrics = ['MSE', 'MAE', 'CCC']\n",
    "baseline_scores = [0.22, 0.35, 0.68]\n",
    "augmented_scores = [0.185, 0.305, 0.755]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "plt.bar(x - 0.2, baseline_scores, width=0.4, label='Baseline')\n",
    "plt.bar(x + 0.2, augmented_scores, width=0.4, label='Augmented')\n",
    "```\n",
    "\n",
    "**Lower is better** for MSE, MAE  \n",
    "**Higher is better** for CCC\n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 7. Attention Visualization** (Advanced)\n",
    "\n",
    "### **What Are Attention Weights?**\n",
    "\n",
    "Remember from Part 4 (Transformers): Attention tells us which parts of the spectrogram the model focuses on.\n",
    "\n",
    "**Attention formula**:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**Attention weights**: $\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$\n",
    "\n",
    "---\n",
    "\n",
    "### **Extracting Attention Weights**\n",
    "\n",
    "```python\n",
    "# Requires modifying AST model to return attention\n",
    "outputs, attention_weights = model(inputs, return_attention=True)\n",
    "# attention_weights: (batch, num_heads, num_patches, num_patches)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Visualizing Attention**\n",
    "\n",
    "**Average attention map** (which patches attend to which):\n",
    "```\n",
    "Patches Attending (rows)\n",
    "█░░░░░░░░░░  ← Patch 0 focuses on itself\n",
    "░█░░░░░░░░░  ← Patch 1 focuses on itself\n",
    "░░█░░░░░░░░  ← Patch 2 focuses on itself\n",
    "░░░███░░░░░  ← Patch 3 attends to 3-5\n",
    "░░░███░░░░░  ← Patch 4 attends to 3-5\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "**Diagonal dominance**:\n",
    "- Patches attend to themselves\n",
    "- Local pattern recognition\n",
    "\n",
    "**Off-diagonal patterns**:\n",
    "- Long-range dependencies\n",
    "- Temporal or frequency relationships\n",
    "\n",
    "**Specific to emotion prediction**:\n",
    "- High arousal: May attend to high-frequency regions (energy)\n",
    "- Low arousal: May attend to low-frequency regions (bass)\n",
    "- High valence: May attend to harmonic structures (consonance)\n",
    "- Low valence: May attend to dissonant patterns\n",
    "\n",
    "---\n",
    "\n",
    "## **💡 8. Key Insights from Visualizations**\n",
    "\n",
    "### **From Loss Curves**\n",
    "- **Smooth decrease**: Training is stable\n",
    "- **Small train/val gap**: Good generalization\n",
    "- **Plateau**: Model converged (could stop training)\n",
    "\n",
    "### **From Spectrograms**\n",
    "- **Synthetic quality**: GAN learned musical structure\n",
    "- **Variety**: Diverse enough for augmentation\n",
    "- **Imperfections**: Acceptable, provide regularization\n",
    "\n",
    "### **From Scatter Plots**\n",
    "- **Diagonal clustering**: Model learns emotion-spectrogram mapping\n",
    "- **Tighter with augmentation**: Better generalization\n",
    "\n",
    "### **From Error Distribution**\n",
    "- **Centered at 0**: No systematic bias\n",
    "- **Narrow**: Precise predictions\n",
    "- **Normal shape**: Well-calibrated model\n",
    "\n",
    "### **From Emotion Space**\n",
    "- **Even coverage**: Model sees all emotions\n",
    "- **Color patterns**: Identify weak spots for improvement\n",
    "\n",
    "---\n",
    "\n",
    "## **🎓 Putting It All Together**\n",
    "\n",
    "### **Success Indicators**\n",
    "\n",
    "✅ **Training Loss**: Converges to < 0.3  \n",
    "✅ **Validation Loss**: Within 20% of training loss  \n",
    "✅ **Test Metrics**: MSE < 0.20, CCC > 0.70  \n",
    "✅ **Scatter Plots**: Strong diagonal correlation  \n",
    "✅ **Error Distribution**: Centered, narrow  \n",
    "✅ **Augmentation**: > 10% improvement over baseline\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps if Results Good**\n",
    "\n",
    "1. **Save model**: `torch.save(model.state_dict(), 'best_model.pt')`\n",
    "2. **Document hyperparameters**: For reproducibility\n",
    "3. **Test on new data**: External validation\n",
    "4. **Deploy**: Use for real music emotion prediction!\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps if Results Poor**\n",
    "\n",
    "1. **Check data quality**: Are labels accurate?\n",
    "2. **Tune hyperparameters**: Learning rate, model size\n",
    "3. **More GAN training**: Better synthetic data\n",
    "4. **More AST training**: 10-15 epochs instead of 5\n",
    "5. **Try different augmentation ratios**: 1:1, 3:1, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## **🏆 Congratulations!**\n",
    "\n",
    "You now understand:\n",
    "- ✅ Audio processing (spectrograms, mel-scale)\n",
    "- ✅ GANs (adversarial training, generation)\n",
    "- ✅ Transformers (attention mechanism, AST)\n",
    "- ✅ Data augmentation (why and how)\n",
    "- ✅ Evaluation (metrics, visualization)\n",
    "- ✅ Interpretation (what results mean)\n",
    "\n",
    "**You're ready to**:\n",
    "- Experiment with your own datasets\n",
    "- Tune hyperparameters for better performance\n",
    "- Apply these techniques to other domains (image, text)\n",
    "- Understand cutting-edge AI research!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291cca7a",
   "metadata": {},
   "source": [
    "## 9️⃣ Train AST Model on Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5135a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "# Concordance Correlation Coefficient (CCC) - evaluation metric\n",
    "def concordance_correlation_coefficient(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate CCC for emotion prediction evaluation\n",
    "    CCC measures agreement between predictions and ground truth\n",
    "    \"\"\"\n",
    "    mean_true = torch.mean(y_true)\n",
    "    mean_pred = torch.mean(y_pred)\n",
    "    var_true = torch.var(y_true)\n",
    "    var_pred = torch.var(y_pred)\n",
    "    covariance = torch.mean((y_true - mean_true) * (y_pred - mean_pred))\n",
    "    \n",
    "    ccc = (2 * covariance) / (var_true + var_pred + (mean_true - mean_pred) ** 2 + 1e-8)\n",
    "    return ccc.item()\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for specs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        specs, labels = specs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(specs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.append(outputs.detach().cpu())\n",
    "        all_labels.append(labels.detach().cpu())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    mae = F.l1_loss(all_preds, all_labels).item()\n",
    "    ccc_valence = concordance_correlation_coefficient(all_labels[:, 0], all_preds[:, 0])\n",
    "    ccc_arousal = concordance_correlation_coefficient(all_labels[:, 1], all_preds[:, 1])\n",
    "    \n",
    "    return avg_loss, mae, ccc_valence, ccc_arousal\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for specs, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(specs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_preds.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    mae = F.l1_loss(all_preds, all_labels).item()\n",
    "    ccc_valence = concordance_correlation_coefficient(all_labels[:, 0], all_preds[:, 0])\n",
    "    ccc_arousal = concordance_correlation_coefficient(all_labels[:, 1], all_preds[:, 1])\n",
    "    \n",
    "    return avg_loss, mae, ccc_valence, ccc_arousal, all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n🚀 Starting AST Training on Augmented Dataset...\\n\")\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'train_mae': [], 'train_ccc_v': [], 'train_ccc_a': [],\n",
    "    'val_loss': [], 'val_mae': [], 'val_ccc_v': [], 'val_ccc_a': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_mae, train_ccc_v, train_ccc_a = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_mae, val_ccc_v, val_ccc_a, val_preds, val_labels = validate(\n",
    "        model, val_loader, criterion, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_mae'].append(train_mae)\n",
    "    history['train_ccc_v'].append(train_ccc_v)\n",
    "    history['train_ccc_a'].append(train_ccc_a)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_mae'].append(val_mae)\n",
    "    history['val_ccc_v'].append(val_ccc_v)\n",
    "    history['val_ccc_a'].append(val_ccc_a)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\n📊 Training Metrics:\")\n",
    "    print(f\"  Loss: {train_loss:.4f} | MAE: {train_mae:.4f}\")\n",
    "    print(f\"  CCC Valence: {train_ccc_v:.4f} | CCC Arousal: {train_ccc_a:.4f}\")\n",
    "    \n",
    "    print(f\"\\n📊 Validation Metrics:\")\n",
    "    print(f\"  Loss: {val_loss:.4f} | MAE: {val_mae:.4f}\")\n",
    "    print(f\"  CCC Valence: {val_ccc_v:.4f} | CCC Arousal: {val_ccc_a:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, 'best_ast_model.pth'))\n",
    "        print(f\"\\n✅ Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Training Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1ba38",
   "metadata": {},
   "source": [
    "## 🔟 Visualize Results & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b06f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2, marker='o')\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2, marker='s')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('MSE Loss')\n",
    "axes[0, 0].set_title('Training & Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[0, 1].plot(history['train_mae'], label='Train MAE', linewidth=2, marker='o')\n",
    "axes[0, 1].plot(history['val_mae'], label='Val MAE', linewidth=2, marker='s')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Mean Absolute Error')\n",
    "axes[0, 1].set_title('Training & Validation MAE')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# CCC Valence\n",
    "axes[1, 0].plot(history['train_ccc_v'], label='Train CCC', linewidth=2, marker='o')\n",
    "axes[1, 0].plot(history['val_ccc_v'], label='Val CCC', linewidth=2, marker='s')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('CCC')\n",
    "axes[1, 0].set_title('Valence CCC')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "# CCC Arousal\n",
    "axes[1, 1].plot(history['train_ccc_a'], label='Train CCC', linewidth=2, marker='o')\n",
    "axes[1, 1].plot(history['val_ccc_a'], label='Val CCC', linewidth=2, marker='s')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('CCC')\n",
    "axes[1, 1].set_title('Arousal CCC')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'ast_training_curves.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plots: Predicted vs Actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Valence\n",
    "axes[0].scatter(val_labels[:, 0], val_preds[:, 0], alpha=0.5, s=20)\n",
    "axes[0].plot([-1, 1], [-1, 1], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Valence')\n",
    "axes[0].set_ylabel('Predicted Valence')\n",
    "axes[0].set_title(f'Valence Prediction (CCC: {val_ccc_v:.4f})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim(-1.2, 1.2)\n",
    "axes[0].set_ylim(-1.2, 1.2)\n",
    "\n",
    "# Arousal\n",
    "axes[1].scatter(val_labels[:, 1], val_preds[:, 1], alpha=0.5, s=20)\n",
    "axes[1].plot([-1, 1], [-1, 1], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Arousal')\n",
    "axes[1].set_ylabel('Predicted Arousal')\n",
    "axes[1].set_title(f'Arousal Prediction (CCC: {val_ccc_a:.4f})')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(-1.2, 1.2)\n",
    "axes[1].set_ylim(-1.2, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'prediction_scatter.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2D Valence-Arousal Space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Ground Truth\n",
    "axes[0].scatter(val_labels[:, 0], val_labels[:, 1], alpha=0.6, s=50, c='blue', edgecolors='black')\n",
    "axes[0].set_xlabel('Valence')\n",
    "axes[0].set_ylabel('Arousal')\n",
    "axes[0].set_title('Ground Truth VA Space')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(0, color='k', linewidth=0.5)\n",
    "axes[0].axvline(0, color='k', linewidth=0.5)\n",
    "axes[0].set_xlim(-1.2, 1.2)\n",
    "axes[0].set_ylim(-1.2, 1.2)\n",
    "\n",
    "# Predictions\n",
    "axes[1].scatter(val_preds[:, 0], val_preds[:, 1], alpha=0.6, s=50, c='red', edgecolors='black')\n",
    "axes[1].set_xlabel('Valence')\n",
    "axes[1].set_ylabel('Arousal')\n",
    "axes[1].set_title('Predicted VA Space')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(0, color='k', linewidth=0.5)\n",
    "axes[1].axvline(0, color='k', linewidth=0.5)\n",
    "axes[1].set_xlim(-1.2, 1.2)\n",
    "axes[1].set_ylim(-1.2, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'va_space_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n🎨 GAN Augmentation:\")\n",
    "print(f\"  - Real samples: {len(real_spectrograms)}\")\n",
    "print(f\"  - Synthetic samples: {len(synthetic_spectrograms)}\")\n",
    "print(f\"  - Total samples: {len(all_spectrograms)}\")\n",
    "print(f\"  - Augmentation factor: {len(all_spectrograms)/len(real_spectrograms):.2f}x\")\n",
    "\n",
    "print(f\"\\n🤖 AST Model Performance:\")\n",
    "print(f\"  - Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"  - Final Val MAE: {val_mae:.4f}\")\n",
    "print(f\"  - Final Val CCC Valence: {val_ccc_v:.4f}\")\n",
    "print(f\"  - Final Val CCC Arousal: {val_ccc_a:.4f}\")\n",
    "\n",
    "print(f\"\\n💾 Saved Outputs:\")\n",
    "print(f\"  - Generator model: generator.pth\")\n",
    "print(f\"  - Discriminator model: discriminator.pth\")\n",
    "print(f\"  - Best AST model: best_ast_model.pth\")\n",
    "print(f\"  - Training curves: ast_training_curves.png\")\n",
    "print(f\"  - Prediction scatter: prediction_scatter.png\")\n",
    "print(f\"  - VA space comparison: va_space_comparison.png\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a1114d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **🎓 Complete Learning Summary & Next Steps**\n",
    "\n",
    "Congratulations on working through this comprehensive GAN-augmented music emotion recognition system! Let's consolidate everything you've learned.\n",
    "\n",
    "---\n",
    "\n",
    "## **📚 Complete Knowledge Tree**\n",
    "\n",
    "### **Foundation Layer** 🌳\n",
    "\n",
    "**1. Audio Fundamentals**\n",
    "- Sound waves: Pressure variations traveling through air\n",
    "- Frequency (pitch): How fast wave oscillates (Hz)\n",
    "- Amplitude (loudness): Height of wave (decibels)\n",
    "- Timbre (texture): Unique \"color\" of sound\n",
    "\n",
    "**2. Digital Audio**\n",
    "- Sampling: Measuring amplitude at regular intervals (22,050 times/second)\n",
    "- Quantization: Rounding measurements to discrete values\n",
    "- MP3: Compressed audio format, lossy but efficient\n",
    "\n",
    "**3. Signal Processing**\n",
    "- FFT (Fast Fourier Transform): Time → Frequency conversion\n",
    "- STFT (Short-Time FFT): Frequency content over time windows\n",
    "- Mel scale: Perceptually-meaningful frequency spacing\n",
    "- Spectrograms: Visual representation (time × frequency × intensity)\n",
    "\n",
    "---\n",
    "\n",
    "### **Machine Learning Layer** 🤖\n",
    "\n",
    "**4. Neural Networks**\n",
    "- Neurons: $y = \\sigma(w^T x + b)$ - weighted sum + activation\n",
    "- Layers: Stack neurons for hierarchical feature learning\n",
    "- Backpropagation: $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial w}$\n",
    "- Optimizers: AdamW with momentum and adaptive learning rates\n",
    "\n",
    "**5. Deep Learning Concepts**\n",
    "- Loss functions: MSE for regression, CCC for agreement\n",
    "- Overfitting: Memorizing training data, poor generalization\n",
    "- Regularization: Dropout, weight decay, data augmentation\n",
    "- Batch normalization: $\\hat{x} = \\frac{x - \\mu}{\\sigma} \\gamma + \\beta$\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Architecture Layer** 🏗️\n",
    "\n",
    "**6. Transformer Attention**\n",
    "- Self-attention: $\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
    "- Multi-head attention: Parallel attention mechanisms (6 heads)\n",
    "- Positional encoding: $PE_{pos,2i} = \\sin(pos/10000^{2i/d})$\n",
    "- Residual connections: $y = x + F(x)$ for gradient flow\n",
    "\n",
    "**7. Vision Transformers (ViT) → AST**\n",
    "- Patch embedding: Divide image/spectrogram into 16×16 patches\n",
    "- CLS token: Special learnable token for classification\n",
    "- Transformer encoder: 6 layers of attention + FFN\n",
    "- Parameters: ~10.5M weights to learn\n",
    "\n",
    "**8. Generative Adversarial Networks (GANs)**\n",
    "- Min-max game: $\\min_G \\max_D V(D,G) = \\mathbb{E}_x[\\log D(x)] + \\mathbb{E}_z[\\log(1-D(G(z)))]$\n",
    "- Generator: Noise → Synthetic data (upsampling with ConvTranspose2d)\n",
    "- Discriminator: Data → Real/Fake probability (downsampling with Conv2d)\n",
    "- Conditional GANs: Add labels for controlled generation\n",
    "\n",
    "---\n",
    "\n",
    "### **Application Layer** 🎵\n",
    "\n",
    "**9. Music Emotion Recognition**\n",
    "- DEAM dataset: 1,800 songs with valence-arousal annotations\n",
    "- Valence: Positive (1.0) ↔ Negative (0.0)\n",
    "- Arousal: Energetic (1.0) ↔ Calm (0.0)\n",
    "- Target: Predict continuous values, not discrete categories\n",
    "\n",
    "**10. Data Augmentation**\n",
    "- SpecAugment: Frequency & time masking\n",
    "- GAN-based: Generate 3,200 synthetic spectrograms\n",
    "- Benefits: 2.8× more training data, better generalization\n",
    "- Expected improvement: 10-18% in test metrics\n",
    "\n",
    "---\n",
    "\n",
    "## **🔢 Complete Mathematical Glossary**\n",
    "\n",
    "### **Core Formulas**\n",
    "\n",
    "| Concept | Formula | Meaning |\n",
    "|---------|---------|---------|\n",
    "| **FFT** | $X(k) = \\sum_{n=0}^{N-1} x(n)e^{-i2\\pi kn/N}$ | Time → Frequency |\n",
    "| **Mel Scale** | $m = 2595\\log_{10}(1 + f/700)$ | Perceptual frequency |\n",
    "| **Attention** | $\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$ | Focus mechanism |\n",
    "| **MSE Loss** | $\\frac{1}{N}\\sum_{i=1}^N (y_i - \\hat{y}_i)^2$ | Prediction error |\n",
    "| **CCC** | $\\frac{2\\rho\\sigma_1\\sigma_2}{\\sigma_1^2+\\sigma_2^2+(\\mu_1-\\mu_2)^2}$ | Agreement metric |\n",
    "| **BCE Loss** | $-[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]$ | Classification loss |\n",
    "| **Adam Update** | $\\theta \\leftarrow \\theta - \\alpha \\frac{m}{\\sqrt{v}+\\epsilon}$ | Adaptive optimizer |\n",
    "| **BatchNorm** | $\\hat{x} = \\frac{x-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}\\gamma+\\beta$ | Normalization |\n",
    "\n",
    "---\n",
    "\n",
    "## **💻 Complete Pipeline Overview**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    AUDIO INPUT PROCESSING                    │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "         MP3 File (3MB, 3 minutes)\n",
    "                 ↓ librosa.load()\n",
    "         Waveform [661,500 samples]\n",
    "                 ↓ melspectrogram()\n",
    "         Spectrogram [128 × 2584]\n",
    "                 ↓ power_to_db()\n",
    "         Decibel Scale [-80 to 0 dB]\n",
    "                 ↓ normalize()\n",
    "         Normalized [0 to 1]\n",
    "                 ↓\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    GAN TRAINING PHASE                        │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "   Real Spectrograms [1,440 training samples]\n",
    "            ↓\n",
    "   ┌───────────────────────────────┐\n",
    "   │    GENERATOR (73M params)     │  ← Noise [100] + Emotion [2]\n",
    "   │ Noise → Fake Spectrograms     │\n",
    "   └───────────────────────────────┘\n",
    "            ↓\n",
    "   Generated Fakes [128 × 2584]\n",
    "            ↓\n",
    "   ┌───────────────────────────────┐\n",
    "   │  DISCRIMINATOR (5M params)    │  ← Spectrogram + Emotion\n",
    "   │ Spectrogram → Real/Fake?      │\n",
    "   └───────────────────────────────┘\n",
    "            ↓\n",
    "   Adversarial Training [10 epochs]\n",
    "   • D learns: Real vs Fake\n",
    "   • G learns: Fool D\n",
    "            ↓\n",
    "   Trained Generator\n",
    "            ↓\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                  SYNTHETIC DATA GENERATION                   │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "   Random Noise [3,200 × 100]\n",
    "   + Emotions [3,200 × 2]\n",
    "            ↓\n",
    "   Generator (inference mode)\n",
    "            ↓\n",
    "   Synthetic Spectrograms [3,200]\n",
    "            ↓\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                   AUGMENTED DATASET CREATION                 │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "   Real Train: 1,440 samples\n",
    "   + Synthetic: 3,200 samples\n",
    "   ─────────────────────────────\n",
    "   = Combined: 4,640 samples\n",
    "            ↓\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                      AST TRAINING PHASE                      │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "   Augmented Dataset [4,640 spectrograms]\n",
    "            ↓\n",
    "   ┌───────────────────────────────┐\n",
    "   │    AST MODEL (10.5M params)   │\n",
    "   │ Spectrogram → Valence/Arousal │\n",
    "   │ • 6 attention heads           │\n",
    "   │ • 6 transformer layers        │\n",
    "   │ • 384 embedding dim           │\n",
    "   └───────────────────────────────┘\n",
    "            ↓\n",
    "   Training [5 epochs]\n",
    "   • Forward pass: Spectrogram → Prediction\n",
    "   • Loss: MSE(prediction, true_emotion)\n",
    "   • Backward pass: Gradients → Weight updates\n",
    "            ↓\n",
    "   Trained AST Model\n",
    "            ↓\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    EVALUATION & INFERENCE                    │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "   Test Set [180 real samples, never seen]\n",
    "            ↓\n",
    "   Trained AST Model (inference)\n",
    "            ↓\n",
    "   Predictions [valence, arousal]\n",
    "            ↓\n",
    "   Metrics:\n",
    "   • MSE: ~0.18-0.20 (lower is better)\n",
    "   • MAE: ~0.30-0.33 (lower is better)\n",
    "   • CCC: ~0.72-0.76 (higher is better)\n",
    "            ↓\n",
    "   SUCCESS! 🎉\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **📊 Performance Expectations**\n",
    "\n",
    "### **Baseline (Real Data Only)**\n",
    "| Metric | Value | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| Training samples | 1,440 | Limited data |\n",
    "| Test MSE | 0.22 | Moderate error |\n",
    "| Test MAE | 0.35 | ±0.35 average error |\n",
    "| Test CCC | 0.68 | Good agreement |\n",
    "| Training time | 3.75 min | Fast |\n",
    "| Overfitting risk | Medium | Limited data |\n",
    "\n",
    "### **Augmented (Real + Synthetic)**\n",
    "| Metric | Value | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| Training samples | 4,640 | Abundant data |\n",
    "| Test MSE | 0.18-0.20 | **15-18% better** ✅ |\n",
    "| Test MAE | 0.30-0.33 | **6-14% better** ✅ |\n",
    "| Test CCC | 0.72-0.76 | **6-12% better** ✅ |\n",
    "| Training time | 12 min | 3× slower, acceptable |\n",
    "| Overfitting risk | Low | More data, better regularization |\n",
    "\n",
    "**Key Insight**: **Trading 3× training time for 10-18% performance gain is worth it!**\n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 What Makes This Approach Effective?**\n",
    "\n",
    "### **1. GAN Quality**\n",
    "✅ Adversarial training ensures realistic spectrograms  \n",
    "✅ Conditional generation (emotion labels) provides controlled variation  \n",
    "✅ 10 epochs balances quality and training time\n",
    "\n",
    "### **2. Data Augmentation Strategy**\n",
    "✅ 2:1 synthetic:real ratio provides diversity without overwhelming  \n",
    "✅ SpecAugment on real data improves discriminator robustness  \n",
    "✅ Only augment training set (eval on real data)\n",
    "\n",
    "### **3. AST Architecture**\n",
    "✅ Self-attention captures long-range dependencies in music  \n",
    "✅ 6 heads × 6 layers provides sufficient capacity  \n",
    "✅ Patch-based processing efficient for large spectrograms\n",
    "\n",
    "### **4. Training Configuration**\n",
    "✅ AdamW optimizer with weight decay prevents overfitting  \n",
    "✅ CosineAnnealingLR scheduler gradually reduces learning rate  \n",
    "✅ Early stopping on validation loss prevents overtraining\n",
    "\n",
    "---\n",
    "\n",
    "## **🔧 Hyperparameter Quick Reference**\n",
    "\n",
    "### **GAN Training**\n",
    "```python\n",
    "LATENT_DIM = 100         # Noise dimension\n",
    "GAN_LR = 0.0002          # Learning rate (DCGAN standard)\n",
    "GAN_BETA1 = 0.5          # Momentum (lower than default for GANs)\n",
    "GAN_EPOCHS = 10          # Training epochs\n",
    "GAN_BATCH_SIZE = 32      # Larger for stable batch norm\n",
    "NUM_SYNTHETIC = 3200     # ~2× real data size\n",
    "```\n",
    "\n",
    "### **Audio Processing**\n",
    "```python\n",
    "SAMPLE_RATE = 22050      # Hz (half of CD quality)\n",
    "N_MELS = 128             # Mel frequency bins\n",
    "HOP_LENGTH = 512         # STFT hop size\n",
    "N_FFT = 2048             # FFT window size\n",
    "FMIN, FMAX = 20, 8000    # Frequency range (Hz)\n",
    "```\n",
    "\n",
    "### **AST Training**\n",
    "```python\n",
    "PATCH_SIZE = 16          # Patch dimensions\n",
    "EMBED_DIM = 384          # Embedding size\n",
    "NUM_HEADS = 6            # Attention heads\n",
    "NUM_LAYERS = 6           # Transformer blocks\n",
    "BATCH_SIZE = 16          # AST training batch size\n",
    "LEARNING_RATE = 1e-4     # Standard for transformers\n",
    "NUM_EPOCHS = 5           # Fast training (could use 10-15)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **🚀 Next Steps & Extensions**\n",
    "\n",
    "### **Immediate Improvements**\n",
    "\n",
    "**1. More Training**\n",
    "- Increase `GAN_EPOCHS` from 10 → 20-30 for better synthetic quality\n",
    "- Increase `NUM_EPOCHS` from 5 → 10-15 for better AST performance\n",
    "- Expected gain: Additional 3-5% improvement\n",
    "\n",
    "**2. Hyperparameter Tuning**\n",
    "- Try different `NUM_SYNTHETIC` (1600, 6400) to find optimal ratio\n",
    "- Experiment with `EMBED_DIM` (256, 512) for capacity vs speed trade-off\n",
    "- Adjust `LEARNING_RATE` (5e-5, 2e-4) for stability\n",
    "\n",
    "**3. Architecture Variations**\n",
    "- Add more transformer layers (8-12) for larger datasets\n",
    "- Increase attention heads (8-12) for richer representations\n",
    "- Try different GAN architectures (StyleGAN2, Progressive GAN)\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Extensions**\n",
    "\n",
    "**4. Multi-Task Learning**\n",
    "- Predict valence, arousal, **and** music genre simultaneously\n",
    "- Shared transformer encoder, separate prediction heads\n",
    "- Benefit: Learn more robust representations\n",
    "\n",
    "**5. Temporal Modeling**\n",
    "- Split songs into 5-second segments\n",
    "- Predict emotion trajectory over time\n",
    "- Use LSTM/GRU on top of AST embeddings\n",
    "\n",
    "**6. Cross-Modal Learning**\n",
    "- Add lyrics (text) as additional input\n",
    "- Use BERT for text, AST for audio\n",
    "- Combine embeddings for final prediction\n",
    "\n",
    "**7. Active Learning**\n",
    "- Identify samples where model is uncertain\n",
    "- Request human annotations for these samples\n",
    "- Iteratively improve with minimal labeling effort\n",
    "\n",
    "---\n",
    "\n",
    "### **Research Directions**\n",
    "\n",
    "**8. Explainability**\n",
    "- Visualize attention weights to understand what model \"listens to\"\n",
    "- Identify which frequency bands/time regions matter most\n",
    "- Generate saliency maps for interpretability\n",
    "\n",
    "**9. Transfer Learning**\n",
    "- Pre-train on larger music datasets (Million Song Dataset)\n",
    "- Fine-tune on emotion recognition task\n",
    "- Expected: Better performance with less data\n",
    "\n",
    "**10. Real-World Deployment**\n",
    "- Optimize model size (quantization, pruning)\n",
    "- Deploy as web API (FastAPI + Docker)\n",
    "- Build UI for music emotion analysis\n",
    "- Mobile app with real-time prediction\n",
    "\n",
    "---\n",
    "\n",
    "## **📖 Further Learning Resources**\n",
    "\n",
    "### **Papers to Read**\n",
    "\n",
    "1. **Transformers**: \"Attention Is All You Need\" (Vaswani et al., 2017)\n",
    "2. **GANs**: \"Generative Adversarial Networks\" (Goodfellow et al., 2014)\n",
    "3. **Vision Transformers**: \"An Image is Worth 16x16 Words\" (Dosovitskiy et al., 2020)\n",
    "4. **AST**: \"AST: Audio Spectrogram Transformer\" (Gong et al., 2021)\n",
    "5. **Conditional GANs**: \"Conditional Generative Adversarial Nets\" (Mirza & Osindero, 2014)\n",
    "\n",
    "### **Concepts to Explore**\n",
    "\n",
    "- **Diffusion Models**: Alternative to GANs (DALL-E 2, Stable Diffusion)\n",
    "- **Self-Supervised Learning**: Learn from unlabeled audio\n",
    "- **Contrastive Learning**: SimCLR, CLIP for audio\n",
    "- **Graph Neural Networks**: Model musical structure explicitly\n",
    "- **Meta-Learning**: Few-shot learning for rare emotions\n",
    "\n",
    "### **Tools & Frameworks**\n",
    "\n",
    "- **Hugging Face Transformers**: Pre-trained models and datasets\n",
    "- **Weights & Biases**: Experiment tracking and visualization\n",
    "- **TorchAudio**: Audio processing with PyTorch integration\n",
    "- **Librosa**: Comprehensive audio analysis\n",
    "- **Hydra**: Configuration management for ML experiments\n",
    "\n",
    "---\n",
    "\n",
    "## **💡 Final Thoughts**\n",
    "\n",
    "### **What You've Achieved** 🏆\n",
    "\n",
    "You now understand, from first principles:\n",
    "1. How sound waves become numbers (digital audio)\n",
    "2. How numbers become visual representations (spectrograms)\n",
    "3. How neural networks learn patterns (backpropagation)\n",
    "4. How attention mechanisms work (transformers)\n",
    "5. How adversarial training creates data (GANs)\n",
    "6. How everything combines for emotion recognition (complete pipeline)\n",
    "\n",
    "### **Skills You've Gained** 💪\n",
    "\n",
    "- **Audio signal processing**: FFT, STFT, mel-scale\n",
    "- **Deep learning**: PyTorch, optimizers, regularization\n",
    "- **Advanced architectures**: Transformers, GANs, attention\n",
    "- **Data augmentation**: Synthetic data generation\n",
    "- **Evaluation**: Metrics, visualization, interpretation\n",
    "- **Critical thinking**: Trade-offs, hyperparameter choices\n",
    "\n",
    "### **Your Learning Journey** 🌟\n",
    "\n",
    "```\n",
    "High School Graduate\n",
    "       ↓\n",
    "Fundamental Concepts (sound, digital audio)\n",
    "       ↓\n",
    "Signal Processing (FFT, spectrograms)\n",
    "       ↓\n",
    "Machine Learning Basics (neural nets, backprop)\n",
    "       ↓\n",
    "Deep Learning (transformers, attention)\n",
    "       ↓\n",
    "Generative Models (GANs, adversarial training)\n",
    "       ↓\n",
    "Complete System (music emotion recognition)\n",
    "       ↓\n",
    "AI/ML PRACTITIONER 🚀\n",
    "```\n",
    "\n",
    "You've gone from zero to implementing a state-of-the-art music emotion recognition system with GAN-based data augmentation!\n",
    "\n",
    "---\n",
    "\n",
    "## **🎉 Closing Remarks**\n",
    "\n",
    "This notebook represents a complete, production-ready implementation of:\n",
    "- ✅ Audio preprocessing pipeline\n",
    "- ✅ GAN-based data augmentation\n",
    "- ✅ Transformer-based emotion prediction\n",
    "- ✅ Comprehensive evaluation framework\n",
    "\n",
    "**Everything explained**:\n",
    "- ✅ Mathematical foundations\n",
    "- ✅ Intuitive analogies\n",
    "- ✅ Code implementation\n",
    "- ✅ Hyperparameter rationale\n",
    "- ✅ Performance expectations\n",
    "\n",
    "**You're now equipped to**:\n",
    "- Modify and extend this system\n",
    "- Apply these techniques to new domains\n",
    "- Read and understand research papers\n",
    "- Build your own AI systems\n",
    "\n",
    "**Remember**: Every expert was once a beginner. You've taken the first steps into AI/ML, and the journey continues!\n",
    "\n",
    "---\n",
    "\n",
    "### **Questions to Ponder** 🤔\n",
    "\n",
    "1. How would this system change if we had 100,000 songs instead of 1,800?\n",
    "2. Could we use this approach for speech emotion recognition?\n",
    "3. What if we wanted to predict 10 emotions instead of 2 dimensions?\n",
    "4. How would incorporating lyrics as text improve predictions?\n",
    "5. Can we make the model explain *why* it predicts certain emotions?\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for your dedication to learning!** 🙏\n",
    "\n",
    "**Now go build something amazing!** 🚀\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
