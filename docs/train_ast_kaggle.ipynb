{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40ed2db",
   "metadata": {},
   "source": [
    "# Audio Spectrogram Transformer (AST) for Music Emotion Recognition\n",
    "\n",
    "## Overview\n",
    "This notebook trains an Audio Spectrogram Transformer (AST) model to predict **valence** and **arousal** from music using the **DEAM (Database for Emotional Analysis of Music)** dataset.\n",
    "\n",
    "### What is AST?\n",
    "Audio Spectrogram Transformer is a Vision Transformer (ViT) architecture adapted for audio classification. It:\n",
    "- Converts audio to mel-spectrograms\n",
    "- Splits spectrograms into patches\n",
    "- Uses self-attention to learn temporal and spectral patterns\n",
    "- Predicts emotional dimensions (valence and arousal)\n",
    "\n",
    "### Dataset Requirements\n",
    "This notebook expects the DEAM dataset to be available as a Kaggle dataset. \n",
    "**Dataset structure:**\n",
    "```\n",
    "/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/\n",
    "â”œâ”€â”€ DEAM_audio/\n",
    "â”‚   â””â”€â”€ MEMD_audio/  (contains .mp3 files)\n",
    "â””â”€â”€ DEAM_Annotations/\n",
    "    â””â”€â”€ annotations/\n",
    "        â””â”€â”€ annotations averaged per song/\n",
    "            â””â”€â”€ song_level/\n",
    "                â”œâ”€â”€ static_annotations_averaged_songs_1_2000.csv\n",
    "                â””â”€â”€ static_annotations_averaged_songs_2000_2058.csv\n",
    "```\n",
    "\n",
    "### Training Configuration\n",
    "- **Model**: Vision Transformer adapted for audio\n",
    "- **Input**: Mel-spectrograms (128 mel bins Ã— 432 time steps)\n",
    "- **Output**: Valence and Arousal predictions (1-9 scale)\n",
    "- **Loss**: Mean Squared Error (MSE)\n",
    "- **Metrics**: MSE, MAE, Concordance Correlation Coefficient (CCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d169aa7",
   "metadata": {},
   "source": [
    "# ğŸ“ Complete Educational Guide: Audio Spectrogram Transformer (AST)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š **Table of Contents**\n",
    "1. [Fundamental Concepts](#fundamentals)\n",
    "2. [Audio Processing Basics](#audio)\n",
    "3. [Machine Learning Foundations](#ml)\n",
    "4. [Transformer Architecture](#transformers)\n",
    "5. [Training Process](#training)\n",
    "6. [Evaluation Metrics](#metrics)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒŸ **What You'll Learn**\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- How computers \"see\" and understand music\n",
    "- What spectrograms are and why they matter\n",
    "- How attention mechanisms work (the key to modern AI)\n",
    "- How transformers process sequential data\n",
    "- The mathematics behind emotion prediction\n",
    "- How to train and evaluate deep learning models\n",
    "\n",
    "**No Prerequisites Required!** We explain everything from first principles.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3e5fb1",
   "metadata": {},
   "source": [
    "<a name=\"fundamentals\"></a>\n",
    "## ğŸ¯ **Part 1: Fundamental Concepts**\n",
    "\n",
    "### **What is Sound?**\n",
    "\n",
    "**Simple Analogy**: Imagine throwing a stone into a pond. You see ripples spreading outwardâ€”these are **waves**. Sound works the same way, but instead of water, it's air molecules vibrating.\n",
    "\n",
    "**Technical Definition**:\n",
    "Sound is a **pressure wave** that travels through air (or any medium). When you pluck a guitar string:\n",
    "1. The string vibrates back and forth\n",
    "2. It pushes air molecules, creating regions of high pressure (compression) and low pressure (rarefaction)\n",
    "3. These pressure changes travel through the air as a wave\n",
    "4. Your ear detects these pressure changes as sound\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Sound Properties**\n",
    "\n",
    "#### 1. **Frequency (Pitch)** ğŸµ\n",
    "\n",
    "**What it is**: How many times per second the wave completes one full cycle.\n",
    "- **Unit**: Hertz (Hz) = cycles per second\n",
    "- **Example**: \n",
    "  - Middle A on a piano = 440 Hz (vibrates 440 times/second)\n",
    "  - Human hearing range: ~20 Hz to 20,000 Hz\n",
    "\n",
    "**Analogy**: Think of a swing in a playground:\n",
    "- **Fast swinging** (high frequency) = high pitch (like a whistle)\n",
    "- **Slow swinging** (low frequency) = low pitch (like a bass drum)\n",
    "\n",
    "**Mathematical Representation**:\n",
    "A pure tone (sine wave) at frequency *f*:\n",
    "```\n",
    "y(t) = A Ã— sin(2Ï€ft)\n",
    "```\n",
    "where:\n",
    "- **A** = amplitude (volume/loudness)\n",
    "- **f** = frequency (pitch)\n",
    "- **t** = time\n",
    "- **2Ï€** = one complete circle (360 degrees in radians)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Amplitude (Loudness)** ğŸ”Š\n",
    "\n",
    "**What it is**: The \"height\" of the waveâ€”how much the air pressure changes.\n",
    "- **High amplitude** = loud sound (strong pressure changes)\n",
    "- **Low amplitude** = quiet sound (weak pressure changes)\n",
    "\n",
    "**Analogy**: Ocean waves:\n",
    "- **Tsunami** (huge wave) = very loud sound\n",
    "- **Ripples** (tiny waves) = quiet sound\n",
    "\n",
    "**Measurement**: Often measured in **decibels (dB)**:\n",
    "- 0 dB = threshold of hearing (barely audible)\n",
    "- 60 dB = normal conversation\n",
    "- 120 dB = rock concert (painful!)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Timbre (Tone Quality)** ğŸ¸\n",
    "\n",
    "**What it is**: Why a piano sounds different from a guitar even when playing the same note.\n",
    "\n",
    "**The Secret**: Real-world sounds are NOT pure sine waves. They're **complex combinations** of multiple frequencies:\n",
    "- **Fundamental frequency**: The main pitch you hear\n",
    "- **Overtones/Harmonics**: Additional frequencies that give character\n",
    "\n",
    "**Example**: When you play middle C on a piano:\n",
    "- **Fundamental**: 261.6 Hz (the note you identify)\n",
    "- **2nd harmonic**: 523.2 Hz (2Ã— fundamental)\n",
    "- **3rd harmonic**: 784.8 Hz (3Ã— fundamental)\n",
    "- ...and many more!\n",
    "\n",
    "The **mix** of these harmonics creates the piano's unique sound.\n",
    "\n",
    "**Mathematical Representation** (Fourier Series):\n",
    "Any complex sound can be decomposed into a sum of simple sine waves:\n",
    "```\n",
    "y(t) = Aâ‚sin(2Ï€fâ‚t) + Aâ‚‚sin(2Ï€fâ‚‚t) + Aâ‚ƒsin(2Ï€fâ‚ƒt) + ...\n",
    "```\n",
    "\n",
    "This is why we need **spectrograms**â€”to see all these frequencies at once!\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Computers Need Special Representation**\n",
    "\n",
    "**The Problem**: \n",
    "- Sound waves are continuous (infinite points in time)\n",
    "- Computers work with discrete numbers (finite data)\n",
    "\n",
    "**The Solution**: **Digital Audio**\n",
    "\n",
    "1. **Sampling**: Measure the wave's amplitude at regular intervals\n",
    "   - **Sample Rate**: How many measurements per second\n",
    "   - CD quality: 44,100 Hz (44,100 samples/second)\n",
    "   - Why 44.1k? Nyquist theorem says you need 2Ã— the highest frequency you want to capture\n",
    "   - Humans hear up to ~20kHz â†’ need 40kHz sample rate (44.1 gives buffer)\n",
    "\n",
    "2. **Quantization**: Convert each measurement to a number\n",
    "   - **Bit Depth**: How precise each measurement is\n",
    "   - 16-bit = 65,536 possible values (CD quality)\n",
    "   - 24-bit = 16.7 million possible values (professional audio)\n",
    "\n",
    "**Analogy**: It's like taking photographs of a moving car:\n",
    "- **Frame rate** (sample rate) = how many photos per second\n",
    "- **Camera resolution** (bit depth) = detail in each photo\n",
    "- Fast frame rate + high resolution = smooth, detailed \"recording\" of motion\n",
    "\n",
    "---\n",
    "\n",
    "### **From Sound Waves to Spectrograms** ğŸ“Š\n",
    "\n",
    "**The Journey**:\n",
    "```\n",
    "Raw Audio Wave â†’ Digital Samples â†’ Frequency Analysis â†’ Spectrogram â†’ AI Model â†’ Emotion Prediction\n",
    "```\n",
    "\n",
    "**Why Not Use Raw Audio Directly?**\n",
    "\n",
    "1. **Too Much Information**: 5 seconds of audio at 44.1kHz = 220,500 numbers!\n",
    "2. **Wrong Representation**: AI needs to \"see\" patterns. A wave is hard to interpret.\n",
    "3. **Frequency is Key**: Musical emotion comes from harmony, melody, rhythmâ€”all frequency-based!\n",
    "\n",
    "**The Solution: Spectrograms**\n",
    "\n",
    "Think of a spectrogram as \"sheet music for computers\":\n",
    "- **X-axis**: Time (when things happen)\n",
    "- **Y-axis**: Frequency (what notes are playing)\n",
    "- **Color/Brightness**: Amplitude (how loud each frequency is)\n",
    "\n",
    "**Analogy**: Imagine watching a piano performance from above:\n",
    "- You see which keys are pressed (frequency)\n",
    "- You see when they're pressed (time)\n",
    "- You see how hard they're pressed (amplitude/color intensity)\n",
    "\n",
    "This 2D image contains ALL the musical information!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c3cf1b",
   "metadata": {},
   "source": [
    "<a name=\"audio\"></a>\n",
    "## ğŸµ **Part 2: Audio Processing Deep Dive**\n",
    "\n",
    "### **The Fourier Transform: From Time to Frequency** ğŸ”¬\n",
    "\n",
    "**The Big Idea**: Any complex sound can be broken down into simple sine waves of different frequencies.\n",
    "\n",
    "**Analogy**: Imagine a smoothie:\n",
    "- **Time domain** (raw audio) = the mixed smoothie (you see the final blend)\n",
    "- **Frequency domain** (after Fourier transform) = individual ingredients (strawberries, bananas, yogurt)\n",
    "\n",
    "**The Mathematics**:\n",
    "\n",
    "The **Fourier Transform** converts time-domain signal x(t) to frequency-domain X(f):\n",
    "\n",
    "```\n",
    "X(f) = âˆ«_{-âˆ}^{âˆ} x(t) Ã— e^(-i2Ï€ft) dt\n",
    "```\n",
    "\n",
    "**What this means**:\n",
    "- **x(t)**: Your audio signal over time\n",
    "- **X(f)**: How much of each frequency *f* is present\n",
    "- **e^(-i2Ï€ft)**: A complex exponential (represents rotationâ€”relates to sine/cosine)\n",
    "\n",
    "**For computers (Discrete Fourier Transform - DFT)**:\n",
    "```\n",
    "X[k] = Î£_{n=0}^{N-1} x[n] Ã— e^(-i2Ï€kn/N)\n",
    "```\n",
    "where:\n",
    "- **n**: Sample index (time)\n",
    "- **k**: Frequency bin\n",
    "- **N**: Total number of samples\n",
    "\n",
    "**In practice**: We use **Fast Fourier Transform (FFT)**â€”a clever algorithm that computes this in O(N log N) time instead of O(NÂ²)!\n",
    "\n",
    "---\n",
    "\n",
    "### **The Short-Time Fourier Transform (STFT)** â±ï¸\n",
    "\n",
    "**The Problem**: Music changes over time! A single Fourier Transform tells you what frequencies exist, but not **when** they occur.\n",
    "\n",
    "**The Solution**: STFT = \"Take many small Fourier Transforms\"\n",
    "\n",
    "**How it Works**:\n",
    "1. **Window**: Take a small chunk of audio (e.g., 25 milliseconds)\n",
    "2. **Transform**: Apply FFT to that chunk\n",
    "3. **Slide**: Move forward slightly (e.g., 10ms) and repeat\n",
    "4. **Stack**: Arrange all results side-by-side â†’ creates a spectrogram!\n",
    "\n",
    "**Mathematical Definition**:\n",
    "```\n",
    "STFT{x[n]}(m, Ï‰) = Î£_{n=-âˆ}^{âˆ} x[n] Ã— w[n - m] Ã— e^(-iÏ‰n)\n",
    "```\n",
    "where:\n",
    "- **m**: Time frame index\n",
    "- **Ï‰**: Angular frequency\n",
    "- **w[n]**: Window function (usually Hann or Hamming window)\n",
    "\n",
    "**Parameters in This Notebook**:\n",
    "- **n_fft = 2048**: FFT window size (46ms at 44.1kHz)\n",
    "  - Determines frequency resolution\n",
    "  - Larger = better frequency detail, worse time detail\n",
    "- **hop_length = 512**: How much we slide the window (11.6ms)\n",
    "  - Smaller = more time detail (but more computation)\n",
    "  - Relation: `time_steps = total_samples / hop_length`\n",
    "\n",
    "---\n",
    "\n",
    "### **Mel-Spectrograms: Matching Human Hearing** ğŸ‘‚\n",
    "\n",
    "**The Problem**: Humans don't hear frequencies linearly!\n",
    "- We're very sensitive to differences between 100 Hz and 200 Hz\n",
    "- We barely notice differences between 10,000 Hz and 10,100 Hz\n",
    "\n",
    "**The Solution**: **Mel Scale** (from \"melody\")\n",
    "\n",
    "**The Mel Scale Formula**:\n",
    "```\n",
    "mel(f) = 2595 Ã— logâ‚â‚€(1 + f/700)\n",
    "```\n",
    "\n",
    "**Inverse (Mel to Hz)**:\n",
    "```\n",
    "f(mel) = 700 Ã— (10^(mel/2595) - 1)\n",
    "```\n",
    "\n",
    "**What This Does**:\n",
    "- Low frequencies: Spread out more (more detail)\n",
    "- High frequencies: Compressed together (less detail)\n",
    "- Matches how the human cochlea (inner ear) processes sound!\n",
    "\n",
    "**Example**:\n",
    "- 100 Hz â†’ 150 mel\n",
    "- 200 Hz â†’ 283 mel (133 mel difference)\n",
    "- 10,000 Hz â†’ 3908 mel\n",
    "- 10,100 Hz â†’ 3914 mel (only 6 mel difference!)\n",
    "\n",
    "**Creating Mel-Spectrograms**:\n",
    "\n",
    "1. **Compute STFT**: Get frequency spectrum over time\n",
    "2. **Power Spectrum**: Square the magnitude\n",
    "   ```\n",
    "   P[k, t] = |X[k, t]|Â²\n",
    "   ```\n",
    "3. **Mel Filter Banks**: Apply triangular filters spaced on the Mel scale\n",
    "   ```\n",
    "   M[m, t] = Î£_{k} H_m[k] Ã— P[k, t]\n",
    "   ```\n",
    "   where H_m[k] are mel-spaced triangular filters\n",
    "\n",
    "4. **Log Compression**: Convert to decibels\n",
    "   ```\n",
    "   M_dB[m, t] = 10 Ã— logâ‚â‚€(M[m, t])\n",
    "   ```\n",
    "   Why log? Human perception of loudness is logarithmic!\n",
    "\n",
    "**In This Notebook**:\n",
    "- **n_mels = 128**: We create 128 mel-frequency bins\n",
    "- Covers frequency range from **fmin = 20 Hz** to **fmax = 8000 Hz**\n",
    "- Each bin captures energy in a specific frequency range\n",
    "\n",
    "---\n",
    "\n",
    "### **Normalization: Making Data ML-Friendly** ğŸ“\n",
    "\n",
    "**Why Normalize?**\n",
    "\n",
    "1. **Different scales**: Some frequencies naturally louder than others\n",
    "2. **Training stability**: Neural networks work best with data centered around 0\n",
    "3. **Faster convergence**: Gradients flow better through normalized data\n",
    "\n",
    "**Decibel (dB) Conversion**:\n",
    "```\n",
    "mel_spec_dB = 10 Ã— logâ‚â‚€(mel_spec)\n",
    "```\n",
    "or with librosa:\n",
    "```python\n",
    "mel_spec_dB = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "```\n",
    "\n",
    "**What `ref=np.max` means**:\n",
    "- Scale relative to the maximum value in the spectrogram\n",
    "- 0 dB = loudest point\n",
    "- Negative values = quieter than maximum\n",
    "\n",
    "**Z-Score Normalization** (used in training):\n",
    "```\n",
    "x_normalized = (x - mean) / std\n",
    "```\n",
    "where:\n",
    "- **mean**: Average value across all spectrograms\n",
    "- **std**: Standard deviation (measures spread)\n",
    "\n",
    "**Result**: Data centered at 0 with standard deviation of 1\n",
    "\n",
    "**Analogy**: Converting temperatures to a standard scale:\n",
    "- Raw: Some in Celsius, some in Fahrenheit (hard to compare)\n",
    "- Normalized: All in the same scale with 0 as average (easy to compare)\n",
    "\n",
    "---\n",
    "\n",
    "### **SpecAugment: Teaching with Incomplete Information** ğŸ­\n",
    "\n",
    "**The Concept**: Data augmentation = artificial data generation to prevent overfitting.\n",
    "\n",
    "**SpecAugment** applies two types of masking:\n",
    "\n",
    "#### **1. Frequency Masking** ğŸšï¸\n",
    "```python\n",
    "# Hide random frequency bands\n",
    "f = random(0, freq_mask_param)  # e.g., 0-10 bins\n",
    "f0 = random(0, n_mels - f)      # starting position\n",
    "spectrogram[f0:f0+f, :] = 0     # set to zero\n",
    "```\n",
    "\n",
    "**Analogy**: Covering a random row of keys on a pianoâ€”the model learns to recognize music even without certain notes.\n",
    "\n",
    "#### **2. Time Masking** â°\n",
    "```python\n",
    "# Hide random time segments\n",
    "t = random(0, time_mask_param)  # e.g., 0-20 frames\n",
    "t0 = random(0, time_steps - t)  # starting position\n",
    "spectrogram[:, t0:t0+t] = 0     # set to zero\n",
    "```\n",
    "\n",
    "**Analogy**: Muting random moments in a songâ€”the model learns to predict emotions even with gaps.\n",
    "\n",
    "**Why This Works**:\n",
    "- Forces model to use **all** parts of the spectrogram\n",
    "- Prevents **overfitting** (memorizing training data)\n",
    "- Improves **generalization** (works on new, unseen music)\n",
    "\n",
    "**Research Origin**: Proposed by Google for speech recognition (Park et al., 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cecb1b6",
   "metadata": {},
   "source": [
    "<a name=\"ml\"></a>\n",
    "## ğŸ¤– **Part 3: Machine Learning Foundations**\n",
    "\n",
    "### **What is Machine Learning?** ğŸ§ \n",
    "\n",
    "**Traditional Programming**:\n",
    "```\n",
    "Rules + Data â†’ Answers\n",
    "```\n",
    "Example: \"If temperature > 30Â°C, say 'hot'\"\n",
    "\n",
    "**Machine Learning**:\n",
    "```\n",
    "Data + Answers â†’ Rules\n",
    "```\n",
    "Example: Show 1000 examples of temperatures labeled \"hot\"/\"cold\" â†’ computer learns the pattern!\n",
    "\n",
    "---\n",
    "\n",
    "### **Neural Networks: Inspired by the Brain** ğŸ§¬\n",
    "\n",
    "**The Biological Analogy**:\n",
    "- **Neurons**: Brain cells that process information\n",
    "- **Synapses**: Connections between neurons (can be strong or weak)\n",
    "- **Learning**: Strengthening/weakening connections based on experience\n",
    "\n",
    "**Artificial Neural Networks**:\n",
    "\n",
    "#### **The Artificial Neuron** (Perceptron)\n",
    "\n",
    "```\n",
    "     xâ‚ â”€â”€â†’ [wâ‚] â”€â”€â”\n",
    "     xâ‚‚ â”€â”€â†’ [wâ‚‚] â”€â”€â”¤\n",
    "     xâ‚ƒ â”€â”€â†’ [wâ‚ƒ] â”€â”€â†’ Î£ â”€â”€â†’ [activation] â”€â”€â†’ output\n",
    "     ...            â”‚\n",
    "     xâ‚™ â”€â”€â†’ [wâ‚™] â”€â”€â”˜\n",
    "            +[bias]\n",
    "```\n",
    "\n",
    "**Mathematical Formula**:\n",
    "```\n",
    "output = activation(Î£áµ¢(wáµ¢ Ã— xáµ¢) + b)\n",
    "```\n",
    "where:\n",
    "- **xáµ¢**: Input features\n",
    "- **wáµ¢**: Weights (learned parameters)\n",
    "- **b**: Bias (learned offset)\n",
    "- **activation**: Non-linear function\n",
    "\n",
    "**Step-by-Step Example**:\n",
    "\n",
    "Suppose we're deciding if a sound is \"happy\":\n",
    "```\n",
    "Inputs:\n",
    "  xâ‚ = tempo (fast/slow)           = 0.8\n",
    "  xâ‚‚ = pitch (high/low)            = 0.6\n",
    "  xâ‚ƒ = energy (loud/quiet)         = 0.7\n",
    "\n",
    "Weights (learned by training):\n",
    "  wâ‚ = 2.0  (fast tempo â†’ happy)\n",
    "  wâ‚‚ = 1.5  (high pitch â†’ happy)\n",
    "  wâ‚ƒ = 1.2  (high energy â†’ happy)\n",
    "  b  = -1.0 (bias)\n",
    "\n",
    "Computation:\n",
    "  z = (2.0 Ã— 0.8) + (1.5 Ã— 0.6) + (1.2 Ã— 0.7) + (-1.0)\n",
    "    = 1.6 + 0.9 + 0.84 - 1.0\n",
    "    = 2.34\n",
    "\n",
    "Activation (Sigmoid):\n",
    "  output = 1 / (1 + e^(-2.34))\n",
    "         â‰ˆ 0.91  â†’ 91% confident it's happy!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Activation Functions: Adding Non-Linearity** ğŸ“ˆ\n",
    "\n",
    "**Why Needed?**: Without activation functions, multiple layers = one layer! We need non-linearity to learn complex patterns.\n",
    "\n",
    "#### **Common Activation Functions**:\n",
    "\n",
    "**1. ReLU (Rectified Linear Unit)** - Most Popular\n",
    "```\n",
    "f(x) = max(0, x)\n",
    "```\n",
    "- **Graph**: Flat at 0, then linear diagonal\n",
    "- **Behavior**: Blocks negative values, passes positive values\n",
    "- **Used**: Hidden layers in most modern networks\n",
    "\n",
    "**2. GELU (Gaussian Error Linear Unit)** - Used in Transformers\n",
    "```\n",
    "f(x) = x Ã— Î¦(x)\n",
    "where Î¦(x) = cumulative distribution function of standard normal\n",
    "```\n",
    "- **Smooth version** of ReLU\n",
    "- **Why better**: Gradients flow better, fewer \"dead neurons\"\n",
    "- **Used**: Our AST model uses GELU!\n",
    "\n",
    "**3. Sigmoid** - Classic\n",
    "```\n",
    "f(x) = 1 / (1 + e^(-x))\n",
    "```\n",
    "- **Range**: (0, 1)\n",
    "- **Used**: Binary classification, old networks\n",
    "\n",
    "**4. Tanh** (Hyperbolic Tangent)\n",
    "```\n",
    "f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "```\n",
    "- **Range**: (-1, 1)\n",
    "- **Better** than sigmoid (zero-centered)\n",
    "\n",
    "**Comparison**:\n",
    "```\n",
    "Input:  -2    -1     0     1     2\n",
    "ReLU:    0     0     0     1     2\n",
    "GELU:  -0.05 -0.16   0   0.84  1.95\n",
    "Sigmoid: 0.12  0.27  0.5  0.73  0.88\n",
    "Tanh:   -0.96 -0.76  0   0.76  0.96\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Deep Learning: Stacking Layers** ğŸ—ï¸\n",
    "\n",
    "**A Deep Neural Network**:\n",
    "```\n",
    "Input â†’ [Layer 1] â†’ [Layer 2] â†’ [Layer 3] â†’ ... â†’ [Output]\n",
    "         Hidden      Hidden       Hidden\n",
    "```\n",
    "\n",
    "Each layer learns **progressively abstract features**:\n",
    "\n",
    "**Example: Image Recognition**:\n",
    "- **Layer 1**: Edges and corners\n",
    "- **Layer 2**: Simple shapes (circles, rectangles)\n",
    "- **Layer 3**: Object parts (eyes, wheels, leaves)\n",
    "- **Output**: Complete objects (cat, car, tree)\n",
    "\n",
    "**For Audio/Music**:\n",
    "- **Layer 1**: Basic frequency patterns, onsets\n",
    "- **Layer 2**: Musical motifs, chord progressions\n",
    "- **Layer 3**: Melodic patterns, rhythm structures\n",
    "- **Output**: Emotional valence and arousal\n",
    "\n",
    "---\n",
    "\n",
    "### **Loss Functions: Measuring Mistakes** ğŸ“‰\n",
    "\n",
    "**The Concept**: How do we tell the model it's wrong? We need a **loss function** (also called cost or error function).\n",
    "\n",
    "**Goal**: **Minimize** the loss = make better predictions\n",
    "\n",
    "#### **Mean Squared Error (MSE)** - Used in This Notebook\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "MSE = (1/N) Ã— Î£áµ¢â‚Œâ‚á´º (yáµ¢ - Å·áµ¢)Â²\n",
    "```\n",
    "where:\n",
    "- **N**: Number of predictions\n",
    "- **yáµ¢**: True value (actual emotion rating)\n",
    "- **Å·áµ¢**: Predicted value (model's guess)\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "True valence:      [7, 3, 8, 5, 6]\n",
    "Predicted valence: [6, 4, 7, 5, 5]\n",
    "Errors:            [1,-1, 1, 0, 1]\n",
    "Squared errors:    [1, 1, 1, 0, 1]\n",
    "MSE = (1+1+1+0+1) / 5 = 0.8\n",
    "```\n",
    "\n",
    "**Why Square?**:\n",
    "1. **Positive errors**: -2 error same as +2 error (both bad)\n",
    "2. **Penalizes large errors**: Error of 2 = 4 penalty, error of 4 = 16 penalty\n",
    "3. **Smooth gradients**: Easy to differentiate mathematically\n",
    "\n",
    "**Alternative: Mean Absolute Error (MAE)**:\n",
    "```\n",
    "MAE = (1/N) Ã— Î£áµ¢â‚Œâ‚á´º |yáµ¢ - Å·áµ¢|\n",
    "```\n",
    "- Simpler: Just average of absolute errors\n",
    "- **Less sensitive** to outliers than MSE\n",
    "\n",
    "---\n",
    "\n",
    "### **Backpropagation: Learning from Mistakes** ğŸ”„\n",
    "\n",
    "**The Learning Process**:\n",
    "\n",
    "1. **Forward Pass**: Input â†’ compute output â†’ calculate loss\n",
    "2. **Backward Pass**: Loss â†’ compute gradients â†’ update weights\n",
    "3. **Repeat**: Until model performs well\n",
    "\n",
    "**The Mathematics** (Chain Rule):\n",
    "\n",
    "For a simple network: `Input â†’ Layer1 â†’ Layer2 â†’ Output â†’ Loss`\n",
    "\n",
    "**Chain Rule**:\n",
    "```\n",
    "âˆ‚Loss/âˆ‚wâ‚ = âˆ‚Loss/âˆ‚Output Ã— âˆ‚Output/âˆ‚Layer2 Ã— âˆ‚Layer2/âˆ‚Layer1 Ã— âˆ‚Layer1/âˆ‚wâ‚\n",
    "```\n",
    "\n",
    "**Intuition**: \"If I change this weight slightly, how much does the loss change?\"\n",
    "\n",
    "**Gradient Descent Update**:\n",
    "```\n",
    "w_new = w_old - learning_rate Ã— âˆ‚Loss/âˆ‚w\n",
    "```\n",
    "\n",
    "**Analogy**: Hiking down a mountain in fog:\n",
    "- **Loss**: Your altitude (want to minimize = reach valley)\n",
    "- **Gradient**: The slope direction (steepest descent)\n",
    "- **Learning rate**: How big your steps are\n",
    "  - Too large: You might overshoot the valley\n",
    "  - Too small: Takes forever to descend\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimizers: Smart Ways to Update Weights** âš¡\n",
    "\n",
    "#### **1. SGD (Stochastic Gradient Descent)** - Basic\n",
    "```\n",
    "w = w - lr Ã— gradient\n",
    "```\n",
    "- **Stochastic**: Use small batches instead of all data (faster!)\n",
    "- **Problem**: Can get stuck in local minima\n",
    "\n",
    "#### **2. Momentum** - Add Speed\n",
    "```\n",
    "v = Î² Ã— v + gradient        # velocity\n",
    "w = w - lr Ã— v\n",
    "```\n",
    "- **Idea**: Build momentum like a rolling ball\n",
    "- **Î²**: Momentum coefficient (usually 0.9)\n",
    "- **Advantage**: Accelerates in consistent directions, dampens oscillations\n",
    "\n",
    "#### **3. Adam (Adaptive Moment Estimation)** - Most Popular\n",
    "```\n",
    "m = Î²â‚ Ã— m + (1 - Î²â‚) Ã— gradient           # First moment (mean)\n",
    "v = Î²â‚‚ Ã— v + (1 - Î²â‚‚) Ã— gradientÂ²          # Second moment (variance)\n",
    "w = w - lr Ã— m / (âˆšv + Îµ)\n",
    "```\n",
    "- **Combines**: Momentum + adaptive learning rates\n",
    "- **Î²â‚**: Usually 0.9 (momentum)\n",
    "- **Î²â‚‚**: Usually 0.999 (RMSprop component)\n",
    "- **Îµ**: Small constant for numerical stability (10â»â¸)\n",
    "\n",
    "#### **4. AdamW** - Used in This Notebook! ğŸŒŸ\n",
    "```\n",
    "Same as Adam, but with weight decay:\n",
    "w = w - lr Ã— (m / (âˆšv + Îµ) + Î» Ã— w)\n",
    "```\n",
    "- **Î»**: Weight decay coefficient (0.01 in our code)\n",
    "- **Purpose**: **Regularization** (prevents overfitting)\n",
    "- **Effect**: Keeps weights small, forces model to learn simpler patterns\n",
    "\n",
    "**Why AdamW is Better Than Adam**:\n",
    "- Decouples weight decay from gradient updates\n",
    "- Improves generalization\n",
    "- Standard choice for transformer models\n",
    "\n",
    "---\n",
    "\n",
    "### **Learning Rate Scheduling** ğŸ“Š\n",
    "\n",
    "**The Problem**: Fixed learning rate isn't optimal:\n",
    "- **Start**: Need large steps to explore\n",
    "- **End**: Need small steps to fine-tune\n",
    "\n",
    "**Cosine Annealing** (Used in This Notebook):\n",
    "```\n",
    "lr_t = lr_min + (lr_max - lr_min) Ã— (1 + cos(Ï€t/T)) / 2\n",
    "```\n",
    "where:\n",
    "- **t**: Current epoch\n",
    "- **T**: Total epochs\n",
    "- **Result**: Smooth cosine curve from lr_max â†’ lr_min\n",
    "\n",
    "**Visualization**:\n",
    "```\n",
    "Epochs:    0    1    2    3    4    5\n",
    "LR:      1e-4  9e-5  7e-5  5e-5  3e-5  1e-5\n",
    "         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²_____________\n",
    "                        Smooth decay\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Smooth, predictable decay\n",
    "- No hyperparameters to tune (unlike step decay)\n",
    "- Works well for transformers\n",
    "\n",
    "---\n",
    "\n",
    "### **Overfitting vs. Underfitting** âš–ï¸\n",
    "\n",
    "**Underfitting**: Model too simple (high error on training AND test)\n",
    "```\n",
    "ğŸ¯ Reality: Complex curve\n",
    "ğŸ“‰ Model:   Straight line (doesn't fit)\n",
    "```\n",
    "\n",
    "**Good Fit**: Just right (low error on training AND test)\n",
    "```\n",
    "ğŸ¯ Reality: Complex curve\n",
    "ğŸ“ˆ Model:   Matching curve (fits well!)\n",
    "```\n",
    "\n",
    "**Overfitting**: Model too complex (low error on training, high on test)\n",
    "```\n",
    "ğŸ¯ Reality: Smooth curve with noise\n",
    "ğŸ“ˆ Model:   Wiggly line through every point (memorized noise!)\n",
    "```\n",
    "\n",
    "**How to Prevent Overfitting**:\n",
    "1. **More data**: Harder to memorize 100,000 examples than 100\n",
    "2. **Regularization**: Penalty for complex models (weight decay, dropout)\n",
    "3. **Data augmentation**: Create variations (SpecAugment!)\n",
    "4. **Early stopping**: Stop training when validation error increases\n",
    "5. **Dropout**: Randomly \"turn off\" neurons during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e91f18",
   "metadata": {},
   "source": [
    "<a name=\"transformers\"></a>\n",
    "## ğŸ”® **Part 4: Transformer Architecture - The Revolution**\n",
    "\n",
    "### **The Problem with Traditional Neural Networks** âš ï¸\n",
    "\n",
    "**Recurrent Neural Networks (RNNs)** were the standard for sequences (text, audio, time-series):\n",
    "\n",
    "```\n",
    "Inputâ‚ â†’ [RNN] â†’ Outputâ‚\n",
    "          â†“\n",
    "Inputâ‚‚ â†’ [RNN] â†’ Outputâ‚‚\n",
    "          â†“\n",
    "Inputâ‚ƒ â†’ [RNN] â†’ Outputâ‚ƒ\n",
    "```\n",
    "\n",
    "**Problems**:\n",
    "1. **Sequential Processing**: Must process one element at a time (slow!)\n",
    "2. **Vanishing Gradients**: Hard to learn long-range dependencies\n",
    "3. **No Parallelization**: Can't use GPUs efficiently\n",
    "\n",
    "---\n",
    "\n",
    "### **The Transformer Revolution** ğŸš€ (Vaswani et al., 2017)\n",
    "\n",
    "**\"Attention Is All You Need\"** - Groundbreaking paper that changed AI forever!\n",
    "\n",
    "**Key Idea**: Process **all** elements simultaneously using **self-attention**\n",
    "\n",
    "**Why It Matters**:\n",
    "- âœ… **Parallel**: Process entire sequence at once (100Ã— faster)\n",
    "- âœ… **Long-range**: Directly connects distant elements\n",
    "- âœ… **Scalable**: Works with massive datasets\n",
    "- âœ… **Versatile**: Powers GPT, BERT, ChatGPT, DALL-E, and our AST model!\n",
    "\n",
    "---\n",
    "\n",
    "### **Attention Mechanism: The Core Innovation** ğŸ’¡\n",
    "\n",
    "**The Question**: \"Which parts of the input should I focus on?\"\n",
    "\n",
    "**Real-World Analogy**:\n",
    "\n",
    "Imagine reading a book:\n",
    "- Your **eyes scan** all words on the page (**keys**)\n",
    "- Your **mind searches** for relevant information (**query**)\n",
    "- You **pay attention** to important words (**attention weights**)\n",
    "- You **extract meaning** from those words (**values**)\n",
    "\n",
    "---\n",
    "\n",
    "### **Self-Attention Mathematics** ğŸ”¢\n",
    "\n",
    "**The Setup**:\n",
    "\n",
    "For each position in the input, we create three vectors:\n",
    "- **Query (Q)**: \"What am I looking for?\"\n",
    "- **Key (K)**: \"What information do I contain?\"\n",
    "- **Value (V)**: \"What information do I provide?\"\n",
    "\n",
    "**Step-by-Step Process**:\n",
    "\n",
    "#### **Step 1: Linear Projections**\n",
    "```\n",
    "Q = Input Ã— W_Q    (transform input to query space)\n",
    "K = Input Ã— W_K    (transform input to key space)\n",
    "V = Input Ã— W_V    (transform input to value space)\n",
    "```\n",
    "\n",
    "where W_Q, W_K, W_V are learned weight matrices.\n",
    "\n",
    "#### **Step 2: Compute Attention Scores**\n",
    "```\n",
    "Scores = (Q Ã— K^T) / âˆšd_k\n",
    "```\n",
    "\n",
    "**What's happening**:\n",
    "- **Q Ã— K^T**: Dot product measures similarity between query and keys\n",
    "- **âˆšd_k**: Scaling factor (d_k = dimension of key vectors)\n",
    "- **Why scale?**: Prevents dot products from becoming too large\n",
    "\n",
    "**Example** (with 3 input positions):\n",
    "```\n",
    "        Kâ‚    Kâ‚‚    Kâ‚ƒ\n",
    "Qâ‚  [  0.9   0.3   0.1  ]    â† Qâ‚ most similar to Kâ‚\n",
    "Qâ‚‚  [  0.2   0.8   0.4  ]    â† Qâ‚‚ most similar to Kâ‚‚\n",
    "Qâ‚ƒ  [  0.1   0.3   0.9  ]    â† Qâ‚ƒ most similar to Kâ‚ƒ\n",
    "```\n",
    "\n",
    "#### **Step 3: Softmax Normalization**\n",
    "```\n",
    "Attention_Weights = softmax(Scores)\n",
    "```\n",
    "\n",
    "**Softmax** converts scores to probabilities (sum = 1):\n",
    "```\n",
    "softmax([xâ‚, xâ‚‚, xâ‚ƒ]) = [e^xâ‚, e^xâ‚‚, e^xâ‚ƒ] / (e^xâ‚ + e^xâ‚‚ + e^xâ‚ƒ)\n",
    "```\n",
    "\n",
    "**After softmax**:\n",
    "```\n",
    "        Vâ‚    Vâ‚‚    Vâ‚ƒ\n",
    "Qâ‚  [  0.7   0.2   0.1  ]    â† 70% attention to Vâ‚\n",
    "Qâ‚‚  [  0.1   0.6   0.3  ]    â† 60% attention to Vâ‚‚\n",
    "Qâ‚ƒ  [  0.1   0.2   0.7  ]    â† 70% attention to Vâ‚ƒ\n",
    "```\n",
    "\n",
    "#### **Step 4: Weighted Sum of Values**\n",
    "```\n",
    "Output = Attention_Weights Ã— V\n",
    "```\n",
    "\n",
    "**Complete Formula**:\n",
    "```\n",
    "Attention(Q, K, V) = softmax((Q Ã— K^T) / âˆšd_k) Ã— V\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Multi-Head Attention: Multiple Perspectives** ğŸ‘¥\n",
    "\n",
    "**The Idea**: Instead of one attention mechanism, use **multiple** in parallel!\n",
    "\n",
    "**Analogy**: Reading a book with different goals:\n",
    "- **Head 1**: Focus on plot (who did what?)\n",
    "- **Head 2**: Focus on emotions (how do characters feel?)\n",
    "- **Head 3**: Focus on setting (where/when does it happen?)\n",
    "- **Head 4**: Focus on themes (what's the deeper meaning?)\n",
    "\n",
    "**Mathematics**:\n",
    "```\n",
    "MultiHead(Q, K, V) = Concat(headâ‚, headâ‚‚, ..., headâ‚•) Ã— W_O\n",
    "\n",
    "where each head_i = Attention(QÃ—W_Qâ±, KÃ—W_Kâ±, VÃ—W_Vâ±)\n",
    "```\n",
    "\n",
    "**In Our Model**:\n",
    "- **num_heads = 4**: We use 4 attention heads\n",
    "- **embed_dim = 256**: Total embedding dimension\n",
    "- **head_dim = 256 / 4 = 64**: Each head works with 64 dimensions\n",
    "\n",
    "**Why Multiple Heads?**:\n",
    "1. Learn **different types** of patterns\n",
    "2. **Redundancy**: If one head fails, others compensate\n",
    "3. **Richer representations**: Capture multiple aspects simultaneously\n",
    "\n",
    "---\n",
    "\n",
    "### **The Complete Transformer Block** ğŸ—ï¸\n",
    "\n",
    "```\n",
    "Input Embedding\n",
    "      â†“\n",
    "[Layer Normalization]\n",
    "      â†“\n",
    "[Multi-Head Self-Attention]  â† Learn relationships\n",
    "      â†“\n",
    "[Residual Connection] â”€â”€â”€â”€â”€â”  â† Skip connection\n",
    "      â†“                     â”‚\n",
    "[Layer Normalization]       â”‚\n",
    "      â†“                     â”‚\n",
    "[Feed-Forward Network]      â”‚  â† Process features\n",
    "      â†“                     â”‚\n",
    "[Residual Connection] â†â”€â”€â”€â”€â”€â”˜  â† Skip connection\n",
    "      â†“\n",
    "Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Components Explained**\n",
    "\n",
    "#### **1. Layer Normalization**\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "LN(x) = Î³ Ã— (x - Î¼) / Ïƒ + Î²\n",
    "```\n",
    "where:\n",
    "- **Î¼**: Mean of features\n",
    "- **Ïƒ**: Standard deviation\n",
    "- **Î³, Î²**: Learned parameters\n",
    "\n",
    "**Purpose**: Stabilize training, prevent internal covariate shift\n",
    "\n",
    "**Difference from Batch Normalization**:\n",
    "- **Batch Norm**: Normalizes across batch dimension\n",
    "- **Layer Norm**: Normalizes across feature dimension (better for sequences!)\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Residual Connections** (Skip Connections)\n",
    "\n",
    "```\n",
    "output = Layer(input) + input\n",
    "```\n",
    "\n",
    "**Why Critical**:\n",
    "- **Gradient Flow**: Allows gradients to flow directly backward\n",
    "- **Deep Networks**: Enables training networks with 100+ layers\n",
    "- **Identity Mapping**: Model can learn to \"do nothing\" if needed\n",
    "\n",
    "**Analogy**: Like a highway bypass:\n",
    "- **Main road**: Goes through the layer (can transform)\n",
    "- **Bypass**: Skips the layer (preserves original)\n",
    "- **Result**: Best of both worlds!\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Feed-Forward Network (FFN)**\n",
    "\n",
    "```\n",
    "FFN(x) = GELU(x Ã— Wâ‚ + bâ‚) Ã— Wâ‚‚ + bâ‚‚\n",
    "```\n",
    "\n",
    "**Structure**:\n",
    "```\n",
    "Input (256 dim)\n",
    "    â†“\n",
    "[Linear: 256 â†’ 1024]    â† Expansion (mlp_ratio = 4)\n",
    "    â†“\n",
    "[GELU Activation]\n",
    "    â†“\n",
    "[Dropout]\n",
    "    â†“\n",
    "[Linear: 1024 â†’ 256]    â† Compression\n",
    "    â†“\n",
    "Output (256 dim)\n",
    "```\n",
    "\n",
    "**Purpose**:\n",
    "- **Non-linear transformation**: Process attended features\n",
    "- **Position-wise**: Applied independently to each position\n",
    "- **Expansion-compression**: Increase capacity, then project back\n",
    "\n",
    "---\n",
    "\n",
    "### **Positional Encoding: Adding Order Information** ğŸ“\n",
    "\n",
    "**The Problem**: Self-attention has **no notion of order**!\n",
    "- \"dog bites man\" vs. \"man bites dog\" look the same to attention!\n",
    "\n",
    "**The Solution**: Add **positional information** to embeddings\n",
    "\n",
    "**Two Approaches**:\n",
    "\n",
    "#### **1. Sinusoidal (Original Transformer)**:\n",
    "```\n",
    "PE(pos, 2i)   = sin(pos / 10000^(2i/d))\n",
    "PE(pos, 2i+1) = cos(pos / 10000^(2i/d))\n",
    "```\n",
    "- **pos**: Position in sequence\n",
    "- **i**: Dimension index\n",
    "- **d**: Embedding dimension\n",
    "\n",
    "**Properties**:\n",
    "- Deterministic (no learning required)\n",
    "- Can generalize to unseen sequence lengths\n",
    "- Encodes relative positions\n",
    "\n",
    "#### **2. Learned Positional Embeddings** (Used in Our AST!):\n",
    "```\n",
    "pos_embed = Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "```\n",
    "- **Learned**: Trained along with model\n",
    "- **Fixed length**: Works for specific input size\n",
    "- **Often performs better** for fixed-size inputs like images/spectrograms\n",
    "\n",
    "---\n",
    "\n",
    "### **Vision Transformer (ViT) â†’ Audio Spectrogram Transformer (AST)** ğŸ¨â†’ğŸµ\n",
    "\n",
    "**ViT Innovation** (Dosovitskiy et al., 2020):\n",
    "Apply transformers to **images** by treating them as sequences of patches!\n",
    "\n",
    "**Our Adaptation** (AST for Audio):\n",
    "Treat **spectrograms** as images â†’ apply ViT!\n",
    "\n",
    "```\n",
    "Spectrogram (128 Ã— 432)\n",
    "        â†“\n",
    "Split into patches (16 Ã— 16)\n",
    "        â†“\n",
    "216 patches (8 height Ã— 27 width)\n",
    "        â†“\n",
    "Flatten each patch â†’ 256-dim vector\n",
    "        â†“\n",
    "Add [CLS] token (for classification)\n",
    "        â†“\n",
    "Add positional embeddings\n",
    "        â†“\n",
    "217 tokens â†’ Transformer Encoder\n",
    "        â†“\n",
    "Extract [CLS] token\n",
    "        â†“\n",
    "Regression head â†’ Valence & Arousal\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Patch Embedding: From Pixels to Tokens** ğŸ§©\n",
    "\n",
    "**Goal**: Convert 2D spectrogram patches into 1D vectors\n",
    "\n",
    "**Method**: Convolutional layer with stride = patch_size\n",
    "\n",
    "```python\n",
    "self.patch_embed = nn.Conv2d(\n",
    "    in_channels=1,        # Grayscale spectrogram\n",
    "    out_channels=256,     # Embedding dimension\n",
    "    kernel_size=16,       # Patch size (16Ã—16)\n",
    "    stride=16             # No overlap between patches\n",
    ")\n",
    "```\n",
    "\n",
    "**What This Does**:\n",
    "\n",
    "**Input**: (Batch, 1, 128, 432)\n",
    "**Operation**: 16Ã—16 convolution with 256 filters, stride 16\n",
    "**Output**: (Batch, 256, 8, 27)  â† 8Ã—27 = 216 patches, each 256-dim\n",
    "\n",
    "**Then flatten**: (Batch, 256, 8, 27) â†’ (Batch, 216, 256)\n",
    "\n",
    "**Analogy**: Imagine a photo booth taking 216 separate photos of different parts of the spectrogram, each photo captured as a 256-number description.\n",
    "\n",
    "---\n",
    "\n",
    "### **The [CLS] Token: A Special Token for Classification** ğŸ‘‘\n",
    "\n",
    "**Origin**: Borrowed from BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "**Concept**:\n",
    "```\n",
    "[CLS] + Patchâ‚ + Patchâ‚‚ + ... + Patchâ‚‚â‚â‚†\n",
    "  â†“\n",
    "Transformer (all tokens interact)\n",
    "  â†“\n",
    "Extract [CLS] â†’ it now contains global information!\n",
    "```\n",
    "\n",
    "**Why It Works**:\n",
    "- [CLS] has **no prior information** (starts as zeros)\n",
    "- Through self-attention, it **aggregates** information from all patches\n",
    "- Acts as a **summary** of the entire spectrogram\n",
    "- Perfect for making a global prediction (valence/arousal)!\n",
    "\n",
    "**Initialization**:\n",
    "```python\n",
    "self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "nn.init.normal_(self.cls_token, std=0.02)  # Small random noise\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Parameter Count: Understanding Model Size** ğŸ“Š\n",
    "\n",
    "**Our AST Model Parameters**:\n",
    "\n",
    "```\n",
    "Component                   Parameters\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Patch Embedding            65,536\n",
    "  (1Ã—256, kernel 16Ã—16)\n",
    "  \n",
    "CLS Token                  256\n",
    "\n",
    "Positional Embeddings      55,552\n",
    "  (217 positions Ã— 256 dim)\n",
    "  \n",
    "Transformer Blocks (4Ã—):   ~2.6M each\n",
    "  - Multi-Head Attention:  788,480\n",
    "    â€¢ Q, K, V projections: 196,608 each\n",
    "    â€¢ Output projection:   65,536\n",
    "  - Layer Norms:           1,024\n",
    "  - FFN:                   1,835,008\n",
    "    â€¢ Linear 256â†’1024:     262,144\n",
    "    â€¢ Linear 1024â†’256:     262,144\n",
    "    \n",
    "Regression Head            33,282\n",
    "  - Linear 256â†’128:        32,768\n",
    "  - Linear 128â†’2:          258\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TOTAL:                     ~10.5 Million parameters\n",
    "```\n",
    "\n",
    "**What This Means**:\n",
    "- **10.5M parameters**: 10.5 million numbers to learn!\n",
    "- **Memory**: ~42 MB (float32) or ~21 MB (float16)\n",
    "- **Computation**: ~2.5 GFLOPs per forward pass\n",
    "\n",
    "**Is 10.5M parameters a lot?**\n",
    "- **Tiny** compared to GPT-3 (175 billion parameters)\n",
    "- **Medium** for audio/vision tasks\n",
    "- **Manageable** on consumer GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef7a9c",
   "metadata": {},
   "source": [
    "<a name=\"training\"></a>\n",
    "## ğŸ‹ï¸ **Part 5: The Training Process - Teaching the Model**\n",
    "\n",
    "### **The Training Loop: A Complete Cycle** ğŸ”„\n",
    "\n",
    "```\n",
    "FOR each epoch (1 to NUM_EPOCHS):\n",
    "    FOR each batch in training data:\n",
    "        1. Forward Pass    â†’ Get predictions\n",
    "        2. Compute Loss    â†’ Measure error\n",
    "        3. Backward Pass   â†’ Compute gradients\n",
    "        4. Update Weights  â†’ Improve model\n",
    "    \n",
    "    Validate on validation set\n",
    "    Save best model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Forward Pass** â¡ï¸\n",
    "\n",
    "**The Journey of Data Through Our Model**:\n",
    "\n",
    "```\n",
    "Input: Audio file (45 seconds MP3)\n",
    "    â†“\n",
    "Load & Resample â†’ 44,100 Hz\n",
    "    â†“\n",
    "Extract 5-second segments\n",
    "    â†“\n",
    "Compute Mel-Spectrogram â†’ (128 Ã— 432)\n",
    "    â†“\n",
    "Normalize â†’ Mean=0, Std=1\n",
    "    â†“\n",
    "Apply SpecAugment (training only)\n",
    "    â†“\n",
    "Patch Embedding â†’ 216 patches of 256-dim\n",
    "    â†“\n",
    "Add [CLS] token â†’ 217 tokens\n",
    "    â†“\n",
    "Add Positional Embeddings\n",
    "    â†“\n",
    "Transformer Layer 1 â†’ Self-Attention + FFN\n",
    "    â†“\n",
    "Transformer Layer 2 â†’ Self-Attention + FFN\n",
    "    â†“\n",
    "Transformer Layer 3 â†’ Self-Attention + FFN\n",
    "    â†“\n",
    "Transformer Layer 4 â†’ Self-Attention + FFN\n",
    "    â†“\n",
    "Extract [CLS] token\n",
    "    â†“\n",
    "Regression Head â†’ (valence, arousal)\n",
    "    â†“\n",
    "Output: [7.2, 5.8]  (predicted emotions!)\n",
    "```\n",
    "\n",
    "**Matrix Dimensions at Each Step**:\n",
    "\n",
    "```\n",
    "Operation                        Shape\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Input spectrogram               (batch, 1, 128, 432)\n",
    "After patch embedding           (batch, 216, 256)\n",
    "Add CLS token                   (batch, 217, 256)\n",
    "Add positional embeddings       (batch, 217, 256)  [no shape change]\n",
    "After each transformer block    (batch, 217, 256)  [no shape change]\n",
    "Extract CLS token               (batch, 256)\n",
    "After FC layer 256â†’128          (batch, 128)\n",
    "Final output                    (batch, 2)         [valence, arousal]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Loss Computation** ğŸ“‰\n",
    "\n",
    "**Ground Truth vs. Prediction**:\n",
    "\n",
    "```\n",
    "Sample 1:\n",
    "  True:      [Valence: 7.5, Arousal: 6.2]\n",
    "  Predicted: [Valence: 7.2, Arousal: 5.8]\n",
    "  Errors:    [        -0.3,          -0.4]\n",
    "\n",
    "Sample 2:\n",
    "  True:      [Valence: 3.1, Arousal: 4.5]\n",
    "  Predicted: [Valence: 3.8, Arousal: 4.9]\n",
    "  Errors:    [        +0.7,          +0.4]\n",
    "```\n",
    "\n",
    "**Mean Squared Error (MSE)**:\n",
    "```\n",
    "MSE = (1/N) Ã— Î£(y_true - y_pred)Â²\n",
    "\n",
    "For Sample 1:\n",
    "  MSE_1 = ((-0.3)Â² + (-0.4)Â²) / 2\n",
    "        = (0.09 + 0.16) / 2\n",
    "        = 0.125\n",
    "\n",
    "For Sample 2:\n",
    "  MSE_2 = ((0.7)Â² + (0.4)Â²) / 2\n",
    "        = (0.49 + 0.16) / 2\n",
    "        = 0.325\n",
    "\n",
    "Batch MSE = (0.125 + 0.325) / 2 = 0.225\n",
    "```\n",
    "\n",
    "**In PyTorch**:\n",
    "```python\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(predictions, targets)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Backpropagation** â¬…ï¸\n",
    "\n",
    "**Goal**: Compute âˆ‚Loss/âˆ‚w for every parameter w\n",
    "\n",
    "**The Chain Rule in Action**:\n",
    "\n",
    "```\n",
    "âˆ‚Loss/âˆ‚wâ‚ = âˆ‚Loss/âˆ‚output Ã— âˆ‚output/âˆ‚layerâ‚„ Ã— âˆ‚layerâ‚„/âˆ‚layerâ‚ƒ Ã— âˆ‚layerâ‚ƒ/âˆ‚layerâ‚‚ Ã— âˆ‚layerâ‚‚/âˆ‚layerâ‚ Ã— âˆ‚layerâ‚/âˆ‚wâ‚\n",
    "```\n",
    "\n",
    "**Visualizing Gradient Flow**:\n",
    "\n",
    "```\n",
    "Forward Pass â†’\n",
    "Input â”€â”€â†’ Layer1 â”€â”€â†’ Layer2 â”€â”€â†’ Layer3 â”€â”€â†’ Output â”€â”€â†’ Loss\n",
    "     â†‘         â†‘         â†‘         â†‘          â†‘\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     â† Backward Pass (Gradients)\n",
    "```\n",
    "\n",
    "**PyTorch Magic**:\n",
    "```python\n",
    "loss.backward()  # Automatically computes all gradients!\n",
    "```\n",
    "\n",
    "**What Happens Internally**:\n",
    "1. PyTorch builds a **computational graph** during forward pass\n",
    "2. Records every operation (multiply, add, activation, etc.)\n",
    "3. During `.backward()`, traverses graph in reverse\n",
    "4. Applies chain rule automatically\n",
    "5. Stores gradients in `.grad` attribute of each parameter\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Gradient Descent Update** ğŸ“ˆ\n",
    "\n",
    "**AdamW Update (Simplified)**:\n",
    "\n",
    "```python\n",
    "# For each parameter w:\n",
    "# 1. Get gradient\n",
    "g = w.grad\n",
    "\n",
    "# 2. Update first moment (momentum)\n",
    "m = Î²â‚ * m + (1 - Î²â‚) * g\n",
    "\n",
    "# 3. Update second moment (variance)\n",
    "v = Î²â‚‚ * v + (1 - Î²â‚‚) * gÂ²\n",
    "\n",
    "# 4. Bias correction\n",
    "m_hat = m / (1 - Î²â‚^t)\n",
    "v_hat = v / (1 - Î²â‚‚^t)\n",
    "\n",
    "# 5. Apply update with weight decay\n",
    "w = w - lr * (m_hat / (âˆšv_hat + Îµ) + Î» * w)\n",
    "```\n",
    "\n",
    "**Numerical Example**:\n",
    "\n",
    "```\n",
    "Parameter w = 0.5\n",
    "Gradient g = 0.1\n",
    "Learning rate lr = 0.001\n",
    "Weight decay Î» = 0.01\n",
    "\n",
    "Adam component:\n",
    "  m_hat = 0.09 (smoothed gradient)\n",
    "  v_hat = 0.01 (smoothed variance)\n",
    "  adam_update = 0.09 / âˆš0.01 = 0.9\n",
    "\n",
    "Weight decay component:\n",
    "  decay = 0.01 * 0.5 = 0.005\n",
    "\n",
    "Total update:\n",
    "  w_new = 0.5 - 0.001 * (0.9 + 0.005)\n",
    "        = 0.5 - 0.000905\n",
    "        = 0.499095\n",
    "```\n",
    "\n",
    "**Why This Works**:\n",
    "- **Momentum (m)**: Accelerates in consistent directions\n",
    "- **Adaptive rates (v)**: Larger updates for rarely-changing params\n",
    "- **Weight decay**: Prevents weights from growing too large\n",
    "\n",
    "---\n",
    "\n",
    "### **Gradient Clipping: Preventing Explosions** ğŸ’£\n",
    "\n",
    "**The Problem**: Gradients can become extremely large â†’ unstable training\n",
    "\n",
    "**The Solution**:\n",
    "```python\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "```\n",
    "\n",
    "**What It Does**:\n",
    "```\n",
    "If ||gradients|| > max_norm:\n",
    "    gradients = gradients * (max_norm / ||gradients||)\n",
    "```\n",
    "\n",
    "**Analogy**: Speed limit on a highway:\n",
    "- If gradients \"drive\" too fast (large norm)\n",
    "- Scale them down to maximum allowed speed\n",
    "- Prevents \"crashes\" (training divergence)\n",
    "\n",
    "---\n",
    "\n",
    "### **Batch Processing: Efficiency Through Parallelism** ğŸš„\n",
    "\n",
    "**Why Batches?**\n",
    "\n",
    "**Bad: Processing one sample at a time**:\n",
    "```\n",
    "FOR each sample in 10,000 samples:\n",
    "    Forward pass (1 sample)     â†’ GPU mostly idle\n",
    "    Compute loss (1 sample)\n",
    "    Backward pass (1 sample)\n",
    "    Update weights\n",
    "Total time: ~10 hours\n",
    "```\n",
    "\n",
    "**Good: Processing batches**:\n",
    "```\n",
    "FOR each batch of 16 samples:\n",
    "    Forward pass (16 samples)   â†’ GPU fully utilized!\n",
    "    Compute loss (16 samples)\n",
    "    Backward pass (16 samples)\n",
    "    Update weights\n",
    "Total time: ~40 minutes (15Ã— faster!)\n",
    "```\n",
    "\n",
    "**Batch Size Trade-offs**:\n",
    "\n",
    "**Small Batches** (e.g., 8):\n",
    "- âœ… More frequent updates (faster learning)\n",
    "- âœ… Better generalization (more noise)\n",
    "- âœ… Less memory required\n",
    "- âŒ Less efficient GPU utilization\n",
    "- âŒ Noisier gradients\n",
    "\n",
    "**Large Batches** (e.g., 64):\n",
    "- âœ… Better GPU utilization (faster training)\n",
    "- âœ… More stable gradients\n",
    "- âŒ Fewer updates per epoch\n",
    "- âŒ May overfit more easily\n",
    "- âŒ Requires more GPU memory\n",
    "\n",
    "**Our Choice: batch_size = 16**\n",
    "- Balanced for most GPUs\n",
    "- Good trade-off between speed and stability\n",
    "\n",
    "---\n",
    "\n",
    "### **Train/Validation/Test Split: Honest Evaluation** ğŸ“Š\n",
    "\n",
    "**Why We Need Three Sets**:\n",
    "\n",
    "```\n",
    "FULL DATASET (100%)\n",
    "    â†“\n",
    "    â”œâ”€â†’ Training Set (80%)      â†’ Model learns from this\n",
    "    â”œâ”€â†’ Validation Set (10%)    â†’ Tune hyperparameters, select best model\n",
    "    â””â”€â†’ Test Set (10%)          â†’ Final evaluation (never seen during training!)\n",
    "```\n",
    "\n",
    "**Analogy**: Learning for an exam:\n",
    "- **Training**: Practice problems you study from\n",
    "- **Validation**: Practice exam to check progress\n",
    "- **Test**: Actual exam (truly measures what you learned!)\n",
    "\n",
    "**Critical Rule**: ğŸš¨ **NEVER** use test set during training or model selection!\n",
    "\n",
    "**Why It Matters**:\n",
    "\n",
    "**Bad Practice**:\n",
    "```\n",
    "Try model A â†’ Test accuracy: 85%\n",
    "Try model B â†’ Test accuracy: 87%  â† Pick this!\n",
    "Try model C â†’ Test accuracy: 84%\n",
    "\n",
    "Report: \"Our model achieves 87% on test set\"\n",
    "Problem: You optimized FOR the test set! (data leakage)\n",
    "```\n",
    "\n",
    "**Good Practice**:\n",
    "```\n",
    "Try model A â†’ Val accuracy: 85%\n",
    "Try model B â†’ Val accuracy: 87%  â† Pick this!\n",
    "Try model C â†’ Val accuracy: 84%\n",
    "\n",
    "Finally evaluate model B on test set â†’ 86%\n",
    "Report: \"Our model achieves 86% on test set\" âœ“\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Epochs: Multiple Passes Through Data** ğŸ”\n",
    "\n",
    "**One Epoch** = One complete pass through all training data\n",
    "\n",
    "**Why Multiple Epochs?**\n",
    "\n",
    "**After 1 Epoch**:\n",
    "```\n",
    "Model: \"I've seen each song once\"\n",
    "Performance: 60% accuracy (still learning basic patterns)\n",
    "```\n",
    "\n",
    "**After 5 Epochs**:\n",
    "```\n",
    "Model: \"I've seen each song 5 times\"\n",
    "Performance: 80% accuracy (learned most patterns)\n",
    "```\n",
    "\n",
    "**After 15 Epochs**:\n",
    "```\n",
    "Model: \"I've seen each song 15 times\"\n",
    "Performance: 85% accuracy (refined understanding)\n",
    "```\n",
    "\n",
    "**After 50 Epochs** (too many!):\n",
    "```\n",
    "Model: \"I've memorized the training songs!\"\n",
    "Train Performance: 98% (great!)\n",
    "Test Performance: 75% (worse than epoch 15!)\n",
    "Problem: OVERFITTING\n",
    "```\n",
    "\n",
    "**Typical Training Curve**:\n",
    "\n",
    "```\n",
    "Accuracy\n",
    "    â†‘\n",
    "100%â”¤                     â•±â”€â”€â”€â”€â”€ Training (overfitting)\n",
    "    â”‚                   â•±\n",
    " 80%â”¤              â•­â”€â”€â”€â•¯â”€â”€â”€â”€â”€â”€â”€ Validation (best)\n",
    "    â”‚          â•­â”€â”€â•¯      â•²\n",
    " 60%â”¤      â•­â”€â”€â•¯            â•²â”€â”€â”€â”€ Validation (declining)\n",
    "    â”‚  â•­â”€â”€â•¯\n",
    " 40%â”¤â”€â•¯\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs\n",
    "     0  5  10  15  20  25  30\n",
    "\n",
    "Optimal stopping point: ~15 epochs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Early Stopping: Knowing When to Stop** â¸ï¸\n",
    "\n",
    "**The Strategy**:\n",
    "```python\n",
    "best_val_loss = infinity\n",
    "\n",
    "for epoch in epochs:\n",
    "    train_loss = train_one_epoch()\n",
    "    val_loss = validate()\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_model()  # âœ“ New best!\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience_limit:\n",
    "        print(\"Early stopping!\")\n",
    "        break  # Stop training\n",
    "```\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Epoch  Val Loss  Action\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  1     1.250    Save (best so far)\n",
    "  2     1.100    Save (improved!)\n",
    "  3     1.050    Save (still improving)\n",
    "  4     1.080    Don't save (worse)\n",
    "  5     1.120    Don't save (worse)\n",
    "  6     1.150    Don't save (worse) â† patience = 3, STOP!\n",
    "\n",
    "Final model: Epoch 3 (val_loss = 1.050)\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- Prevents overfitting\n",
    "- Saves computational resources\n",
    "- Automatically finds optimal training duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a9f4f",
   "metadata": {},
   "source": [
    "<a name=\"metrics\"></a>\n",
    "## ğŸ“ **Part 6: Evaluation Metrics - Measuring Success**\n",
    "\n",
    "### **Why Multiple Metrics?** ğŸ¯\n",
    "\n",
    "Different metrics reveal different aspects of model performance:\n",
    "- **MSE**: Penalizes large errors heavily\n",
    "- **MAE**: Treats all errors equally\n",
    "- **CCC**: Measures agreement (considers both accuracy and correlation)\n",
    "\n",
    "**Analogy**: Grading a student:\n",
    "- **MSE**: Big mistakes hurt grade a lot (failed exam â†’ F)\n",
    "- **MAE**: Average score across all assignments\n",
    "- **CCC**: Consistency between effort and results\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Mean Squared Error (MSE)** ğŸ“‰\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "MSE = (1/N) Ã— Î£áµ¢â‚Œâ‚á´º (yáµ¢ - Å·áµ¢)Â²\n",
    "```\n",
    "\n",
    "**Step-by-Step Calculation**:\n",
    "\n",
    "```\n",
    "Predictions:  [7.2, 5.8, 3.1, 8.5, 4.9]\n",
    "Ground Truth: [7.5, 6.2, 3.0, 8.0, 5.0]\n",
    "\n",
    "Step 1: Compute errors\n",
    "Errors:       [-0.3, -0.4, 0.1, 0.5, -0.1]\n",
    "\n",
    "Step 2: Square each error\n",
    "Squared:      [0.09, 0.16, 0.01, 0.25, 0.01]\n",
    "\n",
    "Step 3: Average\n",
    "MSE = (0.09 + 0.16 + 0.01 + 0.25 + 0.01) / 5\n",
    "    = 0.52 / 5\n",
    "    = 0.104\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- **Lower is better** (0 = perfect predictions)\n",
    "- **Units**: Squared units of measurement (if predicting 1-9 scale, MSE in range [0, 64])\n",
    "- **Sensitivity**: Very sensitive to outliers\n",
    "\n",
    "**Why Square?**:\n",
    "1. **Positive**: Errors in both directions penalized equally\n",
    "2. **Differentiable**: Smooth gradient for optimization\n",
    "3. **Outlier penalty**: Large errors (3.0) penalized much more than small errors (0.3)\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Root Mean Squared Error (RMSE)** ğŸ“\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "RMSE = âˆšMSE = âˆš[(1/N) Ã— Î£áµ¢â‚Œâ‚á´º (yáµ¢ - Å·áµ¢)Â²]\n",
    "```\n",
    "\n",
    "**Continuing Example**:\n",
    "```\n",
    "RMSE = âˆš0.104 = 0.322\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- **Same units** as original measurements (if 1-9 scale, RMSE also 1-9)\n",
    "- **Easier to interpret** than MSE\n",
    "- **Example**: RMSE = 0.5 means \"on average, predictions off by Â±0.5 points\"\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Mean Absolute Error (MAE)** ğŸ“Š\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "MAE = (1/N) Ã— Î£áµ¢â‚Œâ‚á´º |yáµ¢ - Å·áµ¢|\n",
    "```\n",
    "\n",
    "**Step-by-Step Calculation**:\n",
    "\n",
    "```\n",
    "Predictions:  [7.2, 5.8, 3.1, 8.5, 4.9]\n",
    "Ground Truth: [7.5, 6.2, 3.0, 8.0, 5.0]\n",
    "\n",
    "Step 1: Compute errors\n",
    "Errors:       [-0.3, -0.4, 0.1, 0.5, -0.1]\n",
    "\n",
    "Step 2: Absolute value\n",
    "Absolute:     [0.3, 0.4, 0.1, 0.5, 0.1]\n",
    "\n",
    "Step 3: Average\n",
    "MAE = (0.3 + 0.4 + 0.1 + 0.5 + 0.1) / 5\n",
    "    = 1.4 / 5\n",
    "    = 0.28\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- **Lower is better** (0 = perfect)\n",
    "- **Same units** as measurements\n",
    "- **Direct meaning**: \"Average absolute error is 0.28 points\"\n",
    "\n",
    "**MSE vs. MAE Comparison**:\n",
    "\n",
    "```\n",
    "Errors:      [0.1, 0.1, 0.1, 5.0]\n",
    "\n",
    "MAE = (0.1 + 0.1 + 0.1 + 5.0) / 4 = 1.325\n",
    "MSE = (0.01 + 0.01 + 0.01 + 25) / 4 = 6.26\n",
    "\n",
    "The outlier (5.0) has huge impact on MSE!\n",
    "```\n",
    "\n",
    "**When to Use**:\n",
    "- **MAE**: When all errors matter equally (robust to outliers)\n",
    "- **MSE**: When large errors are particularly bad (penalize outliers)\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Concordance Correlation Coefficient (CCC)** ğŸ¯\n",
    "\n",
    "**The Gold Standard for Continuous Prediction Evaluation**\n",
    "\n",
    "**What It Measures**: Agreement between predictions and truth\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "CCC = (2 Ã— Ï Ã— Ïƒ_y Ã— Ïƒ_Å·) / (Ïƒ_yÂ² + Ïƒ_Å·Â² + (Î¼_y - Î¼_Å·)Â²)\n",
    "```\n",
    "\n",
    "where:\n",
    "- **Ï** = Pearson correlation coefficient\n",
    "- **Ïƒ_y** = Standard deviation of ground truth\n",
    "- **Ïƒ_Å·** = Standard deviation of predictions\n",
    "- **Î¼_y** = Mean of ground truth\n",
    "- **Î¼_Å·** = Mean of predictions\n",
    "\n",
    "---\n",
    "\n",
    "### **CCC: Component Breakdown** ğŸ”\n",
    "\n",
    "#### **Component 1: Pearson Correlation (Ï)**\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "Ï = Cov(y, Å·) / (Ïƒ_y Ã— Ïƒ_Å·)\n",
    "```\n",
    "\n",
    "**What it measures**: Linear relationship strength\n",
    "- **Ï = +1**: Perfect positive correlation\n",
    "- **Ï = 0**: No correlation\n",
    "- **Ï = -1**: Perfect negative correlation\n",
    "\n",
    "**Example Calculation**:\n",
    "```\n",
    "Ground Truth: [3, 5, 7, 4, 6]    Mean: 5.0, Std: 1.41\n",
    "Predictions:  [3, 6, 8, 4, 7]    Mean: 5.6, Std: 1.85\n",
    "\n",
    "Covariance:\n",
    "  Cov = Î£[(y - Î¼_y)(Å· - Î¼_Å·)] / N\n",
    "      = [(3-5)(3-5.6) + (5-5)(6-5.6) + (7-5)(8-5.6) + (4-5)(4-5.6) + (6-5)(7-5.6)] / 5\n",
    "      = [5.2 + 0 + 4.8 + 1.6 + 1.4] / 5\n",
    "      = 2.6\n",
    "\n",
    "Correlation:\n",
    "  Ï = 2.6 / (1.41 Ã— 1.85) = 2.6 / 2.61 = 0.996\n",
    "```\n",
    "\n",
    "#### **Component 2: Location Shift (Î¼_y - Î¼_Å·)**\n",
    "\n",
    "**What it measures**: Systematic bias\n",
    "```\n",
    "If Î¼_y = 5.0 and Î¼_Å· = 5.6:\n",
    "  Location shift = 5.0 - 5.6 = -0.6\n",
    "  Interpretation: Predictions systematically 0.6 points higher\n",
    "```\n",
    "\n",
    "#### **Component 3: Scale Difference (Ïƒ_y vs. Ïƒ_Å·)**\n",
    "\n",
    "**What it measures**: Spread difference\n",
    "```\n",
    "If Ïƒ_y = 1.41 and Ïƒ_Å· = 1.85:\n",
    "  Interpretation: Predictions have wider range than truth\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **CCC: Complete Calculation Example** ğŸ“\n",
    "\n",
    "```\n",
    "Ground Truth: [3, 5, 7, 4, 6]\n",
    "Predictions:  [3, 6, 8, 4, 7]\n",
    "\n",
    "Step 1: Compute statistics\n",
    "  Î¼_y = 5.0,    Î¼_Å· = 5.6\n",
    "  Ïƒ_y = 1.41,   Ïƒ_Å· = 1.85\n",
    "  Ï = 0.996 (from above)\n",
    "\n",
    "Step 2: Apply CCC formula\n",
    "  Numerator = 2 Ã— Ï Ã— Ïƒ_y Ã— Ïƒ_Å·\n",
    "            = 2 Ã— 0.996 Ã— 1.41 Ã— 1.85\n",
    "            = 5.19\n",
    "\n",
    "  Denominator = Ïƒ_yÂ² + Ïƒ_Å·Â² + (Î¼_y - Î¼_Å·)Â²\n",
    "              = 1.41Â² + 1.85Â² + (5.0 - 5.6)Â²\n",
    "              = 1.99 + 3.42 + 0.36\n",
    "              = 5.77\n",
    "\n",
    "  CCC = 5.19 / 5.77 = 0.90\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- **CCC = 1.00**: Perfect agreement\n",
    "- **CCC = 0.90**: Excellent agreement (our example)\n",
    "- **CCC = 0.70**: Good agreement\n",
    "- **CCC = 0.50**: Moderate agreement\n",
    "- **CCC = 0**: No agreement\n",
    "- **CCC < 0**: Worse than no agreement\n",
    "\n",
    "---\n",
    "\n",
    "### **CCC vs. Pearson Correlation: Key Difference** âš¡\n",
    "\n",
    "**Pearson Correlation**: Measures **relationship**, ignores location and scale\n",
    "\n",
    "**Example 1**:\n",
    "```\n",
    "Ground Truth: [1, 2, 3, 4, 5]\n",
    "Predictions:  [2, 3, 4, 5, 6]    â† Shifted by +1\n",
    "\n",
    "Pearson Ï = 1.00  (perfect correlation!)\n",
    "CCC = 0.98        (penalized for shift)\n",
    "```\n",
    "\n",
    "**Example 2**:\n",
    "```\n",
    "Ground Truth: [1, 2, 3, 4, 5]\n",
    "Predictions:  [2, 4, 6, 8, 10]   â† Scaled by 2Ã—\n",
    "\n",
    "Pearson Ï = 1.00  (perfect correlation!)\n",
    "CCC = 0.76        (penalized for scale difference)\n",
    "```\n",
    "\n",
    "**Why CCC is Better for Prediction**:\n",
    "- Pearson: \"Are they related?\"\n",
    "- CCC: \"Do they match?\"\n",
    "\n",
    "For prediction tasks, we want **matching**, not just **relationship**!\n",
    "\n",
    "---\n",
    "\n",
    "### **Valence and Arousal: Two-Dimensional Emotion** ğŸ­\n",
    "\n",
    "**The Circumplex Model of Emotion** (Russell, 1980):\n",
    "\n",
    "```\n",
    "        Arousal\n",
    "           â†‘\n",
    "    Angry  â”‚  Excited\n",
    "     ğŸ˜     â”‚    ğŸ˜ƒ\n",
    "           â”‚\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Valence\n",
    "    Sad    â”‚    Calm\n",
    "     ğŸ˜¢    â”‚    ğŸ˜Œ\n",
    "           â†“\n",
    "```\n",
    "\n",
    "**Valence (X-axis)**:\n",
    "- **Low (1-3)**: Negative, unpleasant, sad\n",
    "- **Mid (4-6)**: Neutral\n",
    "- **High (7-9)**: Positive, pleasant, happy\n",
    "\n",
    "**Arousal (Y-axis)**:\n",
    "- **Low (1-3)**: Calm, relaxed, sleepy\n",
    "- **Mid (4-6)**: Moderate energy\n",
    "- **High (7-9)**: Excited, energetic, alert\n",
    "\n",
    "**Example Emotions**:\n",
    "```\n",
    "Valence  Arousal  Emotion\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  2        8      Angry, Tense\n",
    "  8        8      Excited, Joyful\n",
    "  2        2      Sad, Depressed\n",
    "  8        2      Calm, Content\n",
    "  5        5      Neutral, Ambivalent\n",
    "```\n",
    "\n",
    "**Why Two Dimensions?**:\n",
    "- **Richer** than single \"happiness\" score\n",
    "- **Distinguishes**: Calm-happy vs. Excited-happy\n",
    "- **Psychological basis**: Matches how humans perceive emotions\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpreting Our Metrics in Context** ğŸ¼\n",
    "\n",
    "**Example Results**:\n",
    "```\n",
    "Valence Prediction:\n",
    "  MSE: 0.85    â† Average squared error\n",
    "  MAE: 0.72    â† Average error is Â±0.72 points on 1-9 scale\n",
    "  CCC: 0.81    â† 81% agreement (good!)\n",
    "\n",
    "Arousal Prediction:\n",
    "  MSE: 1.20    â† Slightly worse than valence\n",
    "  MAE: 0.89    â† Average error is Â±0.89 points\n",
    "  CCC: 0.73    â† 73% agreement (decent)\n",
    "```\n",
    "\n",
    "**What This Means**:\n",
    "- On average, valence predictions off by **less than 1 point**\n",
    "- Arousal is **harder to predict** (more subjective)\n",
    "- Model has **good agreement** with human ratings\n",
    "\n",
    "**Real-World Comparison**:\n",
    "```\n",
    "Human-Human Agreement:  CCC â‰ˆ 0.85-0.90  (inter-rater reliability)\n",
    "Our Model:              CCC â‰ˆ 0.73-0.81\n",
    "Simple Baseline:        CCC â‰ˆ 0.20-0.30  (predicting mean)\n",
    "```\n",
    "\n",
    "Our model performs **reasonably close** to human-level agreement!\n",
    "\n",
    "---\n",
    "\n",
    "### **Visualizing Predictions** ğŸ“Š\n",
    "\n",
    "**1. Scatter Plot** (Predicted vs. Actual):\n",
    "\n",
    "```\n",
    "Perfect predictions lie on diagonal line:\n",
    "  y = x  (predicted = actual)\n",
    "\n",
    "Points above line: Over-predictions\n",
    "Points below line: Under-predictions\n",
    "\n",
    "Tighter clustering around line = better model\n",
    "```\n",
    "\n",
    "**2. Residual Plot** (Error vs. Actual):\n",
    "\n",
    "```\n",
    "Residual = Predicted - Actual\n",
    "\n",
    "Ideal: Random scatter around y=0\n",
    "Bad: Systematic pattern (indicates bias)\n",
    "```\n",
    "\n",
    "**3. Valence-Arousal Space**:\n",
    "\n",
    "```\n",
    "Compare distribution of predictions to ground truth:\n",
    "  Should cover same emotional space\n",
    "  Should not cluster in one region\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **Summary: You Now Understand!**\n",
    "\n",
    "**From Sound to Emotion**:\n",
    "1. ğŸµ **Audio waves** â†’ digitized samples\n",
    "2. ğŸ”¬ **Fourier Transform** â†’ frequency spectrum\n",
    "3. ğŸ“Š **Mel-spectrogram** â†’ human-perceptual representation\n",
    "4. ğŸ§© **Patches** â†’ divide into manageable chunks\n",
    "5. ğŸ¤– **Transformer** â†’ learn patterns via self-attention\n",
    "6. ğŸ­ **Regression** â†’ predict valence and arousal\n",
    "7. ğŸ“ **Evaluation** â†’ measure with MSE, MAE, CCC\n",
    "\n",
    "**The Magic of Transformers**:\n",
    "- **Self-attention**: \"Look at all parts, focus on important ones\"\n",
    "- **Parallel processing**: Fast training on GPUs\n",
    "- **Scalability**: Works from small to massive datasets\n",
    "\n",
    "**Training is Optimization**:\n",
    "- **Forward pass**: Make predictions\n",
    "- **Loss function**: Measure errors\n",
    "- **Backpropagation**: Compute how to improve\n",
    "- **Gradient descent**: Actually improve\n",
    "\n",
    "**Evaluation is Multi-Faceted**:\n",
    "- **MSE**: Penalize large errors\n",
    "- **MAE**: Average error magnitude\n",
    "- **CCC**: True agreement measure\n",
    "\n",
    "---\n",
    "\n",
    "Now let's train the model and see these concepts in action! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS AND ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check Kaggle environment\n",
    "print(\"Kaggle Environment Check:\")\n",
    "print(f\"Is Kaggle: {os.path.exists('/kaggle')}\")\n",
    "print(f\"GPU Available: {os.path.exists('/dev/nvidia0')}\")\n",
    "print()\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, SequentialSampler\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Librosa version: {librosa.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a482ddd",
   "metadata": {},
   "source": [
    "## 1. Configuration and Paths\n",
    "\n",
    "**Important for Kaggle Users:**\n",
    "- Make sure you've added the DEAM dataset to your notebook\n",
    "- The dataset should be named: `deam-mediaeval-dataset-emotional-analysis-in-music`\n",
    "- Go to \"Add Data\" â†’ Search for \"DEAM\" â†’ Add to notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f08316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION AND PATHS FOR KAGGLE\n",
    "# ============================================================================\n",
    "\n",
    "# Kaggle paths - using the main DEAM dataset\n",
    "DATASET_PATH = '/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music'\n",
    "AUDIO_DIR = os.path.join(DATASET_PATH, 'DEAM_audio', 'MEMD_audio')\n",
    "ANNOTATIONS_DIR = os.path.join(DATASET_PATH, 'DEAM_Annotations', 'annotations', \n",
    "                               'annotations averaged per song', 'song_level')\n",
    "\n",
    "# Output paths (Kaggle working directory)\n",
    "OUTPUT_DIR = '/kaggle/working'\n",
    "MODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, 'best_ast_model.pth')\n",
    "\n",
    "# Annotation file paths (inside the main DEAM dataset)\n",
    "STATIC_CSV_1_2000 = os.path.join(ANNOTATIONS_DIR, 'static_annotations_averaged_songs_1_2000.csv')\n",
    "STATIC_CSV_2000_2058 = os.path.join(ANNOTATIONS_DIR, 'static_annotations_averaged_songs_2000_2058.csv')\n",
    "\n",
    "# Hyperparameters\n",
    "SEGMENT_LENGTH = 5      # seconds per audio segment\n",
    "SAMPLE_RATE = 44100     # Hz\n",
    "N_MELS = 128            # Mel frequency bins\n",
    "TARGET_TIME_STEPS = 432 # Fixed width for padding (divisible by patch_size)\n",
    "PATCH_SIZE = 16         # Patch size for transformer\n",
    "EMBED_DIM = 256         # Embedding dimension\n",
    "NUM_HEADS = 4           # Number of attention heads\n",
    "NUM_LAYERS = 4          # Number of transformer layers\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 16         # Increased for GPU\n",
    "NUM_EPOCHS = 5          # Reduced for faster training (increase to 15-20 for better results)\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Split ratios\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"AST Model Training - Kaggle Environment\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset Path: {DATASET_PATH}\")\n",
    "print(f\"Audio Directory: {AUDIO_DIR}\")\n",
    "print(f\"Annotations Directory: {ANNOTATIONS_DIR}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"Model Save Path: {MODEL_SAVE_PATH}\")\n",
    "print(f\"\\nâš™ï¸  Training Configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS} (increase to 15-20 for better convergence)\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(\"\\nExpected Kaggle dataset structure:\")\n",
    "print(\"  /kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/\")\n",
    "print(\"    â”œâ”€â”€ DEAM_audio/\")\n",
    "print(\"    â”‚   â””â”€â”€ MEMD_audio/  (audio files)\")\n",
    "print(\"    â”œâ”€â”€ DEAM_Annotations/\")\n",
    "print(\"    â”‚   â””â”€â”€ annotations/\")\n",
    "print(\"    â”‚       â””â”€â”€ annotations averaged per song/\")\n",
    "print(\"    â”‚           â””â”€â”€ song_level/  (CSV files)\")\n",
    "print(\"    â””â”€â”€ features/\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e4082",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify paths exist\n",
    "assert os.path.exists(AUDIO_DIR), f\"Audio directory not found: {AUDIO_DIR}\"\n",
    "assert os.path.exists(STATIC_CSV_1_2000), f\"Annotations file not found: {STATIC_CSV_1_2000}\"\n",
    "assert os.path.exists(STATIC_CSV_2000_2058), f\"Annotations file not found: {STATIC_CSV_2000_2058}\"\n",
    "\n",
    "print(\"âœ“ All paths verified!\")\n",
    "\n",
    "# Load and combine annotation files\n",
    "print(\"\\nLoading annotation files...\")\n",
    "df1 = pd.read_csv(STATIC_CSV_1_2000)\n",
    "df2 = pd.read_csv(STATIC_CSV_2000_2058)\n",
    "df_annotations = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "\n",
    "# Strip whitespace from column names (important!)\n",
    "df_annotations.columns = df_annotations.columns.str.strip()\n",
    "\n",
    "print(f\"Loaded {len(df_annotations)} song annotations\")\n",
    "print(f\"Columns: {df_annotations.columns.tolist()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "df_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b0e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify audio files exist\n",
    "print(\"Verifying audio files...\")\n",
    "available_songs = []\n",
    "missing_songs = []\n",
    "\n",
    "for _, row in df_annotations.iterrows():\n",
    "    song_id = int(row['song_id'])\n",
    "    audio_path = os.path.join(AUDIO_DIR, f\"{song_id}.mp3\")\n",
    "    if os.path.exists(audio_path):\n",
    "        available_songs.append(song_id)\n",
    "    else:\n",
    "        missing_songs.append(song_id)\n",
    "\n",
    "print(f\"Available audio files: {len(available_songs)}\")\n",
    "print(f\"Missing audio files: {len(missing_songs)}\")\n",
    "if len(missing_songs) > 0 and len(missing_songs) < 20:\n",
    "    print(f\"Missing song IDs: {missing_songs}\")\n",
    "\n",
    "# Filter to only available songs\n",
    "df_annotations = df_annotations[df_annotations['song_id'].isin(available_songs)].reset_index(drop=True)\n",
    "print(f\"Final dataset size: {len(df_annotations)} songs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66016d0f",
   "metadata": {},
   "source": [
    "## 3. PyTorch Dataset Class\n",
    "\n",
    "This custom dataset class:\n",
    "1. Loads audio files and splits them into 5-second segments\n",
    "2. Computes mel-spectrograms for each segment\n",
    "3. Pads/truncates to fixed width (432 time steps)\n",
    "4. Optionally applies SpecAugment (data augmentation)\n",
    "5. Returns (spectrogram, [valence, arousal]) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168e4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEAMSpectrogramDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for DEAM audio with mel-spectrogram features.\n",
    "    \"\"\"\n",
    "    def __init__(self, df_annotations, audio_dir, segment_length=5, sr=44100, \n",
    "                 target_time_steps=432, use_aug=False):\n",
    "        self.df = df_annotations.reset_index(drop=True)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.segment_length = segment_length\n",
    "        self.sr = sr\n",
    "        self.segment_samples = segment_length * sr\n",
    "        self.target_time_steps = target_time_steps\n",
    "        self.use_aug = use_aug\n",
    "        \n",
    "        # Pre-compute all valid segment indices\n",
    "        self.segment_indices = []\n",
    "        \n",
    "        print(f\"Initializing dataset (aug={'ON' if use_aug else 'OFF'})...\")\n",
    "        for song_idx in range(len(self.df)):\n",
    "            song_id = int(self.df.iloc[song_idx]['song_id'])\n",
    "            audio_path = os.path.join(self.audio_dir, f\"{song_id}.mp3\")\n",
    "            \n",
    "            try:\n",
    "                y, _ = librosa.load(audio_path, sr=self.sr, mono=True)\n",
    "                num_segments = len(y) // self.segment_samples\n",
    "                \n",
    "                for seg_idx in range(num_segments):\n",
    "                    self.segment_indices.append((song_idx, seg_idx))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading song {song_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Dataset ready: {len(self.segment_indices)} segments\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.segment_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        song_idx, seg_idx = self.segment_indices[idx]\n",
    "        song_id = int(self.df.iloc[song_idx]['song_id'])\n",
    "        \n",
    "        # Load audio\n",
    "        audio_path = os.path.join(self.audio_dir, f\"{song_id}.mp3\")\n",
    "        y, _ = librosa.load(audio_path, sr=self.sr, mono=True)\n",
    "        \n",
    "        # Extract segment\n",
    "        start = seg_idx * self.segment_samples\n",
    "        segment = y[start:start + self.segment_samples]\n",
    "        \n",
    "        # Compute mel-spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=segment, sr=self.sr, n_mels=N_MELS, n_fft=2048, hop_length=512\n",
    "        )\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Pad or truncate to target width\n",
    "        if mel_spec_db.shape[1] < self.target_time_steps:\n",
    "            pad_width = self.target_time_steps - mel_spec_db.shape[1]\n",
    "            mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        elif mel_spec_db.shape[1] > self.target_time_steps:\n",
    "            mel_spec_db = mel_spec_db[:, :self.target_time_steps]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        mel_spec_db = torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # SpecAugment (optional)\n",
    "        if self.use_aug:\n",
    "            freq_mask_param = 10\n",
    "            if N_MELS > freq_mask_param:\n",
    "                freq_mask_start = torch.randint(0, N_MELS - freq_mask_param, (1,)).item()\n",
    "                mel_spec_db[:, freq_mask_start:freq_mask_start + freq_mask_param, :] = 0\n",
    "            \n",
    "            time_mask_param = 20\n",
    "            if self.target_time_steps > time_mask_param:\n",
    "                time_mask_start = torch.randint(0, self.target_time_steps - time_mask_param, (1,)).item()\n",
    "                mel_spec_db[:, :, time_mask_start:time_mask_start + time_mask_param] = 0\n",
    "        \n",
    "        # Get labels\n",
    "        label = torch.tensor([\n",
    "            self.df.iloc[song_idx]['valence_mean'], \n",
    "            self.df.iloc[song_idx]['arousal_mean']\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        return mel_spec_db, label\n",
    "\n",
    "print(\"âœ“ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e1093",
   "metadata": {},
   "source": [
    "## 4. Data Preparation and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e797d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial dataset (no augmentation for computing statistics)\n",
    "full_dataset = DEAMSpectrogramDataset(\n",
    "    df_annotations, AUDIO_DIR, \n",
    "    target_time_steps=TARGET_TIME_STEPS, \n",
    "    use_aug=False\n",
    ")\n",
    "\n",
    "# Split into train/val/test\n",
    "train_size = int(TRAIN_RATIO * len(full_dataset))\n",
    "val_size = int(VAL_RATIO * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"  Train: {train_size} ({100*TRAIN_RATIO:.0f}%)\")\n",
    "print(f\"  Val:   {val_size} ({100*VAL_RATIO:.0f}%)\")\n",
    "print(f\"  Test:  {test_size} ({100*TEST_RATIO:.0f}%)\")\n",
    "\n",
    "# Random split\n",
    "train_dataset_temp, val_dataset_temp, test_dataset_temp = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2154179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute normalization statistics from training set\n",
    "print(\"Computing normalization statistics...\")\n",
    "train_specs_list = []\n",
    "max_samples = min(1000, len(train_dataset_temp))\n",
    "\n",
    "for idx in list(train_dataset_temp.indices)[:max_samples]:\n",
    "    spec, _ = full_dataset[idx]\n",
    "    train_specs_list.append(spec)\n",
    "\n",
    "train_specs = torch.cat(train_specs_list, dim=0)\n",
    "global_mean = train_specs.mean()\n",
    "global_std = train_specs.std()\n",
    "\n",
    "print(f\"Global mean: {global_mean:.4f}\")\n",
    "print(f\"Global std:  {global_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final datasets with augmentation\n",
    "train_indices = train_dataset_temp.indices\n",
    "val_indices = val_dataset_temp.indices\n",
    "test_indices = test_dataset_temp.indices\n",
    "\n",
    "train_dataset = DEAMSpectrogramDataset(\n",
    "    df_annotations, AUDIO_DIR, \n",
    "    target_time_steps=TARGET_TIME_STEPS, \n",
    "    use_aug=True  # Enable augmentation for training\n",
    ")\n",
    "\n",
    "val_dataset = DEAMSpectrogramDataset(\n",
    "    df_annotations, AUDIO_DIR, \n",
    "    target_time_steps=TARGET_TIME_STEPS, \n",
    "    use_aug=False\n",
    ")\n",
    "\n",
    "test_dataset = DEAMSpectrogramDataset(\n",
    "    df_annotations, AUDIO_DIR, \n",
    "    target_time_steps=TARGET_TIME_STEPS, \n",
    "    use_aug=False\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, \n",
    "    sampler=SubsetRandomSampler(train_indices),\n",
    "    num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, \n",
    "    sampler=SequentialSampler(val_indices),\n",
    "    num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, \n",
    "    sampler=SequentialSampler(test_indices),\n",
    "    num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"âœ“ DataLoaders created\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches:   {len(val_loader)}\")\n",
    "print(f\"  Test batches:  {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5379fa",
   "metadata": {},
   "source": [
    "## 5. Model Architecture (Audio Spectrogram Transformer)\n",
    "\n",
    "The AST model architecture:\n",
    "- **Patch Embedding**: Splits spectrogram into 16Ã—16 patches using Conv2D\n",
    "- **Positional Encoding**: Learnable position embeddings\n",
    "- **CLS Token**: Special token for classification (like BERT/ViT)\n",
    "- **Transformer Encoder**: Multi-head self-attention layers\n",
    "- **Regression Head**: MLP to predict valence and arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f77edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramTransformer(nn.Module):\n",
    "    \"\"\"Audio Spectrogram Transformer for emotion recognition.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_height=128, input_width=432, patch_size=16, \n",
    "                 embed_dim=256, num_heads=4, num_layers=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Calculate number of patches\n",
    "        self.num_patches_h = input_height // patch_size  # 8\n",
    "        self.num_patches_w = input_width // patch_size   # 27\n",
    "        num_patches = self.num_patches_h * self.num_patches_w  # 216\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # Learnable CLS token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Positional embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + num_patches, embed_dim))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout, activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Regression head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 2)  # valence, arousal\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "        nn.init.kaiming_normal_(self.patch_embed.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if self.patch_embed.bias is not None:\n",
    "            nn.init.constant_(self.patch_embed.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        \n",
    "        # Prepend CLS token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Extract CLS token\n",
    "        cls_output = x[:, 0]\n",
    "        \n",
    "        # Regression head\n",
    "        predictions = self.head(cls_output)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "print(\"âœ“ Model class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6206c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = SpectrogramTransformer(\n",
    "    input_height=N_MELS,\n",
    "    input_width=TARGET_TIME_STEPS,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b704331",
   "metadata": {},
   "source": [
    "## 6. Training Setup and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57973ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "def concordance_correlation_coefficient(y_true, y_pred):\n",
    "    \"\"\"Concordance Correlation Coefficient for evaluating agreement.\"\"\"\n",
    "    mean_true = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    var_true = np.var(y_true)\n",
    "    var_pred = np.var(y_pred)\n",
    "    \n",
    "    covariance = np.cov(y_true, y_pred)[0, 1] if len(y_true) > 1 else 0\n",
    "    denominator = np.sqrt(var_true * var_pred)\n",
    "    rho = covariance / denominator if denominator > 0 else 0\n",
    "    \n",
    "    numerator = 2 * rho * np.sqrt(var_true) * np.sqrt(var_pred)\n",
    "    denominator = var_true + var_pred + (mean_true - mean_pred) ** 2\n",
    "    ccc = numerator / denominator if denominator > 0 else 0\n",
    "    \n",
    "    return ccc\n",
    "\n",
    "print(\"âœ“ Training setup complete\")\n",
    "print(f\"Loss: MSELoss\")\n",
    "print(f\"Optimizer: AdamW (lr={LEARNING_RATE})\")\n",
    "print(f\"Scheduler: CosineAnnealingLR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b90ab7",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "\n",
    "This will take some time depending on your GPU. On Kaggle with GPU enabled, expect ~30-60 minutes for 15 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    for batch_idx, (specs, labels) in enumerate(train_loader):\n",
    "        # Normalize\n",
    "        specs = (specs - global_mean) / global_std\n",
    "        specs, labels = specs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(specs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "        \n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"  Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    train_loss /= train_batches\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Train Loss: {train_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for specs, labels in val_loader:\n",
    "            specs = (specs - global_mean) / global_std\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(specs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "            \n",
    "            val_preds.append(outputs.cpu().numpy())\n",
    "            val_true.append(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss /= val_batches\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Metrics\n",
    "    val_preds = np.vstack(val_preds)\n",
    "    val_true = np.vstack(val_true)\n",
    "    \n",
    "    mse_valence = mean_squared_error(val_true[:, 0], val_preds[:, 0])\n",
    "    mse_arousal = mean_squared_error(val_true[:, 1], val_preds[:, 1])\n",
    "    ccc_valence = concordance_correlation_coefficient(val_true[:, 0], val_preds[:, 0])\n",
    "    ccc_arousal = concordance_correlation_coefficient(val_true[:, 1], val_preds[:, 1])\n",
    "    \n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Valence - MSE: {mse_valence:.4f}, CCC: {ccc_valence:.4f}\")\n",
    "    print(f\"  Arousal - MSE: {mse_arousal:.4f}, CCC: {ccc_arousal:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'global_mean': global_mean,\n",
    "            'global_std': global_std,\n",
    "        }, MODEL_SAVE_PATH)\n",
    "        print(f\"  âœ“ New best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba317d7",
   "metadata": {},
   "source": [
    "## 8. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90adcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(MODEL_SAVE_PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "test_preds = []\n",
    "test_true = []\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "with torch.no_grad():\n",
    "    for specs, labels in test_loader:\n",
    "        specs = (specs - global_mean) / global_std\n",
    "        specs, labels = specs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(specs)\n",
    "        test_preds.append(outputs.cpu().numpy())\n",
    "        test_true.append(labels.cpu().numpy())\n",
    "\n",
    "test_preds = np.vstack(test_preds)\n",
    "test_true = np.vstack(test_true)\n",
    "\n",
    "# Compute metrics\n",
    "test_mse_valence = mean_squared_error(test_true[:, 0], test_preds[:, 0])\n",
    "test_mse_arousal = mean_squared_error(test_true[:, 1], test_preds[:, 1])\n",
    "test_mae_valence = mean_absolute_error(test_true[:, 0], test_preds[:, 0])\n",
    "test_mae_arousal = mean_absolute_error(test_true[:, 1], test_preds[:, 1])\n",
    "test_ccc_valence = concordance_correlation_coefficient(test_true[:, 0], test_preds[:, 0])\n",
    "test_ccc_arousal = concordance_correlation_coefficient(test_true[:, 1], test_preds[:, 1])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Valence - MSE: {test_mse_valence:.4f}, MAE: {test_mae_valence:.4f}, CCC: {test_ccc_valence:.4f}\")\n",
    "print(f\"Arousal - MSE: {test_mse_arousal:.4f}, MAE: {test_mae_arousal:.4f}, CCC: {test_ccc_arousal:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c1ce0",
   "metadata": {},
   "source": [
    "## 9. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7962270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Training curves\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), train_losses, 'b-', label='Train Loss', linewidth=2)\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), val_losses, 'r-', label='Val Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('MSE Loss', fontsize=12)\n",
    "plt.title('Training and Validation Loss', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Predictions vs Ground Truth\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(test_true[:, 0], test_preds[:, 0], alpha=0.5, label='Valence', s=20, c='blue')\n",
    "plt.scatter(test_true[:, 1], test_preds[:, 1], alpha=0.5, label='Arousal', s=20, c='red')\n",
    "plt.plot([1, 9], [1, 9], 'k--', label='Perfect Prediction', linewidth=2)\n",
    "plt.xlabel('Ground Truth', fontsize=12)\n",
    "plt.ylabel('Prediction', fontsize=12)\n",
    "plt.title('Test Set Predictions', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(1, 9)\n",
    "plt.ylim(1, 9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'training_results.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Training curves saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Valence-Arousal Distribution\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(test_true[:, 0], test_true[:, 1], alpha=0.6, c='blue', \n",
    "            label='Ground Truth', s=40, edgecolors='black', linewidth=0.5)\n",
    "plt.scatter(test_preds[:, 0], test_preds[:, 1], alpha=0.6, c='red', \n",
    "            label='Predictions', s=40, edgecolors='black', linewidth=0.5, marker='^')\n",
    "plt.xlabel('Valence (1-9)', fontsize=14)\n",
    "plt.ylabel('Arousal (1-9)', fontsize=14)\n",
    "plt.title('Valence-Arousal Distribution (Test Set)', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(1, 9)\n",
    "plt.ylim(1, 9)\n",
    "\n",
    "# Add quadrant labels\n",
    "plt.axhline(y=5, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=5, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.text(2.5, 7.5, 'Distressed\\n(Low V, High A)', ha='center', fontsize=10, alpha=0.7)\n",
    "plt.text(6.5, 7.5, 'Excited\\n(High V, High A)', ha='center', fontsize=10, alpha=0.7)\n",
    "plt.text(2.5, 2.5, 'Sad\\n(Low V, Low A)', ha='center', fontsize=10, alpha=0.7)\n",
    "plt.text(6.5, 2.5, 'Calm\\n(High V, Low A)', ha='center', fontsize=10, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'valence_arousal_distribution.png'), \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Valence-Arousal plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6edb27e",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "âœ… Loaded and preprocessed the DEAM dataset  \n",
    "âœ… Created mel-spectrogram features from audio  \n",
    "âœ… Built an Audio Spectrogram Transformer model  \n",
    "âœ… Trained the model with data augmentation  \n",
    "âœ… Evaluated on test set with multiple metrics  \n",
    "âœ… Visualized predictions and training progress  \n",
    "\n",
    "### Model Performance\n",
    "The model predicts **valence** (happiness) and **arousal** (energy) on a 1-9 scale.\n",
    "\n",
    "**Key Metrics:**\n",
    "- **MSE (Mean Squared Error)**: Lower is better\n",
    "- **MAE (Mean Absolute Error)**: Average prediction error\n",
    "- **CCC (Concordance Correlation Coefficient)**: Agreement measure (-1 to 1, higher is better)\n",
    "\n",
    "### Saved Outputs\n",
    "All outputs are saved to `/kaggle/working/`:\n",
    "- `best_ast_model.pth` - Trained model checkpoint\n",
    "- `training_results.png` - Training curves\n",
    "- `valence_arousal_distribution.png` - Prediction visualization\n",
    "\n",
    "### Next Steps\n",
    "1. **Fine-tune hyperparameters**: Try different learning rates, batch sizes, or model architectures\n",
    "2. **Increase training epochs**: Train longer for potentially better performance\n",
    "3. **Try transfer learning**: Use pre-trained audio models\n",
    "4. **Ensemble models**: Combine multiple models for better predictions\n",
    "5. **Deploy the model**: Use the saved checkpoint for inference on new audio files\n",
    "\n",
    "### How to Use the Saved Model\n",
    "\n",
    "```python\n",
    "# Load the model\n",
    "checkpoint = torch.load('/kaggle/working/best_ast_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Inference on new audio\n",
    "# ... (compute mel-spectrogram, normalize, predict)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
