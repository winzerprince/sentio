{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99ecb54",
   "metadata": {},
   "source": [
    "# Dynamic Emotion Prediction with LSTMs\n",
    "\n",
    "This notebook focuses on training a dynamic emotion prediction model using a Long Short-Term Memory (LSTM) network. Unlike static models that predict a single emotion value for an entire track, dynamic models predict how emotion (valence and arousal) evolves over time.\n",
    "\n",
    "### Pipeline:\n",
    "1.  **Load Data**: Load time-series features and corresponding dynamic emotion annotations.\n",
    "2.  **Preprocess Data**: Create sequences from the time-series data suitable for an LSTM.\n",
    "3.  **Build Model**: Construct an LSTM model using Keras/TensorFlow.\n",
    "4.  **Train Model**: Train the model on the sequenced data.\n",
    "5.  **Evaluate Model**: Evaluate the model's performance on a test set.\n",
    "6.  **Visualize Results**: Plot the predicted vs. actual emotion values over time for a sample song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5bc540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup: Import Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "\n",
    "# Setup plotting style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 7)\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5c6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Loading\n",
    "ANNOTATIONS_PATH_1 = \"../dataset/DEAM/annotations/annotations per song/dynamic_annotations_averaged_songs_1_2000.csv\"\n",
    "ANNOTATIONS_PATH_2 = \"../dataset/DEAM/annotations/annotations per song/dynamic_annotations_averaged_songs_2000_2058.csv\"\n",
    "FEATURES_DIR = \"../selected\"\n",
    "\n",
    "def load_dynamic_annotations():\n",
    "    \"\"\"Loads and merges dynamic annotation files.\"\"\"\n",
    "    try:\n",
    "        ann1 = pd.read_csv(ANNOTATIONS_PATH_1)\n",
    "        ann2 = pd.read_csv(ANNOTATIONS_PATH_2)\n",
    "        \n",
    "        # Clean column names\n",
    "        ann1.columns = ann1.columns.str.strip()\n",
    "        ann2.columns = ann2.columns.str.strip()\n",
    "        \n",
    "        annotations = pd.concat([ann1, ann2], ignore_index=True)\n",
    "        \n",
    "        # Scale annotations from [1,9] to [-1,1]\n",
    "        for col in ['valence_mean', 'arousal_mean']:\n",
    "            annotations[col] = (annotations[col] - 5.0) / 4.0\n",
    "            \n",
    "        annotations = annotations.rename(columns={\n",
    "            'valence_mean': 'valence',\n",
    "            'arousal_mean': 'arousal'\n",
    "        })\n",
    "        \n",
    "        annotations['song_id'] = annotations['song_id'].astype(str)\n",
    "        return annotations\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading annotations: {e}\")\n",
    "        return None\n",
    "\n",
    "dynamic_annotations_df = load_dynamic_annotations()\n",
    "\n",
    "if dynamic_annotations_df is not None:\n",
    "    print(\"Dynamic annotations loaded successfully.\")\n",
    "    display(dynamic_annotations_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d3054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data Processing: Create Sequences for LSTM\n",
    "\n",
    "def create_sequences(features, labels, sequence_length=10):\n",
    "    \"\"\"Creates overlapping sequences of features and labels.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - sequence_length):\n",
    "        X.append(features[i:(i + sequence_length)])\n",
    "        y.append(labels[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def process_all_songs(feature_dir, annotations_df, sequence_length=10, max_songs=50):\n",
    "    \"\"\"Processes all songs to create a dataset of sequences.\"\"\"\n",
    "    all_X, all_y = [], []\n",
    "    \n",
    "    feature_files = glob(os.path.join(feature_dir, \"*_selected.csv\"))[:max_songs]\n",
    "    \n",
    "    for file_path in feature_files:\n",
    "        song_id = os.path.basename(file_path).split('_')[0]\n",
    "        \n",
    "        # Get features and labels for the current song\n",
    "        song_features_df = pd.read_csv(file_path)\n",
    "        song_labels_df = annotations_df[annotations_df['song_id'] == song_id]\n",
    "        \n",
    "        if song_labels_df.empty:\n",
    "            continue\n",
    "            \n",
    "        # Align features and labels based on time\n",
    "        # This is a simplified alignment; a more robust solution would interpolate\n",
    "        merged_data = pd.merge_asof(song_features_df.sort_values('frameTime'), \n",
    "                                    song_labels_df.sort_values('time'), \n",
    "                                    left_on='frameTime', \n",
    "                                    right_on='time', \n",
    "                                    direction='nearest')\n",
    "        \n",
    "        feature_cols = [col for col in song_features_df.columns if col != 'frameTime']\n",
    "        label_cols = ['valence', 'arousal']\n",
    "        \n",
    "        features = merged_data[feature_cols].values\n",
    "        labels = merged_data[label_cols].values\n",
    "        \n",
    "        # Create sequences for this song\n",
    "        X_song, y_song = create_sequences(features, labels, sequence_length)\n",
    "        \n",
    "        if X_song.shape[0] > 0:\n",
    "            all_X.append(X_song)\n",
    "            all_y.append(y_song)\n",
    "            \n",
    "    return np.vstack(all_X), np.vstack(all_y)\n",
    "\n",
    "# Create the dataset\n",
    "# Using a subset of songs (max_songs=50) to keep processing time reasonable\n",
    "X_seq, y_seq = process_all_songs(FEATURES_DIR, dynamic_annotations_df, sequence_length=10, max_songs=50)\n",
    "\n",
    "print(f\"Sequenced data shapes: X={X_seq.shape}, y={y_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67570dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Training\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "# Reshape data for scaler: from (n_samples, n_timesteps, n_features) to (n_samples * n_timesteps, n_features)\n",
    "nsamples, nx, ny = X_train.shape\n",
    "X_train_reshaped = X_train.reshape((nsamples * nx, ny))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_reshaped = scaler.fit_transform(X_train_reshaped)\n",
    "\n",
    "# Reshape back to original\n",
    "X_train_scaled = X_train_scaled_reshaped.reshape(X_train.shape)\n",
    "\n",
    "# Scale test data\n",
    "nsamples_test, nx_test, ny_test = X_test.shape\n",
    "X_test_reshaped = X_test.reshape((nsamples_test * nx_test, ny_test))\n",
    "X_test_scaled_reshaped = scaler.transform(X_test_reshaped)\n",
    "X_test_scaled = X_test_scaled_reshaped.reshape(X_test.shape)\n",
    "\n",
    "print(\"Data scaled and ready for training.\")\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])),\n",
    "    LSTM(32),\n",
    "    Dense(2)  # Output layer for valence and arousal\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=20, batch_size=64, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968aaefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Evaluation\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate R² score and RMSE\n",
    "r2_valence = r2_score(y_test[:, 0], y_pred[:, 0])\n",
    "r2_arousal = r2_score(y_test[:, 1], y_pred[:, 1])\n",
    "rmse_valence = np.sqrt(mean_squared_error(y_test[:, 0], y_pred[:, 0]))\n",
    "rmse_arousal = np.sqrt(mean_squared_error(y_test[:, 1], y_pred[:, 1]))\n",
    "\n",
    "print(\"Model Evaluation:\")\n",
    "print(f\"Valence - R²: {r2_valence:.4f}, RMSE: {rmse_valence:.4f}\")\n",
    "print(f\"Arousal - R²: {r2_arousal:.4f}, RMSE: {rmse_arousal:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
