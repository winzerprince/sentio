{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ffc7247",
   "metadata": {},
   "source": [
    "# Analysis of Emotion Prediction Models\n",
    "\n",
    "This notebook loads the evaluation results from our model training script (`train_evaluate_models.py`), visualizes the performance metrics, and draws conclusions about which model is best suited for predicting valence and arousal from music features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c111cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the evaluation data\n",
    "EVALUATION_FILE = '../results/model_evaluations.json'\n",
    "with open(EVALUATION_FILE, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the nested dictionary to a more usable DataFrame format\n",
    "records = []\n",
    "for model, metrics in data.items():\n",
    "    for dimension, scores in metrics.items():\n",
    "        if dimension in ['valence', 'arousal']:\n",
    "            records.append({\n",
    "                'model': model,\n",
    "                'dimension': dimension,\n",
    "                'r2_score': scores['r2_score'],\n",
    "                'mse': scores['mse'],\n",
    "                'mae': scores['mae']\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263f1dc8",
   "metadata": {},
   "source": [
    "## 1. R² Score Comparison\n",
    "\n",
    "The R² score (coefficient of determination) represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). A score closer to 1 indicates a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "# Valence R² Scores\n",
    "sns.barplot(data=df[df['dimension'] == 'valence'], x='model', y='r2_score', ax=ax[0], palette='viridis')\n",
    "ax[0].set_title('R² Scores for Valence Prediction')\n",
    "ax[0].set_xlabel('Model')\n",
    "ax[0].set_ylabel('R² Score')\n",
    "ax[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Arousal R² Scores\n",
    "sns.barplot(data=df[df['dimension'] == 'arousal'], x='model', y='r2_score', ax=ax[1], palette='plasma')\n",
    "ax[1].set_title('R² Scores for Arousal Prediction')\n",
    "ax[1].set_xlabel('Model')\n",
    "ax[1].set_ylabel('')\n",
    "\n",
    "plt.suptitle('Model Performance: R² Score Comparison', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4757c9f3",
   "metadata": {},
   "source": [
    "## 2. Error Metrics Comparison (MSE & MAE)\n",
    "\n",
    "Mean Squared Error (MSE) and Mean Absolute Error (MAE) are two common metrics for measuring the average errors between predicted and actual values. Lower values are better.\n",
    "\n",
    "- **MAE**: Represents the average absolute difference between the predicted and actual values. It's less sensitive to outliers.\n",
    "- **MSE**: Represents the average of the squared differences. It penalizes larger errors more heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Valence Error Metrics\n",
    "sns.barplot(data=df[df['dimension'] == 'valence'], x='model', y='mse', ax=ax[0, 0], palette='coolwarm')\n",
    "ax[0, 0].set_title('MSE for Valence Prediction')\n",
    "ax[0, 0].set_xlabel('')\n",
    "ax[0, 0].set_ylabel('MSE')\n",
    "ax[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "sns.barplot(data=df[df['dimension'] == 'valence'], x='model', y='mae', ax=ax[0, 1], palette='coolwarm')\n",
    "ax[0, 1].set_title('MAE for Valence Prediction')\n",
    "ax[0, 1].set_xlabel('')\n",
    "ax[0, 1].set_ylabel('MAE')\n",
    "ax[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Arousal Error Metrics\n",
    "sns.barplot(data=df[df['dimension'] == 'arousal'], x='model', y='mse', ax=ax[1, 0], palette='RdYlGn')\n",
    "ax[1, 0].set_title('MSE for Arousal Prediction')\n",
    "ax[1, 0].set_xlabel('Model')\n",
    "ax[1, 0].set_ylabel('MSE')\n",
    "ax[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "sns.barplot(data=df[df['dimension'] == 'arousal'], x='model', y='mae', ax=ax[1, 1], palette='RdYlGn')\n",
    "ax[1, 1].set_title('MAE for Arousal Prediction')\n",
    "ax[1, 1].set_xlabel('Model')\n",
    "ax[1, 1].set_ylabel('MAE')\n",
    "ax[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('Model Performance: Error Metrics Comparison', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356e48d9",
   "metadata": {},
   "source": [
    "## 3. Conclusion\n",
    "\n",
    "Based on the evaluation metrics:\n",
    "\n",
    "- **Best Overall Performance (R² Score):** The **XGBoost** model consistently provides the highest R² scores for both valence and arousal, indicating it explains the most variance in the data. This makes it the most accurate model overall.\n",
    "\n",
    "- **Error Rates:** XGBoost also shows the lowest MSE and MAE, reinforcing its position as the top performer.\n",
    "\n",
    "- **Model Trade-offs:**\n",
    "  - **Ridge** and **SVR** offer a reasonable balance. Their performance is not far behind XGBoost, and they are typically faster to train. They represent a good baseline and could be suitable for applications where training time is a major constraint.\n",
    "  - **MLP (Multi-layer Perceptron)** performed the poorest in this configuration. Its R² scores are significantly lower, and its error rates are higher. This could be due to the relatively small dataset size or the need for more extensive hyperparameter tuning.\n",
    "\n",
    "**Final Recommendation:** For the highest predictive accuracy in this task, **XGBoost is the recommended model**. If computational resources or training time are a concern, **Ridge** provides a viable and efficient alternative."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
