{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:22:06.566328Z",
     "start_time": "2025-09-16T14:22:06.518282Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 7, 8, 10, 12, 13, 17]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_song_ids() -> list[int]:\n",
    "    \"\"\"\n",
    "    make sure the annotations folder is in your root directory.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "\n",
    "    ROOT = Path(__file__).resolve().parent\n",
    "    static_1_2000 = (\n",
    "        ROOT\n",
    "        / \"annotations\"\n",
    "        / \"annotations averaged per song\"\n",
    "        / \"song_level\"\n",
    "        / \"static_annotations_averaged_songs_1_2000.csv\"\n",
    "    )\n",
    "    static_2000_2058 = (\n",
    "        ROOT\n",
    "        / \"annotations\"\n",
    "        / \"annotations averaged per song\"\n",
    "        / \"song_level\"\n",
    "        / \"static_annotations_averaged_songs_2000_2058.csv\"\n",
    "    )\n",
    "    ids_1_2000 = pd.read_csv(static_1_2000)[\"song_id\"].tolist()\n",
    "    ids_2000_2058 = pd.read_csv(static_2000_2058)[\"song_id\"].tolist()\n",
    "    return ids_1_2000 + ids_2000_2058\n",
    "\n",
    "\n",
    "# getting song ids and selected_features (40)\n",
    "# aiming at producing X: (n_songs, timesteps, n_features) and y: (n_songs, 2) - valence and arousal features44\n",
    "# sequence_lengths: (n_songs,) - original lengths before padding\n",
    "\n",
    "song_ids = get_song_ids()\n",
    "song_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86241b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0final_sma_stddev',\n",
       " 'F0final_sma_amean',\n",
       " 'F0final_sma_de_stddev',\n",
       " 'F0final_sma_de_amean',\n",
       " 'pcm_RMSenergy_sma_stddev',\n",
       " 'pcm_RMSenergy_sma_amean',\n",
       " 'pcm_RMSenergy_sma_de_stddev',\n",
       " 'pcm_RMSenergy_sma_de_amean',\n",
       " 'pcm_fftMag_spectralRollOff75.0_sma_stddev',\n",
       " 'pcm_fftMag_spectralRollOff75.0_sma_amean',\n",
       " 'pcm_fftMag_spectralRollOff90.0_sma_stddev',\n",
       " 'pcm_fftMag_spectralRollOff90.0_sma_amean',\n",
       " 'pcm_fftMag_spectralRollOff75.0_sma_de_stddev',\n",
       " 'pcm_fftMag_spectralRollOff75.0_sma_de_amean',\n",
       " 'pcm_fftMag_spectralRollOff90.0_sma_de_stddev',\n",
       " 'pcm_fftMag_spectralRollOff90.0_sma_de_amean',\n",
       " 'pcm_fftMag_spectralFlux_sma_stddev',\n",
       " 'pcm_fftMag_spectralFlux_sma_amean',\n",
       " 'pcm_fftMag_spectralFlux_sma_de_stddev',\n",
       " 'pcm_fftMag_spectralFlux_sma_de_amean',\n",
       " 'pcm_fftMag_spectralCentroid_sma_stddev',\n",
       " 'pcm_fftMag_spectralCentroid_sma_amean',\n",
       " 'pcm_fftMag_spectralCentroid_sma_de_stddev',\n",
       " 'pcm_fftMag_spectralCentroid_sma_de_amean',\n",
       " 'pcm_fftMag_spectralHarmonicity_sma_stddev',\n",
       " 'pcm_fftMag_spectralHarmonicity_sma_amean',\n",
       " 'pcm_fftMag_spectralHarmonicity_sma_de_stddev',\n",
       " 'pcm_fftMag_spectralHarmonicity_sma_de_amean',\n",
       " 'pcm_fftMag_mfcc_sma[1]_stddev',\n",
       " 'pcm_fftMag_mfcc_sma[1]_amean',\n",
       " 'pcm_fftMag_mfcc_sma[2]_stddev',\n",
       " 'pcm_fftMag_mfcc_sma[2]_amean',\n",
       " 'pcm_fftMag_mfcc_sma[3]_stddev',\n",
       " 'pcm_fftMag_mfcc_sma[3]_amean',\n",
       " 'pcm_fftMag_mfcc_sma_de[1]_stddev',\n",
       " 'pcm_fftMag_mfcc_sma_de[1]_amean',\n",
       " 'pcm_fftMag_mfcc_sma_de[2]_stddev',\n",
       " 'pcm_fftMag_mfcc_sma_de[2]_amean',\n",
       " 'pcm_fftMag_mfcc_sma_de[3]_stddev',\n",
       " 'pcm_fftMag_mfcc_sma_de[3]_amean']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../selected/_2_selected.csv\")\n",
    "selected_features: list[str] = df.columns.values[1:].tolist()\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc7aa33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>valence_mean</th>\n",
       "      <th>valence_std</th>\n",
       "      <th>arousal_mean</th>\n",
       "      <th>arousal_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>1998</td>\n",
       "      <td>6.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      song_id  valence_mean  valence_std  arousal_mean  arousal_std\n",
       "1741     1998           6.4          1.5           6.2          1.6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"\").resolve().parent\n",
    "annots_path_1 = (\n",
    "    root\n",
    "    / \"annotations\"\n",
    "    / \"annotations averaged per song\"\n",
    "    / \"song_level\"\n",
    "    / \"static_annotations_averaged_songs_1_2000.csv\"\n",
    ")\n",
    "annots_path_2 = root / \"notebooks\" / \"static_annots_2058.csv\"\n",
    "df_1 = pd.read_csv(annots_path_1)\n",
    "df_2 = pd.read_csv(annots_path_2)\n",
    "\n",
    "annotations = pd.concat([df_1, df_2], axis=0)\n",
    "annotations[annotations[\"song_id\"] == 1998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08628bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs analyzed: 100\n",
      "Min length: 156\n",
      "Max length: 1071\n",
      "Mean length: 448.6\n",
      "Median length: 427.5\n",
      "25th percentile: 336.5\n",
      "75th percentile: 521.2\n",
      "95th percentile: 769.2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[449,\n",
       " 441,\n",
       " 456,\n",
       " 332,\n",
       " 240,\n",
       " 239,\n",
       " 290,\n",
       " 248,\n",
       " 342,\n",
       " 388,\n",
       " 460,\n",
       " 429,\n",
       " 620,\n",
       " 831,\n",
       " 453,\n",
       " 271,\n",
       " 238,\n",
       " 374,\n",
       " 295,\n",
       " 546,\n",
       " 176,\n",
       " 371,\n",
       " 588,\n",
       " 419,\n",
       " 437,\n",
       " 427,\n",
       " 505,\n",
       " 543,\n",
       " 1071,\n",
       " 557,\n",
       " 465,\n",
       " 337,\n",
       " 453,\n",
       " 415,\n",
       " 407,\n",
       " 428,\n",
       " 464,\n",
       " 390,\n",
       " 605,\n",
       " 156,\n",
       " 335,\n",
       " 717,\n",
       " 304,\n",
       " 487,\n",
       " 175,\n",
       " 451,\n",
       " 903,\n",
       " 282,\n",
       " 532,\n",
       " 347,\n",
       " 541,\n",
       " 631,\n",
       " 428,\n",
       " 718,\n",
       " 427,\n",
       " 341,\n",
       " 767,\n",
       " 480,\n",
       " 391,\n",
       " 289,\n",
       " 305,\n",
       " 293,\n",
       " 508,\n",
       " 538,\n",
       " 291,\n",
       " 458,\n",
       " 383,\n",
       " 522,\n",
       " 514,\n",
       " 1038,\n",
       " 537,\n",
       " 723,\n",
       " 325,\n",
       " 431,\n",
       " 422,\n",
       " 488,\n",
       " 521,\n",
       " 354,\n",
       " 446,\n",
       " 665,\n",
       " 382,\n",
       " 403,\n",
       " 393,\n",
       " 295,\n",
       " 463,\n",
       " 272,\n",
       " 363,\n",
       " 461,\n",
       " 389,\n",
       " 246,\n",
       " 244,\n",
       " 410,\n",
       " 572,\n",
       " 617,\n",
       " 811,\n",
       " 368,\n",
       " 323,\n",
       " 757,\n",
       " 352,\n",
       " 279]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's analyze sequence lengths to determine optimal min/max\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# First, let's get all sequence lengths\n",
    "all_lengths = []\n",
    "for song_id in song_ids[:100]:  # Sample first 100 songs to avoid memory issues\n",
    "    try:\n",
    "        song_data = pd.read_csv(f\"../selected/_{song_id}_selected.csv\")\n",
    "        all_lengths.append(len(song_data))\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "# Analysis of sequence lengths\n",
    "print(f\"Number of songs analyzed: {len(all_lengths)}\")\n",
    "print(f\"Min length: {min(all_lengths)}\")\n",
    "print(f\"Max length: {max(all_lengths)}\")\n",
    "print(f\"Mean length: {np.mean(all_lengths):.1f}\")\n",
    "print(f\"Median length: {np.median(all_lengths):.1f}\")\n",
    "print(f\"25th percentile: {np.percentile(all_lengths, 25):.1f}\")\n",
    "print(f\"75th percentile: {np.percentile(all_lengths, 75):.1f}\")\n",
    "print(f\"95th percentile: {np.percentile(all_lengths, 95):.1f}\")\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_lengths, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(np.mean(all_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(all_lengths):.1f}')\n",
    "plt.axvline(np.median(all_lengths), color='green', linestyle='--', label=f'Median: {np.median(all_lengths):.1f}')\n",
    "plt.axvline(np.percentile(all_lengths, 95), color='orange', linestyle='--', label=f'95th percentile: {np.percentile(all_lengths, 95):.1f}')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Song Sequence Lengths')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "all_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55923c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed: 1802 songs\n",
      "Skipped songs: 0\n",
      "Final shapes:\n",
      "  X (features): (1802, 600, 40)\n",
      "  y (labels): (1802, 2)\n",
      "  sequence_lengths: (1802,)\n",
      "\n",
      "Skipped songs breakdown:\n",
      "\n",
      "Sequence length stats for processed songs:\n",
      "  Min original length: 88\n",
      "  Max original length: 1253\n",
      "  Mean original length: 254.0\n",
      "  Songs that needed padding: 1627\n",
      "  Songs that were truncated: 175\n",
      "(1802, 600, 40)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           2.253395e-01,  1.342798e+00,  2.761010e-01],\n",
       "         [ 4.661850e+01,  4.197005e+01,  1.818014e-02, ...,\n",
       "           2.823714e-01,  1.540742e+00,  2.085407e-01],\n",
       "         [ 2.641163e+01,  8.331398e+01,  2.521972e-02, ...,\n",
       "          -3.895111e-02,  1.353868e+00, -4.939437e-02],\n",
       "         ...,\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00],\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00],\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00]],\n",
       " \n",
       "        [[ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00],\n",
       "         [ 2.453278e+01,  1.491984e+01,  3.104210e-04, ...,\n",
       "           7.940001e-02,  1.242938e+00,  3.255248e-01],\n",
       "         [ 2.322386e+01,  4.249227e+01,  3.569493e-04, ...,\n",
       "           7.041288e-02,  1.429126e+00,  2.425990e-01],\n",
       "         ...,\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00],\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00],\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00]],\n",
       " \n",
       "        [[ 6.897192e+01,  1.675522e+02,  7.068392e-02, ...,\n",
       "           8.457641e-02,  1.168742e+00,  1.042426e-01],\n",
       "         [ 4.775576e+01,  9.183339e+01,  1.747710e-02, ...,\n",
       "          -3.055232e-01,  2.242379e+00,  3.321606e-02],\n",
       "         [ 8.362995e+01,  1.318416e+02,  1.983808e-02, ...,\n",
       "          -7.542915e-02,  2.191911e+00,  3.729563e-02],\n",
       "         ...,\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00],\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00],\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 1.870674e+02,  1.381008e+02,  7.401673e-01, ...,\n",
       "          -6.266186e-02,  1.893708e+00,  8.958523e-02],\n",
       "         [ 2.184320e+02,  1.327693e+02,  2.371935e-02, ...,\n",
       "           4.772130e-02,  1.921510e+00,  9.306734e-03],\n",
       "         [ 1.589192e+02,  7.167239e+01,  5.274361e-04, ...,\n",
       "          -1.318398e-02,  1.601119e+00,  2.458643e-02],\n",
       "         ...,\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00],\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00],\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00]],\n",
       " \n",
       "        [[ 7.050588e+01,  1.556148e+02,  2.945876e-01, ...,\n",
       "          -1.875552e-01,  1.927957e+00,  2.560833e-01],\n",
       "         [ 5.503167e+01,  1.758134e+02,  5.157623e-02, ...,\n",
       "           2.639418e-01,  1.194232e+00, -1.451042e-01],\n",
       "         [ 6.032006e+01,  1.461982e+02,  3.645491e-02, ...,\n",
       "           2.559165e-01,  1.604174e+00, -3.558800e-02],\n",
       "         ...,\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00],\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00],\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00]],\n",
       " \n",
       "        [[ 1.032877e+02,  3.150862e+02,  1.011627e+00, ...,\n",
       "          -3.139796e-01,  2.103998e+00,  1.247190e-01],\n",
       "         [ 9.753291e+01,  3.601447e+02,  8.524803e-03, ...,\n",
       "           1.934584e-02,  1.317113e+00, -6.135808e-02],\n",
       "         [ 9.253907e+01,  4.384642e+02,  5.332451e-03, ...,\n",
       "          -1.412203e-01,  1.171191e+00, -6.597140e-02],\n",
       "         ...,\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00],\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00],\n",
       "         [ 0.000000e+00,  0.000000e+00,  0.000000e+00, ...,\n",
       "           0.000000e+00,  0.000000e+00,  0.000000e+00]]],\n",
       "       shape=(1802, 600, 40)),\n",
       " array([[3.1 , 3.  ],\n",
       "        [3.5 , 3.3 ],\n",
       "        [5.7 , 5.5 ],\n",
       "        ...,\n",
       "        [5.  , 4.6 ],\n",
       "        [3.17, 6.83],\n",
       "        [3.8 , 5.8 ]], shape=(1802, 2)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "labels = []\n",
    "sequence_lengths = []\n",
    "MAX_SEQ_LEN = 600  # Based on analysis - captures ~85-90% of data\n",
    "MIN_SEQ_LEN = 50   # Include very short sequences - even snippet-like audio\n",
    "\n",
    "skipped_songs = []\n",
    "processed_songs = 0\n",
    "\n",
    "for song_id in song_ids:\n",
    "    try:\n",
    "        song_data = pd.read_csv(f\"../selected/_{song_id}_selected.csv\")\n",
    "        feat_seq = song_data[selected_features].values  # shape (time_steps, n_selected_features)\n",
    "        original_length = len(feat_seq)\n",
    "        \n",
    "        # Skip only extremely short songs (less than 50 timesteps)\n",
    "        if original_length < MIN_SEQ_LEN:\n",
    "            skipped_songs.append((song_id, original_length, \"too_short\"))\n",
    "            continue\n",
    "            \n",
    "        # Handle variable lengths: pad or truncate to MAX_SEQ_LEN\n",
    "        if original_length < MAX_SEQ_LEN:\n",
    "            # Pad with zeros at the end (this now includes short songs)\n",
    "            padding = np.zeros((MAX_SEQ_LEN - original_length, feat_seq.shape[1]))\n",
    "            feat_seq = np.vstack([feat_seq, padding])\n",
    "        else:\n",
    "            # Truncate to MAX_SEQ_LEN\n",
    "            feat_seq = feat_seq[:MAX_SEQ_LEN]\n",
    "        \n",
    "        sequences.append(feat_seq)\n",
    "        sequence_lengths.append(original_length)  # Store original length\n",
    "        \n",
    "        # Get annotations\n",
    "        song_annotations = annotations[annotations['song_id'] == song_id]\n",
    "        if len(song_annotations) == 0:\n",
    "            skipped_songs.append((song_id, original_length, \"no_annotations\"))\n",
    "            sequences.pop()  # Remove the sequence we just added\n",
    "            sequence_lengths.pop()\n",
    "            continue\n",
    "            \n",
    "        valence = song_annotations['valence_mean'].iloc[0]\n",
    "        arousal = song_annotations['arousal_mean'].iloc[0]\n",
    "        labels.append([valence, arousal])\n",
    "        \n",
    "        processed_songs += 1\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        skipped_songs.append((song_id, 0, \"file_not_found\"))\n",
    "        continue\n",
    "\n",
    "# Now create arrays\n",
    "X = np.array(sequences)  # Shape: (n_songs, MAX_SEQ_LEN, n_features)\n",
    "y = np.array(labels)     # Shape: (n_songs, 2)\n",
    "sequence_lengths = np.array(sequence_lengths)  # Shape: (n_songs,)\n",
    "\n",
    "print(f\"Successfully processed: {processed_songs} songs\")\n",
    "print(f\"Skipped songs: {len(skipped_songs)}\")\n",
    "print(f\"Final shapes:\")\n",
    "print(f\"  X (features): {X.shape}\")\n",
    "print(f\"  y (labels): {y.shape}\")\n",
    "print(f\"  sequence_lengths: {sequence_lengths.shape}\")\n",
    "\n",
    "# Show some statistics\n",
    "print(f\"\\nSkipped songs breakdown:\")\n",
    "skip_reasons = {}\n",
    "for _, _, reason in skipped_songs:\n",
    "    skip_reasons[reason] = skip_reasons.get(reason, 0) + 1\n",
    "for reason, count in skip_reasons.items():\n",
    "    print(f\"  {reason}: {count}\")\n",
    "\n",
    "print(f\"\\nSequence length stats for processed songs:\")\n",
    "print(f\"  Min original length: {sequence_lengths.min()}\")\n",
    "print(f\"  Max original length: {sequence_lengths.max()}\")\n",
    "print(f\"  Mean original length: {sequence_lengths.mean():.1f}\")\n",
    "print(f\"  Songs that needed padding: {sum(sequence_lengths < MAX_SEQ_LEN)}\")\n",
    "print(f\"  Songs that were truncated: {sum(sequence_lengths > MAX_SEQ_LEN)}\")\n",
    "print(X.shape)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "713602dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# preprocessing sequences for CRNN training.\n",
    "# X: raw feature sequences\n",
    "# y: emotion labels (valence, arousal)\n",
    "# sequence_lengths: origis sequence length\n",
    "\n",
    "# taking in song_ids and selcted features\n",
    "n_songs , max_time_steps, n_features = X.shape\n",
    "\n",
    "# normalize \n",
    "feature_scalers = {}\n",
    "X_normalized = X.copy()\n",
    "\n",
    "for feature_index in range(n_features):\n",
    "    # Collect all real (non-padded) values for this feature across all songs\n",
    "    real_values = []\n",
    "    for song_idx in range(n_songs):\n",
    "        seq_len = sequence_lengths[song_idx]\n",
    "        # Use .tolist() to append values from ndarray to list\n",
    "        real_values += X[song_idx, :seq_len, feature_index].tolist()\n",
    "\n",
    "    # scaler to real values only\n",
    "    scaler = StandardScaler()\n",
    "    real_values = np.array(real_values).reshape(-1, 1)\n",
    "    scaler.fit(real_values)\n",
    "    \n",
    "    # scaler to all values\n",
    "    X_normalized[:, :, feature_index] = scaler.transform(\n",
    "        X[:, :, feature_index].reshape(-1, 1)\n",
    "    ).reshape(n_songs, max_time_steps) # reshape to the original features shape\n",
    "    \n",
    "    feature_scalers[feature_index] = scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87543eba",
   "metadata": {},
   "source": [
    "# NumPy Indexing and Flattening Explanation\n",
    "\n",
    "Let's break down the indexing notation `X[:, :, feature_index]` and the flattening operation:\n",
    "\n",
    "## NumPy Array Indexing with Colons (:)\n",
    "\n",
    "Your data `X` has shape `(1802, 600, 40)` which means:\n",
    "- **Dimension 0**: 1802 songs\n",
    "- **Dimension 1**: 600 timesteps \n",
    "- **Dimension 2**: 40 features\n",
    "\n",
    "The notation `X[:, :, feature_index]` means:\n",
    "- **First `:`**: Take ALL songs (all 1802 songs)\n",
    "- **Second `:`**: Take ALL timesteps (all 600 timesteps) \n",
    "- **`feature_index`**: Take only ONE specific feature (e.g., feature 0, 1, 2, etc.)\n",
    "\n",
    "So `X[:, :, 0]` gives you feature #0 for all songs and all timesteps  shape `(1802, 600)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84502816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.475 , -0.5   ],\n",
       "       [-0.375 , -0.425 ],\n",
       "       [ 0.175 ,  0.125 ],\n",
       "       ...,\n",
       "       [ 0.    , -0.1   ],\n",
       "       [-0.4575,  0.4575],\n",
       "       [-0.3   ,  0.2   ]], shape=(1802, 2))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tranforming the labels to scale of -1, 1\n",
    "def transform_emotion_labels(value):\n",
    "    return (value - 5) / 4\n",
    "\n",
    "y_processed = y.copy()\n",
    "y_processed[:, 0] = transform_emotion_labels(y[:, 0]) # valence\n",
    "y_processed[:, 1] = transform_emotion_labels(y[:, 1]) # arousal\n",
    "\n",
    "y_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0444c228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]], shape=(1802, 600))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention mask for variable length sequences\n",
    "attention_masks = np.zeros((n_songs, max_time_steps))\n",
    "\n",
    "for i, seq_length in enumerate(sequence_lengths):\n",
    "    attention_masks[i, :seq_length] = 1\n",
    "\n",
    "# X_normalized, y_processed, feature_scalers, attention_masks.\n",
    "\n",
    "# above is what we are using in the CRNN\n",
    "attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aec6bb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your X shape: (1802, 600, 40)\n",
      "X has dimensions: (songs, timesteps, features)\n",
      "\n",
      "X[:, :, 0] shape: (1802, 600)\n",
      "This gives us feature #0 for ALL songs and ALL timesteps\n",
      "\n",
      "X[0, :, :] shape: (600, 40)\n",
      "This gives us ALL features for song #0 at ALL timesteps\n",
      "\n",
      "X[0, :, 0] shape: (600,)\n",
      "This gives us feature #0 for song #0 at ALL timesteps\n",
      "\n",
      "==================================================\n",
      "FLATTENING EXPLANATION:\n",
      "==================================================\n",
      "\n",
      "Original X[:, :, 0] shape: (1802, 600)\n",
      "After .flatten(): (1081200,)\n",
      "Total elements: 1081200 = 1081200\n",
      "\n",
      "What flattening does:\n",
      "- Takes the 2D array (1802 songs  600 timesteps)\n",
      "- Converts it to 1D array (1,081,200 values)\n",
      "- Order: [song1_t1, song1_t2, ..., song1_t600, song2_t1, song2_t2, ...]\n"
     ]
    }
   ],
   "source": [
    "# Let's demonstrate with your actual data\n",
    "print(\"Your X shape:\", X.shape)\n",
    "print(\"X has dimensions: (songs, timesteps, features)\")\n",
    "\n",
    "# Example: Extract feature 0 (first feature) for all songs and timesteps\n",
    "feature_0_all = X[:, :, 0]  # Shape: (1802, 600)\n",
    "print(f\"\\nX[:, :, 0] shape: {feature_0_all.shape}\")\n",
    "print(\"This gives us feature #0 for ALL songs and ALL timesteps\")\n",
    "\n",
    "song_0_all = X[0, :, :]  # Shape: (600, 40)\n",
    "print(f\"\\nX[0, :, :] shape: {song_0_all.shape}\")\n",
    "print(\"This gives us ALL features for song #0 at ALL timesteps\")\n",
    "\n",
    "# Example: Extract feature 0 for song 0 at all timesteps\n",
    "song_0_feature_0 = X[0, :, 0]  # Shape: (600,)\n",
    "print(f\"\\nX[0, :, 0] shape: {song_0_feature_0.shape}\")\n",
    "print(\"This gives us feature #0 for song #0 at ALL timesteps\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FLATTENING EXPLANATION:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# The flattening operation\n",
    "feature_values = X[:, :, 0].flatten()  # Convert 2D to 1D\n",
    "print(f\"\\nOriginal X[:, :, 0] shape: {X[:, :, 0].shape}\")\n",
    "print(f\"After .flatten(): {feature_values.shape}\")\n",
    "print(f\"Total elements: {1802 * 600} = {feature_values.shape[0]}\")\n",
    "\n",
    "print(\"\\nWhat flattening does:\")\n",
    "print(\"- Takes the 2D array (1802 songs  600 timesteps)\")\n",
    "print(\"- Converts it to 1D array (1,081,200 values)\")\n",
    "print(\"- Order: [song1_t1, song1_t2, ..., song1_t600, song2_t1, song2_t2, ...]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b1dfefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EmotionCRNN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"EmotionCRNN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " audio_features (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">7,744</span> \n",
       "\n",
       " conv1_bn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> \n",
       "\n",
       " conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> \n",
       "\n",
       " pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " drop1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> \n",
       "\n",
       " conv3_bn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> \n",
       "\n",
       " conv4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> \n",
       "\n",
       " pool2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " drop2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " lstm1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> \n",
       "\n",
       " lstm2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> \n",
       "\n",
       " dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> \n",
       "\n",
       " drop3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> \n",
       "\n",
       " emotion_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " audio_features (\u001b[38;5;33mInputLayer\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m40\u001b[0m)                     \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " conv1 (\u001b[38;5;33mConv1D\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m7,744\u001b[0m \n",
       "\n",
       " conv1_bn (\u001b[38;5;33mBatchNormalization\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m256\u001b[0m \n",
       "\n",
       " conv2 (\u001b[38;5;33mConv1D\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m64\u001b[0m)                \u001b[38;5;34m12,352\u001b[0m \n",
       "\n",
       " pool1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " drop1 (\u001b[38;5;33mDropout\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " conv3 (\u001b[38;5;33mConv1D\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m24,704\u001b[0m \n",
       "\n",
       " conv3_bn (\u001b[38;5;33mBatchNormalization\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m512\u001b[0m \n",
       "\n",
       " conv4 (\u001b[38;5;33mConv1D\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m49,280\u001b[0m \n",
       "\n",
       " pool2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " drop2 (\u001b[38;5;33mDropout\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " lstm1 (\u001b[38;5;33mLSTM\u001b[0m)                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m \n",
       "\n",
       " lstm2 (\u001b[38;5;33mLSTM\u001b[0m)                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m49,408\u001b[0m \n",
       "\n",
       " dense1 (\u001b[38;5;33mDense\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                      \u001b[38;5;34m4,160\u001b[0m \n",
       "\n",
       " drop3 (\u001b[38;5;33mDropout\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense2 (\u001b[38;5;33mDense\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m2,080\u001b[0m \n",
       "\n",
       " emotion_output (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                          \u001b[38;5;34m66\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">282,146</span> (1.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m282,146\u001b[0m (1.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">281,762</span> (1.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m281,762\u001b[0m (1.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import (Conv1D, Input, LSTM, Dense, BatchNormalization, MaxPooling1D, Dropout)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_emotion_crnn(sequence_length, n_features):\n",
    "    \"\"\"Architecture\n",
    "    - Conv1D layers: learn feature relationships and reduce dimensionality\n",
    "    - LSTM layers: Capture temporal emotion evolution\n",
    "    - Dense Layers: Final emotion prediction\n",
    "\n",
    "    Args:\n",
    "        sequence_length (_type_): Time steps like 600\n",
    "        n_features (_type_): selcted features like 40\n",
    "    Returns:\n",
    "        Compiled keras model\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(sequence_length, n_features), name='audio_features') # (batch_size, 500, 40)\n",
    "\n",
    "    # BLOCK 1 - local pattern extraction\n",
    "    conv1 = Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=3, # consec time frames\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        name='conv1'\n",
    "    )(inputs) # (batch_size, 500, 64)\n",
    "\n",
    "    # stablize training\n",
    "    conv1_bn = BatchNormalization(name='conv1_bn')(conv1)\n",
    "\n",
    "    # second layer to learn more complex patterns\n",
    "    conv2 = Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        name='conv2'\n",
    "    )(conv1_bn) # (batch_size, 500, 64)    \n",
    "\n",
    "    # focus on important patterns to reduce temporal resolution\n",
    "    pool1 = MaxPooling1D(pool_size=2, name='pool1')(conv2) # (batch_size, 250, 64)\n",
    "    drop1 = Dropout(0.3, name='drop1')(pool1)\n",
    "\n",
    "    # BLOCK 2 - higher level feature combinations\n",
    "    conv3 = Conv1D(\n",
    "        filters=128, # more filters for complex patterns\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        name='conv3'\n",
    "    )(drop1)\n",
    "\n",
    "    conv3_bn = BatchNormalization(name='conv3_bn')(conv3)\n",
    "\n",
    "    conv4 = Conv1D(\n",
    "        filters=128,\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        name='conv4'\n",
    "    )(conv3_bn) # (batch_size, 250, 128)\n",
    "\n",
    "    pool2 = MaxPooling1D(pool_size=2, name='pool2')(conv4) # (batch_size, 125, 128)\n",
    "\n",
    "    drop2 = Dropout(0.3, name='drop2')(pool2)\n",
    "\n",
    "    # RECURRENT BLOCK - temporal dependecies in emtional expression\n",
    "    lstm1 = LSTM(\n",
    "        units=128,\n",
    "        return_sequences=True,\n",
    "        dropout=0.3,\n",
    "        recurrent_dropout=0.2,\n",
    "        name=\"lstm1\",\n",
    "    )(\n",
    "        drop2\n",
    "    )  # (batch_size, 125, 128)\n",
    "\n",
    "    # Second LSTM: Refine temporal understanding\n",
    "    lstm2 = LSTM(\n",
    "        units=64,  # Smaller hidden state\n",
    "        return_sequences=False,  # Return only final output\n",
    "        dropout=0.3,\n",
    "        recurrent_dropout=0.2,\n",
    "        name=\"lstm2\",\n",
    "    )(lstm1)\n",
    "    # Shape: (batch_size, 64)\n",
    "    \n",
    "    # FINAL PREDICTION BLOCK - map learned representations to valence/arousal\n",
    "    dense1 = Dense(\n",
    "        64,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(0.01),\n",
    "        name='dense1'\n",
    "    )(lstm2)\n",
    "    \n",
    "    drop3 = Dropout(0.4, name='drop3')(dense1)\n",
    "    \n",
    "    # smaller layer\n",
    "    dense2 = Dense(\n",
    "        32,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(0.01),\n",
    "        name='dense2'\n",
    "    )(drop3)\n",
    "    \n",
    "    # output layer: valence and arousal predction\n",
    "    outputs = Dense(\n",
    "        2, # v & a\n",
    "        activation='linear',\n",
    "        name='emotion_output'\n",
    "    )(dense2) # (batch_size, 2)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='EmotionCRNN')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_emotion_crnn(\n",
    "    sequence_length=600,\n",
    "    n_features=40\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dbe3534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EmotionCRNN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"EmotionCRNN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " audio_features (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">7,744</span> \n",
       "\n",
       " conv1_bn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> \n",
       "\n",
       " conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> \n",
       "\n",
       " pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " drop1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> \n",
       "\n",
       " conv3_bn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> \n",
       "\n",
       " conv4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> \n",
       "\n",
       " pool2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " drop2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " lstm1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> \n",
       "\n",
       " lstm2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> \n",
       "\n",
       " dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> \n",
       "\n",
       " drop3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> \n",
       "\n",
       " emotion_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " audio_features (\u001b[38;5;33mInputLayer\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m40\u001b[0m)                     \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " conv1 (\u001b[38;5;33mConv1D\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m7,744\u001b[0m \n",
       "\n",
       " conv1_bn (\u001b[38;5;33mBatchNormalization\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m256\u001b[0m \n",
       "\n",
       " conv2 (\u001b[38;5;33mConv1D\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m600\u001b[0m, \u001b[38;5;34m64\u001b[0m)                \u001b[38;5;34m12,352\u001b[0m \n",
       "\n",
       " pool1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " drop1 (\u001b[38;5;33mDropout\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " conv3 (\u001b[38;5;33mConv1D\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m24,704\u001b[0m \n",
       "\n",
       " conv3_bn (\u001b[38;5;33mBatchNormalization\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m512\u001b[0m \n",
       "\n",
       " conv4 (\u001b[38;5;33mConv1D\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m49,280\u001b[0m \n",
       "\n",
       " pool2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " drop2 (\u001b[38;5;33mDropout\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " lstm1 (\u001b[38;5;33mLSTM\u001b[0m)                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m \n",
       "\n",
       " lstm2 (\u001b[38;5;33mLSTM\u001b[0m)                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m49,408\u001b[0m \n",
       "\n",
       " dense1 (\u001b[38;5;33mDense\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                      \u001b[38;5;34m4,160\u001b[0m \n",
       "\n",
       " drop3 (\u001b[38;5;33mDropout\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense2 (\u001b[38;5;33mDense\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m2,080\u001b[0m \n",
       "\n",
       " emotion_output (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                          \u001b[38;5;34m66\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">282,146</span> (1.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m282,146\u001b[0m (1.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">281,762</span> (1.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m281,762\u001b[0m (1.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "def compile_emotion_crnn(model, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Compile CRNN model with appropriate loss function and metrics\n",
    "\n",
    "    Key Considerations:\n",
    "    - Regression problem (not classification)\n",
    "    - Two outputs: valence and arousal\n",
    "    - Need custom metrics for emotion evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    # Custom loss function for emotion prediction\n",
    "    def emotion_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Combined loss for valence and arousal prediction\n",
    "\n",
    "        Why not just MSE?\n",
    "        - Valence and arousal may have different scales/importance\n",
    "        - Can weight them differently based on prediction difficulty\n",
    "        \"\"\"\n",
    "\n",
    "        valence_true = y_true[:, 0]\n",
    "        arousal_true = y_true[:, 1]\n",
    "        valence_pred = y_pred[:, 0]\n",
    "        arousal_pred = y_pred[:, 1]\n",
    "\n",
    "        # Separate losses for each dimension\n",
    "        valence_loss = tf.reduce_mean(tf.square(valence_true - valence_pred))\n",
    "        arousal_loss = tf.reduce_mean(tf.square(arousal_true - arousal_pred))\n",
    "\n",
    "        # Weight arousal slightly higher (easier to predict)\n",
    "        total_loss = 0.4 * valence_loss + 0.6 * arousal_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    # Custom metrics for evaluation\n",
    "    def valence_mse(y_true, y_pred):\n",
    "        \"\"\"Calculate MSE specifically for valence\"\"\"\n",
    "        return tf.reduce_mean(tf.square(y_true[:, 0] - y_pred[:, 0]))\n",
    "\n",
    "    def arousal_mse(y_true, y_pred):\n",
    "        \"\"\"Calculate MSE specifically for arousal\"\"\"\n",
    "        return tf.reduce_mean(tf.square(y_true[:, 1] - y_pred[:, 1]))\n",
    "\n",
    "    def concordance_correlation_coefficient(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        CCC metric commonly used in emotion recognition\n",
    "        Measures agreement between predicted and true values\n",
    "        \"\"\"\n",
    "        # Flatten predictions\n",
    "        y_true_flat = tf.reshape(y_true, [-1])\n",
    "        y_pred_flat = tf.reshape(y_pred, [-1])\n",
    "\n",
    "        # Calculate means\n",
    "        mean_true = tf.reduce_mean(y_true_flat)\n",
    "        mean_pred = tf.reduce_mean(y_pred_flat)\n",
    "\n",
    "        # Calculate variances and covariance\n",
    "        var_true = tf.reduce_mean(tf.square(y_true_flat - mean_true))\n",
    "        var_pred = tf.reduce_mean(tf.square(y_pred_flat - mean_pred))\n",
    "        covar = tf.reduce_mean((y_true_flat - mean_true) * (y_pred_flat - mean_pred))\n",
    "\n",
    "        # CCC formula\n",
    "        ccc = (2 * covar) / (var_true + var_pred + tf.square(mean_true - mean_pred))\n",
    "\n",
    "        return ccc\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate, clipnorm=1.0),\n",
    "        loss=emotion_loss,  # or 'mse' for simple approach\n",
    "        metrics=[\n",
    "            \"mae\",  # Mean Absolute Error\n",
    "            valence_mse,  # Valence-specific MSE\n",
    "            arousal_mse,  # Arousal-specific MSE\n",
    "            concordance_correlation_coefficient,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model = compile_emotion_crnn(model, learning_rate=0.001)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41710f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Fix the missing tf import for the custom loss function\n",
    "# def compile_emotion_crnn_fixed(model, learning_rate=0.001):\n",
    "#     \"\"\"\n",
    "#     Compile CRNN model with appropriate loss function and metrics\n",
    "#     Fixed version with proper tf import\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Custom loss function for emotion prediction\n",
    "#     def emotion_loss(y_true, y_pred):\n",
    "#         \"\"\"Combined loss for valence and arousal prediction\"\"\"\n",
    "#         valence_true = y_true[:, 0]\n",
    "#         arousal_true = y_true[:, 1]\n",
    "#         valence_pred = y_pred[:, 0]\n",
    "#         arousal_pred = y_pred[:, 1]\n",
    "\n",
    "#         # Separate losses for each dimension\n",
    "#         valence_loss = tf.reduce_mean(tf.square(valence_true - valence_pred))\n",
    "#         arousal_loss = tf.reduce_mean(tf.square(arousal_true - arousal_pred))\n",
    "\n",
    "#         # Weight arousal slightly higher (easier to predict)\n",
    "#         total_loss = 0.4 * valence_loss + 0.6 * arousal_loss\n",
    "#         return total_loss\n",
    "\n",
    "#     # Custom metrics for evaluation\n",
    "#     def valence_mse(y_true, y_pred):\n",
    "#         return tf.reduce_mean(tf.square(y_true[:, 0] - y_pred[:, 0]))\n",
    "\n",
    "#     def arousal_mse(y_true, y_pred):\n",
    "#         return tf.reduce_mean(tf.square(y_true[:, 1] - y_pred[:, 1]))\n",
    "\n",
    "#     # Compile model\n",
    "#     model.compile(\n",
    "#         optimizer=Adam(learning_rate=learning_rate, clipnorm=1.0),\n",
    "#         loss=emotion_loss,\n",
    "#         metrics=[\"mae\", valence_mse, arousal_mse]\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# # Recompile with fixed function\n",
    "# model = compile_emotion_crnn_fixed(model, learning_rate=0.001)\n",
    "# print(\"Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c461bf",
   "metadata": {},
   "source": [
    "# Training Setup - Data Splitting\n",
    "\n",
    "Now let's split our data for training and set up callbacks for monitoring the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e93c7286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed:\n",
      "  Training set: 1152 songs (63.9%)\n",
      "  Validation set: 289 songs (16.0%)\n",
      "  Test set: 361 songs (20.0%)\n",
      "\n",
      "Final shapes:\n",
      "  X_train: (1152, 600, 40)\n",
      "  y_train: (1152, 2)\n",
      "  X_val: (289, 600, 40)\n",
      "  y_val: (289, 2)\n",
      "  X_test: (361, 600, 40)\n",
      "  y_test: (361, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_emotion_data(X, y, attention_masks, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data into train/validation/test sets\n",
    "    \n",
    "    Args:\n",
    "        X: Feature sequences (n_songs, seq_len, n_features)\n",
    "        y: Emotion labels (n_songs, 2)\n",
    "        attention_masks: Masks for variable lengths (n_songs, seq_len)\n",
    "        test_size: Proportion for test set\n",
    "        val_size: Proportion of remaining data for validation\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with train/val/test splits\n",
    "    \"\"\"\n",
    "    \n",
    "    # First split: separate test set\n",
    "    X_temp, X_test, y_temp, y_test, masks_temp, masks_test = train_test_split(\n",
    "        X, y, attention_masks, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=None  # Can't stratify continuous targets\n",
    "    )\n",
    "    \n",
    "    # Second split: separate train and validation from remaining data\n",
    "    X_train, X_val, y_train, y_val, masks_train, masks_val = train_test_split(\n",
    "        X_temp, y_temp, masks_temp,\n",
    "        test_size=val_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"Data split completed:\")\n",
    "    print(f\"  Training set: {X_train.shape[0]} songs ({X_train.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "    print(f\"  Validation set: {X_val.shape[0]} songs ({X_val.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "    print(f\"  Test set: {X_test.shape[0]} songs ({X_test.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train, 'y_train': y_train, 'masks_train': masks_train,\n",
    "        'X_val': X_val, 'y_val': y_val, 'masks_val': masks_val,\n",
    "        'X_test': X_test, 'y_test': y_test, 'masks_test': masks_test\n",
    "    }\n",
    "\n",
    "# Split the data\n",
    "data_splits = split_emotion_data(X_normalized, y_processed, attention_masks)\n",
    "\n",
    "# Extract for easier access\n",
    "X_train = data_splits['X_train']\n",
    "y_train = data_splits['y_train']\n",
    "X_val = data_splits['X_val'] \n",
    "y_val = data_splits['y_val']\n",
    "X_test = data_splits['X_test']\n",
    "y_test = data_splits['y_test']\n",
    "\n",
    "print(f\"\\nFinal shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}\")\n",
    "print(f\"  y_val: {y_val.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614822b2",
   "metadata": {},
   "source": [
    "# Training Callbacks Setup\n",
    "\n",
    "Set up callbacks to monitor training progress, save best models, and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b5c6c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks configured:\n",
      "  Model will be saved to: ../output/models/emotion_crnn_20250917_152753\n",
      "  Logs will be saved to: ../output/results/emotion_crnn_20250917_152753\n",
      "\n",
      "Callback setup complete!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, \n",
    "    CSVLogger, TensorBoard\n",
    ")\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_training_callbacks(model_name=\"emotion_crnn\"):\n",
    "    \"\"\"\n",
    "    Set up callbacks for training monitoring and model saving\n",
    "    \n",
    "    Returns:\n",
    "        List of configured callbacks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create directories for saving\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_dir = f\"../output/models/{model_name}_{timestamp}\"\n",
    "    logs_dir = f\"../output/results/{model_name}_{timestamp}\"\n",
    "    \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    \n",
    "    callbacks = [\n",
    "        # Save best model based on validation loss\n",
    "        ModelCheckpoint(\n",
    "            filepath=f\"{model_dir}/best_model.h5\",\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Early stopping to prevent overfitting\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,  # Stop if no improvement for 15 epochs\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Reduce learning rate when training plateaus\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,  # Reduce LR by half\n",
    "            patience=8,   # Wait 8 epochs before reducing\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Log training metrics to CSV\n",
    "        CSVLogger(\n",
    "            filename=f\"{logs_dir}/training_log.csv\",\n",
    "            append=True\n",
    "        ),\n",
    "        \n",
    "        # TensorBoard for visualization\n",
    "        TensorBoard(\n",
    "            log_dir=f\"{logs_dir}/tensorboard\",\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            update_freq='epoch'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"Callbacks configured:\")\n",
    "    print(f\"  Model will be saved to: {model_dir}\")\n",
    "    print(f\"  Logs will be saved to: {logs_dir}\")\n",
    "    \n",
    "    return callbacks, model_dir, logs_dir\n",
    "\n",
    "# Set up callbacks\n",
    "callbacks, model_save_path, logs_path = setup_training_callbacks()\n",
    "print(f\"\\nCallback setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ee0d5",
   "metadata": {},
   "source": [
    "# Model Training Execution\n",
    "\n",
    "Now let's train the CRNN model with our prepared data and callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e387dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting CRNN training...\n",
      "Starting training:\n",
      "  Training samples: 1152\n",
      "  Validation samples: 289\n",
      "  Batch size: 16\n",
      "  Max epochs: 50\n",
      "  Input shape: (600, 40)\n",
      "  Output shape: (2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 15:29:12.804774: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 110592000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - arousal_mse: 0.1213 - concordance_correlation_coefficient: -0.0175 - loss: 0.9994 - mae: 0.2733 - valence_mse: 0.1004\n",
      "Epoch 1: val_loss improved from None to 0.56354, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.56354, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2s/step - arousal_mse: 0.1110 - concordance_correlation_coefficient: 0.0062 - loss: 0.8370 - mae: 0.2636 - valence_mse: 0.0912 - val_arousal_mse: 0.1130 - val_concordance_correlation_coefficient: 0.0025 - val_loss: 0.5635 - val_mae: 0.2571 - val_valence_mse: 0.0916 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "Epoch 2/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - arousal_mse: 0.1024 - concordance_correlation_coefficient: 0.0084 - loss: 0.4741 - mae: 0.2604 - valence_mse: 0.0916\n",
      "Epoch 2: val_loss improved from 0.56354 to 0.27995, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.56354 to 0.27995, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - arousal_mse: 0.1047 - concordance_correlation_coefficient: 0.0136 - loss: 0.4004 - mae: 0.2571 - valence_mse: 0.0860 - val_arousal_mse: 0.1145 - val_concordance_correlation_coefficient: 0.0084 - val_loss: 0.2799 - val_mae: 0.2571 - val_valence_mse: 0.0908 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "Epoch 3/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - arousal_mse: 0.1017 - concordance_correlation_coefficient: 0.0152 - loss: 0.2382 - mae: 0.2553 - valence_mse: 0.0859\n",
      "Epoch 3: val_loss improved from 0.27995 to 0.16456, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.27995 to 0.16456, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 2s/step - arousal_mse: 0.1035 - concordance_correlation_coefficient: 0.0154 - loss: 0.2091 - mae: 0.2566 - valence_mse: 0.0851 - val_arousal_mse: 0.1134 - val_concordance_correlation_coefficient: 0.0075 - val_loss: 0.1646 - val_mae: 0.2567 - val_valence_mse: 0.0905 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "Epoch 4/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - arousal_mse: 0.1087 - concordance_correlation_coefficient: 0.0202 - loss: 0.1514 - mae: 0.2617 - valence_mse: 0.0901\n",
      "Epoch 4: val_loss improved from 0.16456 to 0.12218, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 0.16456 to 0.12218, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - arousal_mse: 0.1040 - concordance_correlation_coefficient: 0.0162 - loss: 0.1353 - mae: 0.2555 - valence_mse: 0.0843 - val_arousal_mse: 0.1137 - val_concordance_correlation_coefficient: 0.0079 - val_loss: 0.1222 - val_mae: 0.2572 - val_valence_mse: 0.0907 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "Epoch 5/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538ms/step - arousal_mse: 0.1055 - concordance_correlation_coefficient: 0.0058 - loss: 0.1134 - mae: 0.2565 - valence_mse: 0.0832\n",
      "Epoch 5: val_loss improved from 0.12218 to 0.10804, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 0.12218 to 0.10804, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 571ms/step - arousal_mse: 0.1039 - concordance_correlation_coefficient: 0.0112 - loss: 0.1095 - mae: 0.2563 - valence_mse: 0.0852 - val_arousal_mse: 0.1139 - val_concordance_correlation_coefficient: 0.0075 - val_loss: 0.1080 - val_mae: 0.2572 - val_valence_mse: 0.0906 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "Epoch 6/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - arousal_mse: 0.1053 - concordance_correlation_coefficient: 0.0129 - loss: 0.1047 - mae: 0.2604 - valence_mse: 0.0892\n",
      "Epoch 6: val_loss improved from 0.10804 to 0.10352, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 0.10804 to 0.10352, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 515ms/step - arousal_mse: 0.1041 - concordance_correlation_coefficient: 0.0127 - loss: 0.1011 - mae: 0.2568 - valence_mse: 0.0851 - val_arousal_mse: 0.1138 - val_concordance_correlation_coefficient: 0.0085 - val_loss: 0.1035 - val_mae: 0.2570 - val_valence_mse: 0.0904 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "Epoch 7/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481ms/step - arousal_mse: 0.1005 - concordance_correlation_coefficient: 0.0173 - loss: 0.0979 - mae: 0.2576 - valence_mse: 0.0883\n",
      "Epoch 7: val_loss improved from 0.10352 to 0.10217, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 0.10352 to 0.10217, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 512ms/step - arousal_mse: 0.1037 - concordance_correlation_coefficient: 0.0204 - loss: 0.0980 - mae: 0.2560 - valence_mse: 0.0844 - val_arousal_mse: 0.1139 - val_concordance_correlation_coefficient: 0.0128 - val_loss: 0.1022 - val_mae: 0.2567 - val_valence_mse: 0.0898 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "Epoch 8/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step - arousal_mse: 0.1021 - concordance_correlation_coefficient: 0.0263 - loss: 0.0972 - mae: 0.2569 - valence_mse: 0.0863\n",
      "Epoch 8: val_loss improved from 0.10217 to 0.10144, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 0.10217 to 0.10144, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 520ms/step - arousal_mse: 0.1037 - concordance_correlation_coefficient: 0.0247 - loss: 0.0970 - mae: 0.2550 - valence_mse: 0.0835 - val_arousal_mse: 0.1145 - val_concordance_correlation_coefficient: 0.0147 - val_loss: 0.1014 - val_mae: 0.2570 - val_valence_mse: 0.0900 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "Epoch 9/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532ms/step - arousal_mse: 0.1094 - concordance_correlation_coefficient: 0.0253 - loss: 0.1004 - mae: 0.2588 - valence_mse: 0.0844\n",
      "Epoch 9: val_loss did not improve from 0.10144\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.10144\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 564ms/step - arousal_mse: 0.1035 - concordance_correlation_coefficient: 0.0289 - loss: 0.0966 - mae: 0.2552 - valence_mse: 0.0837 - val_arousal_mse: 0.1139 - val_concordance_correlation_coefficient: 0.0131 - val_loss: 0.1017 - val_mae: 0.2569 - val_valence_mse: 0.0897 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 564ms/step - arousal_mse: 0.1035 - concordance_correlation_coefficient: 0.0289 - loss: 0.0966 - mae: 0.2552 - valence_mse: 0.0837 - val_arousal_mse: 0.1139 - val_concordance_correlation_coefficient: 0.0131 - val_loss: 0.1017 - val_mae: 0.2569 - val_valence_mse: 0.0897 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490ms/step - arousal_mse: 0.1035 - concordance_correlation_coefficient: 0.0233 - loss: 0.0977 - mae: 0.2599 - valence_mse: 0.0868\n",
      "Epoch 10: val_loss improved from 0.10144 to 0.10128, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 0.10144 to 0.10128, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 536ms/step - arousal_mse: 0.1033 - concordance_correlation_coefficient: 0.0309 - loss: 0.0962 - mae: 0.2547 - valence_mse: 0.0833 - val_arousal_mse: 0.1144 - val_concordance_correlation_coefficient: 0.0238 - val_loss: 0.1013 - val_mae: 0.2566 - val_valence_mse: 0.0886 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "Epoch 11/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514ms/step - arousal_mse: 0.0997 - concordance_correlation_coefficient: 0.0928 - loss: 0.0924 - mae: 0.2479 - valence_mse: 0.0783\n",
      "Epoch 11: val_loss improved from 0.10128 to 0.08671, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 0.10128 to 0.08671, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 561ms/step - arousal_mse: 0.0984 - concordance_correlation_coefficient: 0.1338 - loss: 0.0913 - mae: 0.2466 - valence_mse: 0.0775 - val_arousal_mse: 0.0963 - val_concordance_correlation_coefficient: 0.2243 - val_loss: 0.0867 - val_mae: 0.2306 - val_valence_mse: 0.0764 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "Epoch 12/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511ms/step - arousal_mse: 0.0848 - concordance_correlation_coefficient: 0.2611 - loss: 0.0804 - mae: 0.2288 - valence_mse: 0.0690\n",
      "Epoch 12: val_loss did not improve from 0.08671\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.08671\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 545ms/step - arousal_mse: 0.0819 - concordance_correlation_coefficient: 0.3085 - loss: 0.0786 - mae: 0.2251 - valence_mse: 0.0681 - val_arousal_mse: 0.1635 - val_concordance_correlation_coefficient: 0.1138 - val_loss: 0.1581 - val_mae: 0.3177 - val_valence_mse: 0.1437 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 545ms/step - arousal_mse: 0.0819 - concordance_correlation_coefficient: 0.3085 - loss: 0.0786 - mae: 0.2251 - valence_mse: 0.0681 - val_arousal_mse: 0.1635 - val_concordance_correlation_coefficient: 0.1138 - val_loss: 0.1581 - val_mae: 0.3177 - val_valence_mse: 0.1437 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - arousal_mse: 0.0807 - concordance_correlation_coefficient: 0.2873 - loss: 0.0796 - mae: 0.2252 - valence_mse: 0.0719\n",
      "Epoch 13: val_loss improved from 0.08671 to 0.08136, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 0.08671 to 0.08136, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 584ms/step - arousal_mse: 0.0839 - concordance_correlation_coefficient: 0.2817 - loss: 0.0819 - mae: 0.2302 - valence_mse: 0.0732 - val_arousal_mse: 0.0850 - val_concordance_correlation_coefficient: 0.3832 - val_loss: 0.0814 - val_mae: 0.2174 - val_valence_mse: 0.0740 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "Epoch 14/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - arousal_mse: 0.0776 - concordance_correlation_coefficient: 0.3614 - loss: 0.0748 - mae: 0.2156 - valence_mse: 0.0643\n",
      "Epoch 14: val_loss improved from 0.08136 to 0.07293, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 0.08136 to 0.07293, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 583ms/step - arousal_mse: 0.0762 - concordance_correlation_coefficient: 0.3712 - loss: 0.0744 - mae: 0.2159 - valence_mse: 0.0659 - val_arousal_mse: 0.0774 - val_concordance_correlation_coefficient: 0.4252 - val_loss: 0.0729 - val_mae: 0.2090 - val_valence_mse: 0.0640 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "Epoch 15/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554ms/step - arousal_mse: 0.0750 - concordance_correlation_coefficient: 0.3993 - loss: 0.0740 - mae: 0.2171 - valence_mse: 0.0658\n",
      "Epoch 15: val_loss did not improve from 0.07293\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.07293\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 587ms/step - arousal_mse: 0.0755 - concordance_correlation_coefficient: 0.3922 - loss: 0.0741 - mae: 0.2162 - valence_mse: 0.0655 - val_arousal_mse: 0.0827 - val_concordance_correlation_coefficient: 0.3452 - val_loss: 0.0768 - val_mae: 0.2152 - val_valence_mse: 0.0660 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 587ms/step - arousal_mse: 0.0755 - concordance_correlation_coefficient: 0.3922 - loss: 0.0741 - mae: 0.2162 - valence_mse: 0.0655 - val_arousal_mse: 0.0827 - val_concordance_correlation_coefficient: 0.3452 - val_loss: 0.0768 - val_mae: 0.2152 - val_valence_mse: 0.0660 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544ms/step - arousal_mse: 0.0754 - concordance_correlation_coefficient: 0.4043 - loss: 0.0732 - mae: 0.2133 - valence_mse: 0.0623\n",
      "Epoch 16: val_loss did not improve from 0.07293\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.07293\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 576ms/step - arousal_mse: 0.0731 - concordance_correlation_coefficient: 0.4172 - loss: 0.0722 - mae: 0.2125 - valence_mse: 0.0640 - val_arousal_mse: 0.0829 - val_concordance_correlation_coefficient: 0.3182 - val_loss: 0.0778 - val_mae: 0.2174 - val_valence_mse: 0.0667 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 576ms/step - arousal_mse: 0.0731 - concordance_correlation_coefficient: 0.4172 - loss: 0.0722 - mae: 0.2125 - valence_mse: 0.0640 - val_arousal_mse: 0.0829 - val_concordance_correlation_coefficient: 0.3182 - val_loss: 0.0778 - val_mae: 0.2174 - val_valence_mse: 0.0667 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507ms/step - arousal_mse: 0.0662 - concordance_correlation_coefficient: 0.4347 - loss: 0.0667 - mae: 0.2041 - valence_mse: 0.0611\n",
      "Epoch 17: val_loss did not improve from 0.07293\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.07293\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 539ms/step - arousal_mse: 0.0690 - concordance_correlation_coefficient: 0.4465 - loss: 0.0683 - mae: 0.2063 - valence_mse: 0.0612 - val_arousal_mse: 0.0821 - val_concordance_correlation_coefficient: 0.3345 - val_loss: 0.0771 - val_mae: 0.2160 - val_valence_mse: 0.0662 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 539ms/step - arousal_mse: 0.0690 - concordance_correlation_coefficient: 0.4465 - loss: 0.0683 - mae: 0.2063 - valence_mse: 0.0612 - val_arousal_mse: 0.0821 - val_concordance_correlation_coefficient: 0.3345 - val_loss: 0.0771 - val_mae: 0.2160 - val_valence_mse: 0.0662 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 547ms/step - arousal_mse: 0.0729 - concordance_correlation_coefficient: 0.4210 - loss: 0.0719 - mae: 0.2121 - valence_mse: 0.0649\n",
      "Epoch 18: val_loss did not improve from 0.07293\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.07293\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 590ms/step - arousal_mse: 0.0712 - concordance_correlation_coefficient: 0.4442 - loss: 0.0694 - mae: 0.2073 - valence_mse: 0.0612 - val_arousal_mse: 0.0773 - val_concordance_correlation_coefficient: 0.3873 - val_loss: 0.0742 - val_mae: 0.2136 - val_valence_mse: 0.0651 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 590ms/step - arousal_mse: 0.0712 - concordance_correlation_coefficient: 0.4442 - loss: 0.0694 - mae: 0.2073 - valence_mse: 0.0612 - val_arousal_mse: 0.0773 - val_concordance_correlation_coefficient: 0.3873 - val_loss: 0.0742 - val_mae: 0.2136 - val_valence_mse: 0.0651 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580ms/step - arousal_mse: 0.0743 - concordance_correlation_coefficient: 0.4076 - loss: 0.0712 - mae: 0.2119 - valence_mse: 0.0610\n",
      "Epoch 19: val_loss did not improve from 0.07293\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.07293\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 613ms/step - arousal_mse: 0.0727 - concordance_correlation_coefficient: 0.4321 - loss: 0.0702 - mae: 0.2088 - valence_mse: 0.0606 - val_arousal_mse: 0.0886 - val_concordance_correlation_coefficient: 0.3576 - val_loss: 0.0841 - val_mae: 0.2269 - val_valence_mse: 0.0706 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 613ms/step - arousal_mse: 0.0727 - concordance_correlation_coefficient: 0.4321 - loss: 0.0702 - mae: 0.2088 - valence_mse: 0.0606 - val_arousal_mse: 0.0886 - val_concordance_correlation_coefficient: 0.3576 - val_loss: 0.0841 - val_mae: 0.2269 - val_valence_mse: 0.0706 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558ms/step - arousal_mse: 0.0615 - concordance_correlation_coefficient: 0.4915 - loss: 0.0634 - mae: 0.1969 - valence_mse: 0.0600\n",
      "Epoch 20: val_loss did not improve from 0.07293\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.07293\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 608ms/step - arousal_mse: 0.0656 - concordance_correlation_coefficient: 0.4707 - loss: 0.0662 - mae: 0.2015 - valence_mse: 0.0612 - val_arousal_mse: 0.0815 - val_concordance_correlation_coefficient: 0.4424 - val_loss: 0.0791 - val_mae: 0.2167 - val_valence_mse: 0.0687 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 608ms/step - arousal_mse: 0.0656 - concordance_correlation_coefficient: 0.4707 - loss: 0.0662 - mae: 0.2015 - valence_mse: 0.0612 - val_arousal_mse: 0.0815 - val_concordance_correlation_coefficient: 0.4424 - val_loss: 0.0791 - val_mae: 0.2167 - val_valence_mse: 0.0687 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555ms/step - arousal_mse: 0.0658 - concordance_correlation_coefficient: 0.5224 - loss: 0.0654 - mae: 0.1990 - valence_mse: 0.0587\n",
      "Epoch 21: val_loss improved from 0.07293 to 0.06978, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.07293 to 0.06978, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 595ms/step - arousal_mse: 0.0654 - concordance_correlation_coefficient: 0.5080 - loss: 0.0646 - mae: 0.1989 - valence_mse: 0.0578 - val_arousal_mse: 0.0732 - val_concordance_correlation_coefficient: 0.4417 - val_loss: 0.0698 - val_mae: 0.2064 - val_valence_mse: 0.0602 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "Epoch 22/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624ms/step - arousal_mse: 0.0674 - concordance_correlation_coefficient: 0.4822 - loss: 0.0658 - mae: 0.2037 - valence_mse: 0.0579\n",
      "Epoch 22: val_loss did not improve from 0.06978\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.06978\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 660ms/step - arousal_mse: 0.0731 - concordance_correlation_coefficient: 0.4374 - loss: 0.0708 - mae: 0.2105 - valence_mse: 0.0616 - val_arousal_mse: 0.1465 - val_concordance_correlation_coefficient: 0.1150 - val_loss: 0.1349 - val_mae: 0.2890 - val_valence_mse: 0.1150 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 660ms/step - arousal_mse: 0.0731 - concordance_correlation_coefficient: 0.4374 - loss: 0.0708 - mae: 0.2105 - valence_mse: 0.0616 - val_arousal_mse: 0.1465 - val_concordance_correlation_coefficient: 0.1150 - val_loss: 0.1349 - val_mae: 0.2890 - val_valence_mse: 0.1150 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621ms/step - arousal_mse: 0.0811 - concordance_correlation_coefficient: 0.3660 - loss: 0.0776 - mae: 0.2204 - valence_mse: 0.0657\n",
      "Epoch 23: val_loss did not improve from 0.06978\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.06978\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 658ms/step - arousal_mse: 0.0759 - concordance_correlation_coefficient: 0.3833 - loss: 0.0742 - mae: 0.2167 - valence_mse: 0.0658 - val_arousal_mse: 0.0767 - val_concordance_correlation_coefficient: 0.3937 - val_loss: 0.0713 - val_mae: 0.2086 - val_valence_mse: 0.0597 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 658ms/step - arousal_mse: 0.0759 - concordance_correlation_coefficient: 0.3833 - loss: 0.0742 - mae: 0.2167 - valence_mse: 0.0658 - val_arousal_mse: 0.0767 - val_concordance_correlation_coefficient: 0.3937 - val_loss: 0.0713 - val_mae: 0.2086 - val_valence_mse: 0.0597 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579ms/step - arousal_mse: 0.0686 - concordance_correlation_coefficient: 0.4861 - loss: 0.0689 - mae: 0.2084 - valence_mse: 0.0631\n",
      "Epoch 24: val_loss did not improve from 0.06978\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.06978\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 617ms/step - arousal_mse: 0.0701 - concordance_correlation_coefficient: 0.4564 - loss: 0.0694 - mae: 0.2080 - valence_mse: 0.0627 - val_arousal_mse: 0.0789 - val_concordance_correlation_coefficient: 0.3733 - val_loss: 0.0742 - val_mae: 0.2121 - val_valence_mse: 0.0648 - learning_rate: 0.0010\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 617ms/step - arousal_mse: 0.0701 - concordance_correlation_coefficient: 0.4564 - loss: 0.0694 - mae: 0.2080 - valence_mse: 0.0627 - val_arousal_mse: 0.0789 - val_concordance_correlation_coefficient: 0.3733 - val_loss: 0.0742 - val_mae: 0.2121 - val_valence_mse: 0.0648 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "Epoch 25/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617ms/step - arousal_mse: 0.0617 - concordance_correlation_coefficient: 0.4949 - loss: 0.0617 - mae: 0.1965 - valence_mse: 0.0564\n",
      "Epoch 25: val_loss did not improve from 0.06978\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.06978\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 650ms/step - arousal_mse: 0.0689 - concordance_correlation_coefficient: 0.4746 - loss: 0.0673 - mae: 0.2057 - valence_mse: 0.0597 - val_arousal_mse: 0.0739 - val_concordance_correlation_coefficient: 0.4290 - val_loss: 0.0706 - val_mae: 0.2049 - val_valence_mse: 0.0610 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 650ms/step - arousal_mse: 0.0689 - concordance_correlation_coefficient: 0.4746 - loss: 0.0673 - mae: 0.2057 - valence_mse: 0.0597 - val_arousal_mse: 0.0739 - val_concordance_correlation_coefficient: 0.4290 - val_loss: 0.0706 - val_mae: 0.2049 - val_valence_mse: 0.0610 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616ms/step - arousal_mse: 0.0683 - concordance_correlation_coefficient: 0.4887 - loss: 0.0647 - mae: 0.1993 - valence_mse: 0.0535\n",
      "Epoch 26: val_loss did not improve from 0.06978\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.06978\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 648ms/step - arousal_mse: 0.0674 - concordance_correlation_coefficient: 0.4826 - loss: 0.0665 - mae: 0.2032 - valence_mse: 0.0598 - val_arousal_mse: 0.0744 - val_concordance_correlation_coefficient: 0.4267 - val_loss: 0.0718 - val_mae: 0.2072 - val_valence_mse: 0.0629 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 648ms/step - arousal_mse: 0.0674 - concordance_correlation_coefficient: 0.4826 - loss: 0.0665 - mae: 0.2032 - valence_mse: 0.0598 - val_arousal_mse: 0.0744 - val_concordance_correlation_coefficient: 0.4267 - val_loss: 0.0718 - val_mae: 0.2072 - val_valence_mse: 0.0629 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - arousal_mse: 0.0666 - concordance_correlation_coefficient: 0.5250 - loss: 0.0649 - mae: 0.1962 - valence_mse: 0.0568\n",
      "Epoch 27: val_loss improved from 0.06978 to 0.06957, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.06978 to 0.06957, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 573ms/step - arousal_mse: 0.0653 - concordance_correlation_coefficient: 0.5027 - loss: 0.0641 - mae: 0.1971 - valence_mse: 0.0569 - val_arousal_mse: 0.0721 - val_concordance_correlation_coefficient: 0.4484 - val_loss: 0.0696 - val_mae: 0.2048 - val_valence_mse: 0.0611 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "Epoch 28/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596ms/step - arousal_mse: 0.0718 - concordance_correlation_coefficient: 0.4671 - loss: 0.0677 - mae: 0.2055 - valence_mse: 0.0566\n",
      "Epoch 28: val_loss did not improve from 0.06957\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.06957\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 632ms/step - arousal_mse: 0.0689 - concordance_correlation_coefficient: 0.4673 - loss: 0.0674 - mae: 0.2046 - valence_mse: 0.0603 - val_arousal_mse: 0.0720 - val_concordance_correlation_coefficient: 0.4188 - val_loss: 0.0697 - val_mae: 0.2053 - val_valence_mse: 0.0613 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 632ms/step - arousal_mse: 0.0689 - concordance_correlation_coefficient: 0.4673 - loss: 0.0674 - mae: 0.2046 - valence_mse: 0.0603 - val_arousal_mse: 0.0720 - val_concordance_correlation_coefficient: 0.4188 - val_loss: 0.0697 - val_mae: 0.2053 - val_valence_mse: 0.0613 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628ms/step - arousal_mse: 0.0665 - concordance_correlation_coefficient: 0.4804 - loss: 0.0652 - mae: 0.1986 - valence_mse: 0.0580\n",
      "Epoch 29: val_loss improved from 0.06957 to 0.06882, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.06957 to 0.06882, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 661ms/step - arousal_mse: 0.0674 - concordance_correlation_coefficient: 0.4827 - loss: 0.0667 - mae: 0.2022 - valence_mse: 0.0603 - val_arousal_mse: 0.0712 - val_concordance_correlation_coefficient: 0.4827 - val_loss: 0.0688 - val_mae: 0.2032 - val_valence_mse: 0.0608 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "Epoch 30/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - arousal_mse: 0.0695 - concordance_correlation_coefficient: 0.4976 - loss: 0.0679 - mae: 0.2056 - valence_mse: 0.0597\n",
      "Epoch 30: val_loss did not improve from 0.06882\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.06882\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 571ms/step - arousal_mse: 0.0670 - concordance_correlation_coefficient: 0.4980 - loss: 0.0650 - mae: 0.1998 - valence_mse: 0.0563 - val_arousal_mse: 0.0749 - val_concordance_correlation_coefficient: 0.4154 - val_loss: 0.0730 - val_mae: 0.2100 - val_valence_mse: 0.0630 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 571ms/step - arousal_mse: 0.0670 - concordance_correlation_coefficient: 0.4980 - loss: 0.0650 - mae: 0.1998 - valence_mse: 0.0563 - val_arousal_mse: 0.0749 - val_concordance_correlation_coefficient: 0.4154 - val_loss: 0.0730 - val_mae: 0.2100 - val_valence_mse: 0.0630 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525ms/step - arousal_mse: 0.0648 - concordance_correlation_coefficient: 0.5096 - loss: 0.0636 - mae: 0.1974 - valence_mse: 0.0553\n",
      "Epoch 31: val_loss did not improve from 0.06882\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.06882\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 556ms/step - arousal_mse: 0.0673 - concordance_correlation_coefficient: 0.4890 - loss: 0.0663 - mae: 0.2020 - valence_mse: 0.0583 - val_arousal_mse: 0.0740 - val_concordance_correlation_coefficient: 0.3773 - val_loss: 0.0734 - val_mae: 0.2124 - val_valence_mse: 0.0658 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 556ms/step - arousal_mse: 0.0673 - concordance_correlation_coefficient: 0.4890 - loss: 0.0663 - mae: 0.2020 - valence_mse: 0.0583 - val_arousal_mse: 0.0740 - val_concordance_correlation_coefficient: 0.3773 - val_loss: 0.0734 - val_mae: 0.2124 - val_valence_mse: 0.0658 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537ms/step - arousal_mse: 0.0670 - concordance_correlation_coefficient: 0.4722 - loss: 0.0657 - mae: 0.1991 - valence_mse: 0.0577\n",
      "Epoch 32: val_loss did not improve from 0.06882\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.06882\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 568ms/step - arousal_mse: 0.0658 - concordance_correlation_coefficient: 0.4875 - loss: 0.0655 - mae: 0.2010 - valence_mse: 0.0589 - val_arousal_mse: 0.0727 - val_concordance_correlation_coefficient: 0.4636 - val_loss: 0.0721 - val_mae: 0.2083 - val_valence_mse: 0.0632 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 568ms/step - arousal_mse: 0.0658 - concordance_correlation_coefficient: 0.4875 - loss: 0.0655 - mae: 0.2010 - valence_mse: 0.0589 - val_arousal_mse: 0.0727 - val_concordance_correlation_coefficient: 0.4636 - val_loss: 0.0721 - val_mae: 0.2083 - val_valence_mse: 0.0632 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535ms/step - arousal_mse: 0.0671 - concordance_correlation_coefficient: 0.4586 - loss: 0.0690 - mae: 0.2063 - valence_mse: 0.0650\n",
      "Epoch 33: val_loss did not improve from 0.06882\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.06882\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 568ms/step - arousal_mse: 0.0699 - concordance_correlation_coefficient: 0.4362 - loss: 0.0699 - mae: 0.2073 - valence_mse: 0.0628 - val_arousal_mse: 0.0713 - val_concordance_correlation_coefficient: 0.4137 - val_loss: 0.0719 - val_mae: 0.2086 - val_valence_mse: 0.0662 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 568ms/step - arousal_mse: 0.0699 - concordance_correlation_coefficient: 0.4362 - loss: 0.0699 - mae: 0.2073 - valence_mse: 0.0628 - val_arousal_mse: 0.0713 - val_concordance_correlation_coefficient: 0.4137 - val_loss: 0.0719 - val_mae: 0.2086 - val_valence_mse: 0.0662 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637ms/step - arousal_mse: 0.0655 - concordance_correlation_coefficient: 0.4926 - loss: 0.0648 - mae: 0.1973 - valence_mse: 0.0566\n",
      "Epoch 34: val_loss did not improve from 0.06882\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.06882\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 681ms/step - arousal_mse: 0.0669 - concordance_correlation_coefficient: 0.4800 - loss: 0.0670 - mae: 0.2024 - valence_mse: 0.0601 - val_arousal_mse: 0.1193 - val_concordance_correlation_coefficient: 0.0378 - val_loss: 0.1112 - val_mae: 0.2623 - val_valence_mse: 0.0926 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 681ms/step - arousal_mse: 0.0669 - concordance_correlation_coefficient: 0.4800 - loss: 0.0670 - mae: 0.2024 - valence_mse: 0.0601 - val_arousal_mse: 0.1193 - val_concordance_correlation_coefficient: 0.0378 - val_loss: 0.1112 - val_mae: 0.2623 - val_valence_mse: 0.0926 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584ms/step - arousal_mse: 0.0723 - concordance_correlation_coefficient: 0.4102 - loss: 0.0713 - mae: 0.2104 - valence_mse: 0.0633\n",
      "Epoch 35: val_loss did not improve from 0.06882\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.06882\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 629ms/step - arousal_mse: 0.0679 - concordance_correlation_coefficient: 0.4625 - loss: 0.0680 - mae: 0.2051 - valence_mse: 0.0620 - val_arousal_mse: 0.0764 - val_concordance_correlation_coefficient: 0.4079 - val_loss: 0.0747 - val_mae: 0.2135 - val_valence_mse: 0.0652 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 629ms/step - arousal_mse: 0.0679 - concordance_correlation_coefficient: 0.4625 - loss: 0.0680 - mae: 0.2051 - valence_mse: 0.0620 - val_arousal_mse: 0.0764 - val_concordance_correlation_coefficient: 0.4079 - val_loss: 0.0747 - val_mae: 0.2135 - val_valence_mse: 0.0652 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553ms/step - arousal_mse: 0.0671 - concordance_correlation_coefficient: 0.4758 - loss: 0.0663 - mae: 0.2017 - valence_mse: 0.0595\n",
      "Epoch 36: val_loss did not improve from 0.06882\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.06882\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 586ms/step - arousal_mse: 0.0644 - concordance_correlation_coefficient: 0.4967 - loss: 0.0648 - mae: 0.1995 - valence_mse: 0.0595 - val_arousal_mse: 0.0721 - val_concordance_correlation_coefficient: 0.4186 - val_loss: 0.0697 - val_mae: 0.2063 - val_valence_mse: 0.0641 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 586ms/step - arousal_mse: 0.0644 - concordance_correlation_coefficient: 0.4967 - loss: 0.0648 - mae: 0.1995 - valence_mse: 0.0595 - val_arousal_mse: 0.0721 - val_concordance_correlation_coefficient: 0.4186 - val_loss: 0.0697 - val_mae: 0.2063 - val_valence_mse: 0.0641 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523ms/step - arousal_mse: 0.0582 - concordance_correlation_coefficient: 0.5231 - loss: 0.0605 - mae: 0.1918 - valence_mse: 0.0575\n",
      "Epoch 37: val_loss did not improve from 0.06882\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.06882\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 554ms/step - arousal_mse: 0.0644 - concordance_correlation_coefficient: 0.5086 - loss: 0.0641 - mae: 0.1977 - valence_mse: 0.0575 - val_arousal_mse: 0.0802 - val_concordance_correlation_coefficient: 0.3270 - val_loss: 0.0775 - val_mae: 0.2190 - val_valence_mse: 0.0703 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 554ms/step - arousal_mse: 0.0644 - concordance_correlation_coefficient: 0.5086 - loss: 0.0641 - mae: 0.1977 - valence_mse: 0.0575 - val_arousal_mse: 0.0802 - val_concordance_correlation_coefficient: 0.3270 - val_loss: 0.0775 - val_mae: 0.2190 - val_valence_mse: 0.0703 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551ms/step - arousal_mse: 0.0664 - concordance_correlation_coefficient: 0.4775 - loss: 0.0656 - mae: 0.1989 - valence_mse: 0.0588\n",
      "Epoch 38: val_loss did not improve from 0.06882\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.06882\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 584ms/step - arousal_mse: 0.0654 - concordance_correlation_coefficient: 0.4929 - loss: 0.0652 - mae: 0.1986 - valence_mse: 0.0591 - val_arousal_mse: 0.0773 - val_concordance_correlation_coefficient: 0.4123 - val_loss: 0.0724 - val_mae: 0.2103 - val_valence_mse: 0.0670 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 584ms/step - arousal_mse: 0.0654 - concordance_correlation_coefficient: 0.4929 - loss: 0.0652 - mae: 0.1986 - valence_mse: 0.0591 - val_arousal_mse: 0.0773 - val_concordance_correlation_coefficient: 0.4123 - val_loss: 0.0724 - val_mae: 0.2103 - val_valence_mse: 0.0670 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603ms/step - arousal_mse: 0.0589 - concordance_correlation_coefficient: 0.5369 - loss: 0.0601 - mae: 0.1932 - valence_mse: 0.0562\n",
      "Epoch 39: val_loss did not improve from 0.06882\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.06882\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 638ms/step - arousal_mse: 0.0611 - concordance_correlation_coefficient: 0.5404 - loss: 0.0608 - mae: 0.1921 - valence_mse: 0.0546 - val_arousal_mse: 0.0690 - val_concordance_correlation_coefficient: 0.4724 - val_loss: 0.0698 - val_mae: 0.2056 - val_valence_mse: 0.0627 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 638ms/step - arousal_mse: 0.0611 - concordance_correlation_coefficient: 0.5404 - loss: 0.0608 - mae: 0.1921 - valence_mse: 0.0546 - val_arousal_mse: 0.0690 - val_concordance_correlation_coefficient: 0.4724 - val_loss: 0.0698 - val_mae: 0.2056 - val_valence_mse: 0.0627 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536ms/step - arousal_mse: 0.0606 - concordance_correlation_coefficient: 0.5338 - loss: 0.0609 - mae: 0.1929 - valence_mse: 0.0555\n",
      "Epoch 40: val_loss did not improve from 0.06882\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.06882\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 567ms/step - arousal_mse: 0.0613 - concordance_correlation_coefficient: 0.5429 - loss: 0.0604 - mae: 0.1911 - valence_mse: 0.0535 - val_arousal_mse: 0.0692 - val_concordance_correlation_coefficient: 0.4512 - val_loss: 0.0698 - val_mae: 0.2057 - val_valence_mse: 0.0631 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 567ms/step - arousal_mse: 0.0613 - concordance_correlation_coefficient: 0.5429 - loss: 0.0604 - mae: 0.1911 - valence_mse: 0.0535 - val_arousal_mse: 0.0692 - val_concordance_correlation_coefficient: 0.4512 - val_loss: 0.0698 - val_mae: 0.2057 - val_valence_mse: 0.0631 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555ms/step - arousal_mse: 0.0629 - concordance_correlation_coefficient: 0.5567 - loss: 0.0614 - mae: 0.1939 - valence_mse: 0.0538\n",
      "Epoch 41: val_loss improved from 0.06882 to 0.06835, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.06882 to 0.06835, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 589ms/step - arousal_mse: 0.0605 - concordance_correlation_coefficient: 0.5509 - loss: 0.0607 - mae: 0.1927 - valence_mse: 0.0557 - val_arousal_mse: 0.0685 - val_concordance_correlation_coefficient: 0.4379 - val_loss: 0.0683 - val_mae: 0.2046 - val_valence_mse: 0.0613 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "Epoch 42/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step - arousal_mse: 0.0584 - concordance_correlation_coefficient: 0.5603 - loss: 0.0586 - mae: 0.1905 - valence_mse: 0.0539\n",
      "Epoch 42: val_loss did not improve from 0.06835\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.06835\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 520ms/step - arousal_mse: 0.0605 - concordance_correlation_coefficient: 0.5445 - loss: 0.0604 - mae: 0.1906 - valence_mse: 0.0550 - val_arousal_mse: 0.0725 - val_concordance_correlation_coefficient: 0.4061 - val_loss: 0.0721 - val_mae: 0.2092 - val_valence_mse: 0.0639 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 520ms/step - arousal_mse: 0.0605 - concordance_correlation_coefficient: 0.5445 - loss: 0.0604 - mae: 0.1906 - valence_mse: 0.0550 - val_arousal_mse: 0.0725 - val_concordance_correlation_coefficient: 0.4061 - val_loss: 0.0721 - val_mae: 0.2092 - val_valence_mse: 0.0639 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522ms/step - arousal_mse: 0.0622 - concordance_correlation_coefficient: 0.5442 - loss: 0.0624 - mae: 0.1973 - valence_mse: 0.0572\n",
      "Epoch 43: val_loss did not improve from 0.06835\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.06835\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 566ms/step - arousal_mse: 0.0591 - concordance_correlation_coefficient: 0.5463 - loss: 0.0595 - mae: 0.1912 - valence_mse: 0.0545 - val_arousal_mse: 0.0702 - val_concordance_correlation_coefficient: 0.4616 - val_loss: 0.0691 - val_mae: 0.2052 - val_valence_mse: 0.0622 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 566ms/step - arousal_mse: 0.0591 - concordance_correlation_coefficient: 0.5463 - loss: 0.0595 - mae: 0.1912 - valence_mse: 0.0545 - val_arousal_mse: 0.0702 - val_concordance_correlation_coefficient: 0.4616 - val_loss: 0.0691 - val_mae: 0.2052 - val_valence_mse: 0.0622 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512ms/step - arousal_mse: 0.0591 - concordance_correlation_coefficient: 0.5700 - loss: 0.0588 - mae: 0.1898 - valence_mse: 0.0528\n",
      "Epoch 44: val_loss improved from 0.06835 to 0.06731, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.06835 to 0.06731, saving model to ../output/models/emotion_crnn_20250917_152753/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 544ms/step - arousal_mse: 0.0595 - concordance_correlation_coefficient: 0.5571 - loss: 0.0595 - mae: 0.1902 - valence_mse: 0.0538 - val_arousal_mse: 0.0672 - val_concordance_correlation_coefficient: 0.4565 - val_loss: 0.0673 - val_mae: 0.2043 - val_valence_mse: 0.0604 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "Epoch 45/50\n",
      "\u001b[1m51/72\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 537ms/step - arousal_mse: 0.0565 - concordance_correlation_coefficient: 0.5513 - loss: 0.0589 - mae: 0.1902 - valence_mse: 0.0569"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def train_emotion_crnn(model, X_train, y_train, X_val, y_val, \n",
    "                      callbacks, epochs=100, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train the CRNN model for emotion recognition\n",
    "    \n",
    "    Args:\n",
    "        model: Compiled Keras model\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data  \n",
    "        callbacks: List of training callbacks\n",
    "        epochs: Maximum training epochs\n",
    "        batch_size: Training batch size\n",
    "    \n",
    "    Returns:\n",
    "        Training history object\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting training:\")\n",
    "    print(f\"  Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"  Validation samples: {X_val.shape[0]}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Max epochs: {epochs}\")\n",
    "    print(f\"  Input shape: {X_train.shape[1:]}\")\n",
    "    print(f\"  Output shape: {y_train.shape[1:]}\")\n",
    "    \n",
    "    # Start training\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"  Total epochs trained: {len(history.history['loss'])}\")\n",
    "    print(f\"  Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"  Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train the model\n",
    "print(\" Starting CRNN training...\")\n",
    "history = train_emotion_crnn(\n",
    "    model=model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train, \n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    callbacks=callbacks,\n",
    "    epochs=50,  # Start with fewer epochs for testing\n",
    "    batch_size=16  # Smaller batch size for memory efficiency\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3bf79b",
   "metadata": {},
   "source": [
    "# Training Results Visualization\n",
    "\n",
    "Let's visualize the training progress and analyze the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics\n",
    "    \n",
    "    Args:\n",
    "        history: Keras training history object\n",
    "        save_path: Optional path to save plots\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[0, 0].set_title('Model Loss Over Time')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot MAE (Mean Absolute Error)\n",
    "    axes[0, 1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
    "    axes[0, 1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "    axes[0, 1].set_title('Mean Absolute Error Over Time')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('MAE')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot Valence MSE\n",
    "    if 'valence_mse' in history.history:\n",
    "        axes[1, 0].plot(history.history['valence_mse'], label='Training Valence MSE', linewidth=2)\n",
    "        axes[1, 0].plot(history.history['val_valence_mse'], label='Validation Valence MSE', linewidth=2)\n",
    "        axes[1, 0].set_title('Valence MSE Over Time')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Valence MSE')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot Arousal MSE\n",
    "    if 'arousal_mse' in history.history:\n",
    "        axes[1, 1].plot(history.history['arousal_mse'], label='Training Arousal MSE', linewidth=2)\n",
    "        axes[1, 1].plot(history.history['val_arousal_mse'], label='Validation Arousal MSE', linewidth=2)\n",
    "        axes[1, 1].set_title('Arousal MSE Over Time')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Arousal MSE')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/training_history.png\", dpi=300, bbox_inches='tight')\n",
    "        print(f\"Training plots saved to: {save_path}/training_history.png\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics summary\n",
    "    print(\"\\n Final Training Metrics:\")\n",
    "    print(\"-\" * 40)\n",
    "    final_epoch = len(history.history['loss']) - 1\n",
    "    \n",
    "    for metric in ['loss', 'mae', 'valence_mse', 'arousal_mse']:\n",
    "        if metric in history.history:\n",
    "            train_val = history.history[metric][final_epoch]\n",
    "            val_val = history.history[f'val_{metric}'][final_epoch]\n",
    "            print(f\"{metric.upper():12}: Train={train_val:.4f}, Val={val_val:.4f}\")\n",
    "\n",
    "# Plot the training results\n",
    "plot_training_history(history, save_path=logs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d58a2",
   "metadata": {},
   "source": [
    "# Model Evaluation on Test Set\n",
    "\n",
    "Let's evaluate the trained model on our held-out test set to get unbiased performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b70204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_emotion_model(model, X_test, y_test, save_path=None):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the emotion recognition model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        X_test, y_test: Test data\n",
    "        save_path: Optional path to save results\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\" Evaluating model on test set...\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = model.predict(X_test, verbose=1)\n",
    "    \n",
    "    # Separate valence and arousal\n",
    "    y_true_valence = y_test[:, 0]\n",
    "    y_true_arousal = y_test[:, 1]\n",
    "    y_pred_valence = predictions[:, 0]\n",
    "    y_pred_arousal = predictions[:, 1]\n",
    "    \n",
    "    # Calculate metrics for each dimension\n",
    "    metrics = {}\n",
    "    \n",
    "    # Valence metrics\n",
    "    metrics['valence'] = {\n",
    "        'mse': mean_squared_error(y_true_valence, y_pred_valence),\n",
    "        'mae': mean_absolute_error(y_true_valence, y_pred_valence),\n",
    "        'r2': r2_score(y_true_valence, y_pred_valence),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_true_valence, y_pred_valence))\n",
    "    }\n",
    "    \n",
    "    # Arousal metrics  \n",
    "    metrics['arousal'] = {\n",
    "        'mse': mean_squared_error(y_true_arousal, y_pred_arousal),\n",
    "        'mae': mean_absolute_error(y_true_arousal, y_pred_arousal),\n",
    "        'r2': r2_score(y_true_arousal, y_pred_arousal),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_true_arousal, y_pred_arousal))\n",
    "    }\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_mse = mean_squared_error(y_test.flatten(), predictions.flatten())\n",
    "    overall_mae = mean_absolute_error(y_test.flatten(), predictions.flatten())\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n Test Set Evaluation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"\\n VALENCE Predictions:\")\n",
    "    print(f\"  MSE:  {metrics['valence']['mse']:.4f}\")\n",
    "    print(f\"  MAE:  {metrics['valence']['mae']:.4f}\")\n",
    "    print(f\"  RMSE: {metrics['valence']['rmse']:.4f}\")\n",
    "    print(f\"  R:   {metrics['valence']['r2']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n AROUSAL Predictions:\")\n",
    "    print(f\"  MSE:  {metrics['arousal']['mse']:.4f}\")\n",
    "    print(f\"  MAE:  {metrics['arousal']['mae']:.4f}\")\n",
    "    print(f\"  RMSE: {metrics['arousal']['rmse']:.4f}\")\n",
    "    print(f\"  R:   {metrics['arousal']['r2']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n OVERALL Performance:\")\n",
    "    print(f\"  Combined MSE: {overall_mse:.4f}\")\n",
    "    print(f\"  Combined MAE: {overall_mae:.4f}\")\n",
    "    \n",
    "    # Create prediction scatter plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Valence scatter plot\n",
    "    axes[0].scatter(y_true_valence, y_pred_valence, alpha=0.6, s=30)\n",
    "    axes[0].plot([-1, 1], [-1, 1], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[0].set_xlabel('True Valence')\n",
    "    axes[0].set_ylabel('Predicted Valence')\n",
    "    axes[0].set_title(f'Valence Predictions (R = {metrics[\"valence\"][\"r2\"]:.3f})')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_xlim([-1.1, 1.1])\n",
    "    axes[0].set_ylim([-1.1, 1.1])\n",
    "    \n",
    "    # Arousal scatter plot\n",
    "    axes[1].scatter(y_true_arousal, y_pred_arousal, alpha=0.6, s=30)\n",
    "    axes[1].plot([-1, 1], [-1, 1], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[1].set_xlabel('True Arousal')\n",
    "    axes[1].set_ylabel('Predicted Arousal')\n",
    "    axes[1].set_title(f'Arousal Predictions (R = {metrics[\"arousal\"][\"r2\"]:.3f})')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_xlim([-1.1, 1.1])\n",
    "    axes[1].set_ylim([-1.1, 1.1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/test_predictions.png\", dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nPrediction plots saved to: {save_path}/test_predictions.png\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return metrics, predictions\n",
    "\n",
    "# Evaluate the model\n",
    "test_metrics, test_predictions = evaluate_emotion_model(\n",
    "    model=model, \n",
    "    X_test=X_test, \n",
    "    y_test=y_test,\n",
    "    save_path=logs_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c15c0",
   "metadata": {},
   "source": [
    "#  Understanding Callbacks & Training Performance Analysis\n",
    "\n",
    "## How Callbacks Work in Keras\n",
    "\n",
    "**Callbacks are automatically used internally by Keras during training!** You don't need to call them manually.\n",
    "\n",
    "When you pass the `callbacks` list to `model.fit()`, Keras automatically:\n",
    "1. **Calls each callback at specific training events** (start of epoch, end of batch, etc.)\n",
    "2. **Monitors the specified metrics** (like `val_loss`)\n",
    "3. **Takes actions based on the callback logic**\n",
    "\n",
    "Let's break down what each callback is doing behind the scenes:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
